{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "895e7096145459d4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SHAP importances for ETF: XLB\n",
      "Computing SHAP importances for ETF: XLE\n",
      "Computing SHAP importances for ETF: XLF\n",
      "Computing SHAP importances for ETF: XLI\n",
      "Computing SHAP importances for ETF: XLK\n",
      "Computing SHAP importances for ETF: XLP\n",
      "Computing SHAP importances for ETF: XLY\n",
      "Computing SHAP importances for ETF: XLV\n",
      "Computing SHAP importances for ETF: XLU\n",
      "\n",
      "==== Training models for ETF: XLB ====\n",
      "Selected features for XLB: ['XLB_Vol_5', 'SMB_lag_2', 'XLB_Mom_3', 'HML_lag_2', 'XLB_SMA_5', 'XLB_LagRet_1', 'XLB_EMA_12', 'SMB', 'XLB_RSI_7', 'CMA_lag_1', 'RMW_lag_3', 'Mkt-RF', 'XLB_MACD', 'XLB_LagRet_2', 'RMW', 'HML', 'RMW_lag_2', 'Mkt-RF_lag_2', 'Mkt-RF_lag_1', 'CMA', 'SMB_lag_1', 'XLB_LagRet_3', 'CMA_lag_3', 'RMW_lag_1', 'Mkt-RF_lag_3', 'SMB_lag_3', 'HML_lag_1', 'CMA_lag_2', 'HML_lag_3', 'VIX']\n",
      "\n",
      "Training window starting 2009\n",
      "MSE: 0.000451  RMSE: 0.021225  MAE: 0.016740  R²: -0.004045  DirAcc: 53.57%\n",
      "Window processed in 58.81 seconds\n",
      "\n",
      "Training window starting 2010\n",
      "MSE: 0.000246  RMSE: 0.015670  MAE: 0.011754  R²: -0.005369  DirAcc: 50.40%\n",
      "Window processed in 75.25 seconds\n",
      "\n",
      "Training window starting 2011\n",
      "MSE: 0.000387  RMSE: 0.019676  MAE: 0.014538  R²: -0.014805  DirAcc: 48.41%\n",
      "Window processed in 61.52 seconds\n",
      "\n",
      "Training window starting 2012\n",
      "MSE: 0.000123  RMSE: 0.011112  MAE: 0.008488  R²: 0.000233  DirAcc: 49.60%\n",
      "Window processed in 60.68 seconds\n",
      "\n",
      "Training window starting 2013\n",
      "MSE: 0.000081  RMSE: 0.008984  MAE: 0.007009  R²: -0.002773  DirAcc: 56.35%\n",
      "Window processed in 59.22 seconds\n",
      "\n",
      "Training window starting 2014\n",
      "MSE: 0.000079  RMSE: 0.008909  MAE: 0.006648  R²: -0.019257  DirAcc: 51.98%\n",
      "Window processed in 48.17 seconds\n",
      "\n",
      "Training window starting 2015\n",
      "MSE: 0.000142  RMSE: 0.011922  MAE: 0.009085  R²: -0.029135  DirAcc: 46.03%\n",
      "Window processed in 50.20 seconds\n",
      "\n",
      "Training window starting 2016\n",
      "MSE: 0.000128  RMSE: 0.011335  MAE: 0.008413  R²: -0.029044  DirAcc: 51.98%\n",
      "Window processed in 47.65 seconds\n",
      "\n",
      "Training window starting 2017\n",
      "MSE: 0.000044  RMSE: 0.006628  MAE: 0.004951  R²: -0.009087  DirAcc: 54.98%\n",
      "Window processed in 56.31 seconds\n",
      "\n",
      "Training window starting 2018\n",
      "MSE: 0.000149  RMSE: 0.012195  MAE: 0.009462  R²: -0.024763  DirAcc: 49.00%\n",
      "Window processed in 48.18 seconds\n",
      "\n",
      "Training window starting 2019\n",
      "MSE: 0.000099  RMSE: 0.009929  MAE: 0.007553  R²: -0.004011  DirAcc: 53.17%\n",
      "Window processed in 48.31 seconds\n",
      "\n",
      "Training window starting 2020\n",
      "MSE: 0.000590  RMSE: 0.024280  MAE: 0.016410  R²: -0.008091  DirAcc: 48.62%\n",
      "Window processed in 48.78 seconds\n",
      "\n",
      "Training window starting 2021\n",
      "MSE: 0.000122  RMSE: 0.011062  MAE: 0.008696  R²: -0.053047  DirAcc: 47.22%\n",
      "Window processed in 33.21 seconds\n",
      "\n",
      "Training window starting 2022\n",
      "MSE: 0.000270  RMSE: 0.016432  MAE: 0.012971  R²: -0.074846  DirAcc: 46.22%\n",
      "Window processed in 34.29 seconds\n",
      "\n",
      "Training window starting 2023\n",
      "MSE: 0.000113  RMSE: 0.010619  MAE: 0.008396  R²: -0.002839  DirAcc: 53.20%\n",
      "Window processed in 30.35 seconds\n",
      "\n",
      "Training window starting 2024\n",
      "MSE: 0.000072  RMSE: 0.008464  MAE: 0.006644  R²: -0.008326  DirAcc: 52.40%\n",
      "Window processed in 32.55 seconds\n",
      "\n",
      "==== Training models for ETF: XLE ====\n",
      "Selected features for XLE: ['XLE_Vol_5', 'SMB_lag_2', 'XLE_Mom_3', 'HML_lag_2', 'XLE_SMA_5', 'XLE_LagRet_1', 'XLE_EMA_12', 'SMB', 'XLE_RSI_7', 'CMA_lag_1', 'RMW_lag_3', 'Mkt-RF', 'XLE_MACD', 'XLE_LagRet_2', 'RMW', 'HML', 'RMW_lag_2', 'Mkt-RF_lag_2', 'Mkt-RF_lag_1', 'CMA', 'SMB_lag_1', 'XLE_LagRet_3', 'CMA_lag_3', 'RMW_lag_1', 'Mkt-RF_lag_3', 'SMB_lag_3', 'HML_lag_1', 'CMA_lag_2', 'HML_lag_3', 'VIX']\n",
      "\n",
      "Training window starting 2009\n",
      "MSE: 0.000498  RMSE: 0.022317  MAE: 0.016975  R²: -0.024640  DirAcc: 52.78%\n",
      "Window processed in 34.61 seconds\n",
      "\n",
      "Training window starting 2010\n",
      "MSE: 0.000214  RMSE: 0.014620  MAE: 0.010549  R²: 0.002805  DirAcc: 53.97%\n",
      "Window processed in 33.46 seconds\n",
      "\n",
      "Training window starting 2011\n",
      "MSE: 0.000396  RMSE: 0.019891  MAE: 0.014730  R²: 0.009091  DirAcc: 50.40%\n",
      "Window processed in 30.96 seconds\n",
      "\n",
      "Training window starting 2012\n",
      "MSE: 0.000129  RMSE: 0.011378  MAE: 0.008970  R²: -0.000060  DirAcc: 50.00%\n",
      "Window processed in 39.52 seconds\n",
      "\n",
      "Training window starting 2013\n",
      "MSE: 0.000084  RMSE: 0.009169  MAE: 0.007211  R²: -0.047183  DirAcc: 51.98%\n",
      "Window processed in 32.86 seconds\n",
      "\n",
      "Training window starting 2014\n",
      "MSE: 0.000131  RMSE: 0.011461  MAE: 0.008278  R²: -0.001945  DirAcc: 51.59%\n",
      "Window processed in 36.22 seconds\n",
      "\n",
      "Training window starting 2015\n",
      "MSE: 0.000256  RMSE: 0.016010  MAE: 0.012113  R²: -0.035485  DirAcc: 45.24%\n",
      "Window processed in 41.33 seconds\n",
      "\n",
      "Training window starting 2016\n",
      "MSE: 0.000246  RMSE: 0.015689  MAE: 0.011690  R²: -0.023319  DirAcc: 49.60%\n",
      "Window processed in 38.94 seconds\n",
      "\n",
      "Training window starting 2017\n",
      "MSE: 0.000071  RMSE: 0.008404  MAE: 0.006508  R²: -0.002613  DirAcc: 52.19%\n",
      "Window processed in 45.08 seconds\n",
      "\n",
      "Training window starting 2018\n",
      "MSE: 0.000210  RMSE: 0.014481  MAE: 0.010724  R²: -0.061230  DirAcc: 43.43%\n",
      "Window processed in 36.34 seconds\n",
      "\n",
      "Training window starting 2019\n",
      "MSE: 0.000145  RMSE: 0.012047  MAE: 0.009323  R²: 0.001494  DirAcc: 50.79%\n",
      "Window processed in 32.59 seconds\n",
      "\n",
      "Training window starting 2020\n",
      "MSE: 0.001509  RMSE: 0.038844  MAE: 0.026734  R²: -0.032525  DirAcc: 50.59%\n",
      "Window processed in 33.05 seconds\n",
      "\n",
      "Training window starting 2021\n",
      "MSE: 0.000366  RMSE: 0.019122  MAE: 0.015201  R²: -0.034709  DirAcc: 48.81%\n",
      "Window processed in 43.53 seconds\n",
      "\n",
      "Training window starting 2022\n",
      "MSE: 0.000760  RMSE: 0.027571  MAE: 0.020738  R²: -0.556115  DirAcc: 47.81%\n",
      "Window processed in 46.59 seconds\n",
      "\n",
      "Training window starting 2023\n",
      "MSE: 0.000203  RMSE: 0.014254  MAE: 0.011036  R²: -0.001236  DirAcc: 50.00%\n",
      "Window processed in 45.59 seconds\n",
      "\n",
      "Training window starting 2024\n",
      "MSE: 0.000124  RMSE: 0.011122  MAE: 0.008590  R²: -0.006683  DirAcc: 53.71%\n",
      "Window processed in 44.00 seconds\n",
      "\n",
      "==== Training models for ETF: XLF ====\n",
      "Selected features for XLF: ['XLF_Vol_5', 'SMB_lag_2', 'XLF_Mom_3', 'HML_lag_2', 'XLF_SMA_5', 'XLF_LagRet_1', 'XLF_EMA_12', 'SMB', 'XLF_RSI_7', 'CMA_lag_1', 'RMW_lag_3', 'Mkt-RF', 'XLF_MACD', 'XLF_LagRet_2', 'RMW', 'HML', 'RMW_lag_2', 'Mkt-RF_lag_2', 'Mkt-RF_lag_1', 'CMA', 'SMB_lag_1', 'XLF_LagRet_3', 'CMA_lag_3', 'RMW_lag_1', 'Mkt-RF_lag_3', 'SMB_lag_3', 'HML_lag_1', 'CMA_lag_2', 'HML_lag_3', 'VIX']\n",
      "\n",
      "Training window starting 2009\n",
      "MSE: 0.001780  RMSE: 0.042189  MAE: 0.029649  R²: -0.070780  DirAcc: 48.81%\n",
      "Window processed in 67.90 seconds\n",
      "\n",
      "Training window starting 2010\n",
      "MSE: 0.000266  RMSE: 0.016310  MAE: 0.012403  R²: -0.037594  DirAcc: 44.05%\n",
      "Window processed in 51.47 seconds\n",
      "\n",
      "Training window starting 2011\n",
      "MSE: 0.000461  RMSE: 0.021479  MAE: 0.014796  R²: -0.051769  DirAcc: 54.76%\n",
      "Window processed in 52.46 seconds\n",
      "\n",
      "Training window starting 2012\n",
      "MSE: 0.000124  RMSE: 0.011126  MAE: 0.008574  R²: -0.010291  DirAcc: 46.40%\n",
      "Window processed in 53.31 seconds\n",
      "\n",
      "Training window starting 2013\n",
      "MSE: 0.000080  RMSE: 0.008939  MAE: 0.006986  R²: -0.019140  DirAcc: 41.67%\n",
      "Window processed in 44.95 seconds\n",
      "\n",
      "Training window starting 2014\n",
      "MSE: 0.000067  RMSE: 0.008164  MAE: 0.006153  R²: -0.019576  DirAcc: 54.37%\n",
      "Window processed in 46.43 seconds\n",
      "\n",
      "Training window starting 2015\n",
      "MSE: 0.000128  RMSE: 0.011324  MAE: 0.008556  R²: -0.015014  DirAcc: 50.40%\n",
      "Window processed in 70.04 seconds\n",
      "\n",
      "Training window starting 2016\n",
      "MSE: 0.000142  RMSE: 0.011901  MAE: 0.008307  R²: -0.026463  DirAcc: 53.17%\n",
      "Window processed in 49.20 seconds\n",
      "\n",
      "Training window starting 2017\n",
      "MSE: 0.000070  RMSE: 0.008370  MAE: 0.006180  R²: -0.039868  DirAcc: 48.21%\n",
      "Window processed in 52.13 seconds\n",
      "\n",
      "Training window starting 2018\n",
      "MSE: 0.000154  RMSE: 0.012419  MAE: 0.008982  R²: -0.011856  DirAcc: 46.22%\n",
      "Window processed in 45.45 seconds\n",
      "\n",
      "Training window starting 2019\n",
      "MSE: 0.000098  RMSE: 0.009900  MAE: 0.007333  R²: -0.026409  DirAcc: 55.95%\n",
      "Window processed in 45.03 seconds\n",
      "\n",
      "Training window starting 2020\n",
      "MSE: 0.000847  RMSE: 0.029096  MAE: 0.018891  R²: -0.037259  DirAcc: 55.34%\n",
      "Window processed in 46.04 seconds\n",
      "\n",
      "Training window starting 2021\n",
      "MSE: 0.000148  RMSE: 0.012179  MAE: 0.009507  R²: -0.043226  DirAcc: 51.98%\n",
      "Window processed in 59.98 seconds\n",
      "\n",
      "Training window starting 2022\n",
      "MSE: 0.000358  RMSE: 0.018921  MAE: 0.014359  R²: -0.518900  DirAcc: 44.62%\n",
      "Window processed in 53.76 seconds\n",
      "\n",
      "Training window starting 2023\n",
      "MSE: 0.000106  RMSE: 0.010276  MAE: 0.007582  R²: -0.017884  DirAcc: 50.00%\n",
      "Window processed in 47.22 seconds\n",
      "\n",
      "Training window starting 2024\n",
      "MSE: 0.000079  RMSE: 0.008881  MAE: 0.006594  R²: -0.011120  DirAcc: 55.90%\n",
      "Window processed in 43.37 seconds\n",
      "\n",
      "==== Training models for ETF: XLI ====\n",
      "Selected features for XLI: ['XLI_Vol_5', 'SMB_lag_2', 'XLI_Mom_3', 'HML_lag_2', 'XLI_SMA_5', 'XLI_LagRet_1', 'XLI_EMA_12', 'SMB', 'XLI_RSI_7', 'CMA_lag_1', 'RMW_lag_3', 'Mkt-RF', 'XLI_MACD', 'XLI_LagRet_2', 'RMW', 'HML', 'RMW_lag_2', 'Mkt-RF_lag_2', 'Mkt-RF_lag_1', 'CMA', 'SMB_lag_1', 'XLI_LagRet_3', 'CMA_lag_3', 'RMW_lag_1', 'Mkt-RF_lag_3', 'SMB_lag_3', 'HML_lag_1', 'CMA_lag_2', 'HML_lag_3', 'VIX']\n",
      "\n",
      "Training window starting 2009\n",
      "MSE: 0.000424  RMSE: 0.020585  MAE: 0.015539  R²: 0.002229  DirAcc: 55.16%\n",
      "Window processed in 46.69 seconds\n",
      "\n",
      "Training window starting 2010\n",
      "MSE: 0.000198  RMSE: 0.014080  MAE: 0.010030  R²: -0.036230  DirAcc: 46.83%\n",
      "Window processed in 59.47 seconds\n",
      "\n",
      "Training window starting 2011\n",
      "MSE: 0.000308  RMSE: 0.017555  MAE: 0.012747  R²: -0.041820  DirAcc: 47.22%\n",
      "Window processed in 53.49 seconds\n",
      "\n",
      "Training window starting 2012\n",
      "MSE: 0.000094  RMSE: 0.009717  MAE: 0.007398  R²: -0.004252  DirAcc: 49.60%\n",
      "Window processed in 57.95 seconds\n",
      "\n",
      "Training window starting 2013\n",
      "MSE: 0.000068  RMSE: 0.008229  MAE: 0.006388  R²: -0.031545  DirAcc: 54.76%\n",
      "Window processed in 54.00 seconds\n",
      "\n",
      "Training window starting 2014\n",
      "MSE: 0.000078  RMSE: 0.008806  MAE: 0.006570  R²: -0.056224  DirAcc: 55.56%\n",
      "Window processed in 50.58 seconds\n",
      "\n",
      "Training window starting 2015\n",
      "MSE: 0.000110  RMSE: 0.010477  MAE: 0.007881  R²: -0.045866  DirAcc: 46.43%\n",
      "Window processed in 54.99 seconds\n",
      "\n",
      "Training window starting 2016\n",
      "MSE: 0.000087  RMSE: 0.009315  MAE: 0.006665  R²: -0.017951  DirAcc: 50.40%\n",
      "Window processed in 46.56 seconds\n",
      "\n",
      "Training window starting 2017\n",
      "MSE: 0.000034  RMSE: 0.005795  MAE: 0.004366  R²: -0.015512  DirAcc: 54.98%\n",
      "Window processed in 50.55 seconds\n",
      "\n",
      "Training window starting 2018\n",
      "MSE: 0.000145  RMSE: 0.012048  MAE: 0.008700  R²: -0.010654  DirAcc: 52.99%\n",
      "Window processed in 32.70 seconds\n",
      "\n",
      "Training window starting 2019\n",
      "MSE: 0.000099  RMSE: 0.009948  MAE: 0.007370  R²: -0.009634  DirAcc: 51.59%\n",
      "Window processed in 31.68 seconds\n",
      "\n",
      "Training window starting 2020\n",
      "MSE: 0.000605  RMSE: 0.024597  MAE: 0.016147  R²: 0.030325  DirAcc: 53.75%\n",
      "Window processed in 32.21 seconds\n",
      "\n",
      "Training window starting 2021\n",
      "MSE: 0.000107  RMSE: 0.010354  MAE: 0.008037  R²: -0.124403  DirAcc: 44.84%\n",
      "Window processed in 37.33 seconds\n",
      "\n",
      "Training window starting 2022\n",
      "MSE: 0.000232  RMSE: 0.015244  MAE: 0.011933  R²: -0.232310  DirAcc: 47.81%\n",
      "Window processed in 37.32 seconds\n",
      "\n",
      "Training window starting 2023\n",
      "MSE: 0.000088  RMSE: 0.009380  MAE: 0.007448  R²: -0.015834  DirAcc: 52.00%\n",
      "Window processed in 32.13 seconds\n",
      "\n",
      "Training window starting 2024\n",
      "MSE: 0.000076  RMSE: 0.008732  MAE: 0.006716  R²: -0.049248  DirAcc: 42.79%\n",
      "Window processed in 31.56 seconds\n",
      "\n",
      "==== Training models for ETF: XLK ====\n",
      "Selected features for XLK: ['XLK_Vol_5', 'SMB_lag_2', 'XLK_Mom_3', 'HML_lag_2', 'XLK_SMA_5', 'XLK_LagRet_1', 'XLK_EMA_12', 'SMB', 'XLK_RSI_7', 'CMA_lag_1', 'RMW_lag_3', 'Mkt-RF', 'XLK_MACD', 'XLK_LagRet_2', 'RMW', 'HML', 'RMW_lag_2', 'Mkt-RF_lag_2', 'Mkt-RF_lag_1', 'CMA', 'SMB_lag_1', 'XLK_LagRet_3', 'CMA_lag_3', 'RMW_lag_1', 'Mkt-RF_lag_3', 'SMB_lag_3', 'HML_lag_1', 'CMA_lag_2', 'HML_lag_3', 'VIX']\n",
      "\n",
      "Training window starting 2009\n",
      "MSE: 0.000256  RMSE: 0.016014  MAE: 0.011684  R²: 0.004590  DirAcc: 51.59%\n",
      "Window processed in 40.16 seconds\n",
      "\n",
      "Training window starting 2010\n",
      "MSE: 0.000140  RMSE: 0.011851  MAE: 0.008494  R²: -0.017535  DirAcc: 54.37%\n",
      "Window processed in 34.87 seconds\n",
      "\n",
      "Training window starting 2011\n",
      "MSE: 0.000207  RMSE: 0.014391  MAE: 0.010589  R²: -0.025771  DirAcc: 52.78%\n",
      "Window processed in 35.54 seconds\n",
      "\n",
      "Training window starting 2012\n",
      "MSE: 0.000090  RMSE: 0.009494  MAE: 0.007165  R²: -0.001783  DirAcc: 46.00%\n",
      "Window processed in 36.90 seconds\n",
      "\n",
      "Training window starting 2013\n",
      "MSE: 0.000063  RMSE: 0.007907  MAE: 0.006396  R²: -0.258080  DirAcc: 41.67%\n",
      "Window processed in 33.99 seconds\n",
      "\n",
      "Training window starting 2014\n",
      "MSE: 0.000064  RMSE: 0.007991  MAE: 0.006013  R²: -0.005910  DirAcc: 43.25%\n",
      "Window processed in 33.56 seconds\n",
      "\n",
      "Training window starting 2015\n",
      "MSE: 0.000127  RMSE: 0.011277  MAE: 0.008300  R²: -0.002901  DirAcc: 50.79%\n",
      "Window processed in 48.04 seconds\n",
      "\n",
      "Training window starting 2016\n",
      "MSE: 0.000102  RMSE: 0.010094  MAE: 0.007272  R²: -0.092586  DirAcc: 50.00%\n",
      "Window processed in 61.23 seconds\n",
      "\n",
      "Training window starting 2017\n",
      "MSE: 0.000044  RMSE: 0.006631  MAE: 0.004674  R²: -0.028030  DirAcc: 60.16%\n",
      "Window processed in 65.41 seconds\n",
      "\n",
      "Training window starting 2018\n",
      "MSE: 0.000217  RMSE: 0.014716  MAE: 0.010563  R²: -0.003965  DirAcc: 51.79%\n",
      "Window processed in 52.76 seconds\n",
      "\n",
      "Training window starting 2019\n",
      "MSE: 0.000133  RMSE: 0.011536  MAE: 0.008382  R²: -0.022602  DirAcc: 55.95%\n",
      "Window processed in 47.07 seconds\n",
      "\n",
      "Training window starting 2020\n",
      "MSE: 0.000616  RMSE: 0.024817  MAE: 0.016218  R²: 0.034543  DirAcc: 54.15%\n",
      "Window processed in 46.72 seconds\n",
      "\n",
      "Training window starting 2021\n",
      "MSE: 0.000152  RMSE: 0.012343  MAE: 0.009488  R²: -0.041404  DirAcc: 51.59%\n",
      "Window processed in 33.58 seconds\n",
      "\n",
      "Training window starting 2022\n",
      "MSE: 0.000432  RMSE: 0.020781  MAE: 0.016526  R²: -0.015120  DirAcc: 47.41%\n",
      "Window processed in 38.27 seconds\n",
      "\n",
      "Training window starting 2023\n",
      "MSE: 0.000139  RMSE: 0.011806  MAE: 0.009294  R²: -0.004865  DirAcc: 57.20%\n",
      "Window processed in 37.69 seconds\n",
      "\n",
      "Training window starting 2024\n",
      "MSE: 0.000200  RMSE: 0.014148  MAE: 0.010503  R²: -0.010872  DirAcc: 43.67%\n",
      "Window processed in 31.47 seconds\n",
      "\n",
      "==== Training models for ETF: XLP ====\n",
      "Selected features for XLP: ['XLP_Vol_5', 'SMB_lag_2', 'XLP_Mom_3', 'HML_lag_2', 'XLP_SMA_5', 'XLP_LagRet_1', 'XLP_EMA_12', 'SMB', 'XLP_RSI_7', 'CMA_lag_1', 'RMW_lag_3', 'Mkt-RF', 'XLP_MACD', 'XLP_LagRet_2', 'RMW', 'HML', 'RMW_lag_2', 'Mkt-RF_lag_2', 'Mkt-RF_lag_1', 'CMA', 'SMB_lag_1', 'XLP_LagRet_3', 'CMA_lag_3', 'RMW_lag_1', 'Mkt-RF_lag_3', 'SMB_lag_3', 'HML_lag_1', 'CMA_lag_2', 'HML_lag_3', 'VIX']\n",
      "\n",
      "Training window starting 2009\n",
      "MSE: 0.000118  RMSE: 0.010862  MAE: 0.008369  R²: 0.004780  DirAcc: 50.79%\n",
      "Window processed in 33.93 seconds\n",
      "\n",
      "Training window starting 2010\n",
      "MSE: 0.000058  RMSE: 0.007625  MAE: 0.005647  R²: 0.014114  DirAcc: 48.02%\n",
      "Window processed in 33.54 seconds\n",
      "\n",
      "Training window starting 2011\n",
      "MSE: 0.000090  RMSE: 0.009504  MAE: 0.006922  R²: -0.044487  DirAcc: 51.59%\n",
      "Window processed in 34.44 seconds\n",
      "\n",
      "Training window starting 2012\n",
      "MSE: 0.000039  RMSE: 0.006281  MAE: 0.004617  R²: 0.000449  DirAcc: 49.60%\n",
      "Window processed in 34.70 seconds\n",
      "\n",
      "Training window starting 2013\n",
      "MSE: 0.000050  RMSE: 0.007044  MAE: 0.005275  R²: -0.005365  DirAcc: 55.16%\n",
      "Window processed in 31.26 seconds\n",
      "\n",
      "Training window starting 2014\n",
      "MSE: 0.000037  RMSE: 0.006115  MAE: 0.004720  R²: -0.010549  DirAcc: 50.00%\n",
      "Window processed in 30.73 seconds\n",
      "\n",
      "Training window starting 2015\n",
      "MSE: 0.000078  RMSE: 0.008851  MAE: 0.006663  R²: -0.007819  DirAcc: 55.16%\n",
      "Window processed in 40.32 seconds\n",
      "\n",
      "Training window starting 2016\n",
      "MSE: 0.000055  RMSE: 0.007430  MAE: 0.005622  R²: -0.003379  DirAcc: 54.76%\n",
      "Window processed in 33.56 seconds\n",
      "\n",
      "Training window starting 2017\n",
      "MSE: 0.000024  RMSE: 0.004885  MAE: 0.003694  R²: 0.005470  DirAcc: 54.98%\n",
      "Window processed in 36.69 seconds\n",
      "\n",
      "Training window starting 2018\n",
      "MSE: 0.000086  RMSE: 0.009275  MAE: 0.006859  R²: -0.031062  DirAcc: 45.82%\n",
      "Window processed in 32.71 seconds\n",
      "\n",
      "Training window starting 2019\n",
      "MSE: 0.000046  RMSE: 0.006803  MAE: 0.005121  R²: -0.003609  DirAcc: 57.14%\n",
      "Window processed in 32.24 seconds\n",
      "\n",
      "Training window starting 2020\n",
      "MSE: 0.000302  RMSE: 0.017373  MAE: 0.010430  R²: 0.025936  DirAcc: 56.13%\n",
      "Window processed in 31.57 seconds\n",
      "\n",
      "Training window starting 2021\n",
      "MSE: 0.000049  RMSE: 0.006974  MAE: 0.005264  R²: -0.007427  DirAcc: 48.02%\n",
      "Window processed in 33.78 seconds\n",
      "\n",
      "Training window starting 2022\n",
      "MSE: 0.000123  RMSE: 0.011088  MAE: 0.008180  R²: -0.018309  DirAcc: 48.61%\n",
      "Window processed in 37.98 seconds\n",
      "\n",
      "Training window starting 2023\n",
      "MSE: 0.000051  RMSE: 0.007176  MAE: 0.005677  R²: 0.002129  DirAcc: 52.40%\n",
      "Window processed in 31.12 seconds\n",
      "\n",
      "Training window starting 2024\n",
      "MSE: 0.000043  RMSE: 0.006543  MAE: 0.005219  R²: -0.152589  DirAcc: 50.66%\n",
      "Window processed in 35.64 seconds\n",
      "\n",
      "==== Training models for ETF: XLY ====\n",
      "Selected features for XLY: ['XLY_Vol_5', 'SMB_lag_2', 'XLY_Mom_3', 'HML_lag_2', 'XLY_SMA_5', 'XLY_LagRet_1', 'XLY_EMA_12', 'SMB', 'XLY_RSI_7', 'CMA_lag_1', 'RMW_lag_3', 'Mkt-RF', 'XLY_MACD', 'XLY_LagRet_2', 'RMW', 'HML', 'RMW_lag_2', 'Mkt-RF_lag_2', 'Mkt-RF_lag_1', 'CMA', 'SMB_lag_1', 'XLY_LagRet_3', 'CMA_lag_3', 'RMW_lag_1', 'Mkt-RF_lag_3', 'SMB_lag_3', 'HML_lag_1', 'CMA_lag_2', 'HML_lag_3', 'VIX']\n",
      "\n",
      "Training window starting 2009\n",
      "MSE: 0.000386  RMSE: 0.019648  MAE: 0.014787  R²: -0.012975  DirAcc: 52.38%\n",
      "Window processed in 36.23 seconds\n",
      "\n",
      "Training window starting 2010\n",
      "MSE: 0.000172  RMSE: 0.013105  MAE: 0.009534  R²: -0.015804  DirAcc: 41.27%\n",
      "Window processed in 35.38 seconds\n",
      "\n",
      "Training window starting 2011\n",
      "MSE: 0.000228  RMSE: 0.015101  MAE: 0.011082  R²: 0.001043  DirAcc: 51.98%\n",
      "Window processed in 35.59 seconds\n",
      "\n",
      "Training window starting 2012\n",
      "MSE: 0.000080  RMSE: 0.008954  MAE: 0.006773  R²: -0.010789  DirAcc: 50.40%\n",
      "Window processed in 33.14 seconds\n",
      "\n",
      "Training window starting 2013\n",
      "MSE: 0.000063  RMSE: 0.007925  MAE: 0.006293  R²: -0.017154  DirAcc: 59.13%\n",
      "Window processed in 33.32 seconds\n",
      "\n",
      "Training window starting 2014\n",
      "MSE: 0.000070  RMSE: 0.008382  MAE: 0.006543  R²: -0.017102  DirAcc: 51.98%\n",
      "Window processed in 31.97 seconds\n",
      "\n",
      "Training window starting 2015\n",
      "MSE: 0.000111  RMSE: 0.010519  MAE: 0.008027  R²: -0.026885  DirAcc: 47.62%\n",
      "Window processed in 31.64 seconds\n",
      "\n",
      "Training window starting 2016\n",
      "MSE: 0.000083  RMSE: 0.009135  MAE: 0.006677  R²: -0.016600  DirAcc: 50.79%\n",
      "Window processed in 34.85 seconds\n",
      "\n",
      "Training window starting 2017\n",
      "MSE: 0.000027  RMSE: 0.005216  MAE: 0.004058  R²: -0.010123  DirAcc: 56.97%\n",
      "Window processed in 31.12 seconds\n",
      "\n",
      "Training window starting 2018\n",
      "MSE: 0.000148  RMSE: 0.012176  MAE: 0.008749  R²: -0.002253  DirAcc: 53.39%\n",
      "Window processed in 31.04 seconds\n",
      "\n",
      "Training window starting 2019\n",
      "MSE: 0.000079  RMSE: 0.008901  MAE: 0.006747  R²: -0.005066  DirAcc: 57.54%\n",
      "Window processed in 32.78 seconds\n",
      "\n",
      "Training window starting 2020\n",
      "MSE: 0.000465  RMSE: 0.021562  MAE: 0.013963  R²: -0.034013  DirAcc: 52.96%\n",
      "Window processed in 31.60 seconds\n",
      "\n",
      "Training window starting 2021\n",
      "MSE: 0.000138  RMSE: 0.011740  MAE: 0.008886  R²: -0.039860  DirAcc: 57.54%\n",
      "Window processed in 34.10 seconds\n",
      "\n",
      "Training window starting 2022\n",
      "MSE: 0.000484  RMSE: 0.021998  MAE: 0.017290  R²: -0.050782  DirAcc: 43.43%\n",
      "Window processed in 36.85 seconds\n",
      "\n",
      "Training window starting 2023\n",
      "MSE: 0.000159  RMSE: 0.012622  MAE: 0.009780  R²: -0.008015  DirAcc: 58.00%\n",
      "Window processed in 36.36 seconds\n",
      "\n",
      "Training window starting 2024\n",
      "MSE: 0.000131  RMSE: 0.011443  MAE: 0.009034  R²: -0.001636  DirAcc: 53.71%\n",
      "Window processed in 32.18 seconds\n",
      "\n",
      "==== Training models for ETF: XLV ====\n",
      "Selected features for XLV: ['XLV_Vol_5', 'SMB_lag_2', 'XLV_Mom_3', 'HML_lag_2', 'XLV_SMA_5', 'XLV_LagRet_1', 'XLV_EMA_12', 'SMB', 'XLV_RSI_7', 'CMA_lag_1', 'RMW_lag_3', 'Mkt-RF', 'XLV_MACD', 'XLV_LagRet_2', 'RMW', 'HML', 'RMW_lag_2', 'Mkt-RF_lag_2', 'Mkt-RF_lag_1', 'CMA', 'SMB_lag_1', 'XLV_LagRet_3', 'CMA_lag_3', 'RMW_lag_1', 'Mkt-RF_lag_3', 'SMB_lag_3', 'HML_lag_1', 'CMA_lag_2', 'HML_lag_3', 'VIX']\n",
      "\n",
      "Training window starting 2009\n",
      "MSE: 0.000158  RMSE: 0.012561  MAE: 0.009202  R²: 0.006213  DirAcc: 56.75%\n",
      "Window processed in 40.41 seconds\n",
      "\n",
      "Training window starting 2010\n",
      "MSE: 0.000089  RMSE: 0.009414  MAE: 0.006997  R²: -0.008390  DirAcc: 50.00%\n",
      "Window processed in 43.87 seconds\n",
      "\n",
      "Training window starting 2011\n",
      "MSE: 0.000155  RMSE: 0.012438  MAE: 0.008769  R²: -0.010182  DirAcc: 51.98%\n",
      "Window processed in 38.19 seconds\n",
      "\n",
      "Training window starting 2012\n",
      "MSE: 0.000048  RMSE: 0.006952  MAE: 0.005276  R²: -0.004288  DirAcc: 52.40%\n",
      "Window processed in 48.97 seconds\n",
      "\n",
      "Training window starting 2013\n",
      "MSE: 0.000061  RMSE: 0.007806  MAE: 0.006070  R²: -0.065363  DirAcc: 45.63%\n",
      "Window processed in 48.21 seconds\n",
      "\n",
      "Training window starting 2014\n",
      "MSE: 0.000083  RMSE: 0.009105  MAE: 0.006866  R²: -0.010018  DirAcc: 51.59%\n",
      "Window processed in 50.43 seconds\n",
      "\n",
      "Training window starting 2015\n",
      "MSE: 0.000137  RMSE: 0.011718  MAE: 0.008840  R²: -0.020539  DirAcc: 51.98%\n",
      "Window processed in 59.93 seconds\n",
      "\n",
      "Training window starting 2016\n",
      "MSE: 0.000093  RMSE: 0.009649  MAE: 0.007305  R²: -0.015943  DirAcc: 48.41%\n",
      "Window processed in 45.42 seconds\n",
      "\n",
      "Training window starting 2017\n",
      "MSE: 0.000030  RMSE: 0.005480  MAE: 0.004278  R²: 0.004959  DirAcc: 54.98%\n",
      "Window processed in 50.25 seconds\n",
      "\n",
      "Training window starting 2018\n",
      "MSE: 0.000127  RMSE: 0.011257  MAE: 0.008153  R²: -0.037087  DirAcc: 44.22%\n",
      "Window processed in 47.39 seconds\n",
      "\n",
      "Training window starting 2019\n",
      "MSE: 0.000070  RMSE: 0.008385  MAE: 0.006244  R²: -0.011081  DirAcc: 53.57%\n",
      "Window processed in 46.21 seconds\n",
      "\n",
      "Training window starting 2020\n",
      "MSE: 0.000388  RMSE: 0.019702  MAE: 0.012209  R²: -0.049038  DirAcc: 52.96%\n",
      "Window processed in 48.90 seconds\n",
      "\n",
      "Training window starting 2021\n",
      "MSE: 0.000065  RMSE: 0.008079  MAE: 0.006270  R²: -0.179074  DirAcc: 46.83%\n",
      "Window processed in 44.91 seconds\n",
      "\n",
      "Training window starting 2022\n",
      "MSE: 0.000147  RMSE: 0.012145  MAE: 0.009760  R²: -0.038744  DirAcc: 45.82%\n",
      "Window processed in 48.29 seconds\n",
      "\n",
      "Training window starting 2023\n",
      "MSE: 0.000054  RMSE: 0.007326  MAE: 0.005973  R²: -0.000802  DirAcc: 52.00%\n",
      "Window processed in 46.95 seconds\n",
      "\n",
      "Training window starting 2024\n",
      "MSE: 0.000074  RMSE: 0.008592  MAE: 0.006978  R²: -0.637356  DirAcc: 48.91%\n",
      "Window processed in 62.84 seconds\n",
      "\n",
      "==== Training models for ETF: XLU ====\n",
      "Selected features for XLU: ['XLU_Vol_5', 'SMB_lag_2', 'XLU_Mom_3', 'HML_lag_2', 'XLU_SMA_5', 'XLU_LagRet_1', 'XLU_EMA_12', 'SMB', 'XLU_RSI_7', 'CMA_lag_1', 'RMW_lag_3', 'Mkt-RF', 'XLU_MACD', 'XLU_LagRet_2', 'RMW', 'HML', 'RMW_lag_2', 'Mkt-RF_lag_2', 'Mkt-RF_lag_1', 'CMA', 'SMB_lag_1', 'XLU_LagRet_3', 'CMA_lag_3', 'RMW_lag_1', 'Mkt-RF_lag_3', 'SMB_lag_3', 'HML_lag_1', 'CMA_lag_2', 'HML_lag_3', 'VIX']\n",
      "\n",
      "Training window starting 2009\n",
      "MSE: 0.000170  RMSE: 0.013028  MAE: 0.009976  R²: -0.040800  DirAcc: 53.17%\n",
      "Window processed in 48.60 seconds\n",
      "\n",
      "Training window starting 2010\n",
      "MSE: 0.000091  RMSE: 0.009562  MAE: 0.007168  R²: -0.008253  DirAcc: 48.81%\n",
      "Window processed in 45.50 seconds\n",
      "\n",
      "Training window starting 2011\n",
      "MSE: 0.000109  RMSE: 0.010427  MAE: 0.007707  R²: 0.020299  DirAcc: 48.41%\n",
      "Window processed in 45.75 seconds\n",
      "\n",
      "Training window starting 2012\n",
      "MSE: 0.000038  RMSE: 0.006203  MAE: 0.004815  R²: -0.000019  DirAcc: 46.40%\n",
      "Window processed in 45.53 seconds\n",
      "\n",
      "Training window starting 2013\n",
      "MSE: 0.000060  RMSE: 0.007717  MAE: 0.005987  R²: 0.007910  DirAcc: 50.00%\n",
      "Window processed in 50.45 seconds\n",
      "\n",
      "Training window starting 2014\n",
      "MSE: 0.000073  RMSE: 0.008526  MAE: 0.006759  R²: -0.007721  DirAcc: 49.60%\n",
      "Window processed in 54.03 seconds\n",
      "\n",
      "Training window starting 2015\n",
      "MSE: 0.000117  RMSE: 0.010813  MAE: 0.008163  R²: 0.006268  DirAcc: 57.54%\n",
      "Window processed in 68.61 seconds\n",
      "\n",
      "Training window starting 2016\n",
      "MSE: 0.000101  RMSE: 0.010060  MAE: 0.007444  R²: -0.030227  DirAcc: 54.37%\n",
      "Window processed in 73.15 seconds\n",
      "\n",
      "Training window starting 2017\n",
      "MSE: 0.000038  RMSE: 0.006199  MAE: 0.004900  R²: -0.008295  DirAcc: 52.19%\n",
      "Window processed in 72.16 seconds\n",
      "\n",
      "Training window starting 2018\n",
      "MSE: 0.000090  RMSE: 0.009507  MAE: 0.007267  R²: 0.012898  DirAcc: 52.59%\n",
      "Window processed in 53.92 seconds\n",
      "\n",
      "Training window starting 2019\n",
      "MSE: 0.000051  RMSE: 0.007163  MAE: 0.005464  R²: -0.010436  DirAcc: 59.52%\n",
      "Window processed in 67.06 seconds\n",
      "\n",
      "Training window starting 2020\n",
      "MSE: 0.000646  RMSE: 0.025418  MAE: 0.015350  R²: -0.087659  DirAcc: 49.41%\n",
      "Window processed in 68.00 seconds\n",
      "\n",
      "Training window starting 2021\n",
      "MSE: 0.000086  RMSE: 0.009273  MAE: 0.007393  R²: 0.013485  DirAcc: 52.78%\n",
      "Window processed in 51.08 seconds\n",
      "\n",
      "Training window starting 2022\n",
      "MSE: 0.000182  RMSE: 0.013483  MAE: 0.010366  R²: -0.017063  DirAcc: 49.00%\n",
      "Window processed in 48.65 seconds\n",
      "\n",
      "Training window starting 2023\n",
      "MSE: 0.000127  RMSE: 0.011271  MAE: 0.008519  R²: -0.003552  DirAcc: 50.80%\n",
      "Window processed in 51.38 seconds\n",
      "\n",
      "Training window starting 2024\n",
      "MSE: 0.000095  RMSE: 0.009765  MAE: 0.007654  R²: -0.021743  DirAcc: 48.47%\n",
      "Window processed in 71.13 seconds\n",
      "Stage 1 completed and data saved for Stage 2.\n"
     ]
    }
   ],
   "source": [
    "# stage 1 training and prediction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import ta\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "random.seed(GLOBAL_SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(GLOBAL_SEED)\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "# ------------------------------------------------------------------------\n",
    "# 0. Load and align data\n",
    "# ------------------------------------------------------------------------\n",
    "factors = pd.read_csv(\"aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns = pd.read_csv(\"daily_returns_10ETFs.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Align dates to ensure matching indices\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors = factors.loc[dates]\n",
    "returns = returns.loc[dates]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1. Compute technical indicators and lagged features per ETF\n",
    "# ------------------------------------------------------------------------\n",
    "all_tech_features = []\n",
    "\n",
    "for etf in returns.columns:\n",
    "    close = (1 + returns[etf]).cumprod()\n",
    "    tech_df = pd.DataFrame(index=returns.index)\n",
    "\n",
    "    # Selected indicators (others commented out to reduce noise)\n",
    "    tech_df[f'{etf}_SMA_5']   = ta.trend.sma_indicator(close, window=5)\n",
    "    tech_df[f'{etf}_EMA_12']  = ta.trend.ema_indicator(close, window=12)\n",
    "    tech_df[f'{etf}_RSI_7']   = ta.momentum.rsi(close, window=7)\n",
    "    tech_df[f'{etf}_MACD']    = ta.trend.macd_diff(close)\n",
    "    tech_df[f'{etf}_ATR']     = ta.volatility.average_true_range(\n",
    "        high=close * 1.01, low=close * 0.99, close=close, window=10\n",
    "    )\n",
    "    tech_df[f'{etf}_Vol_5']   = returns[etf].rolling(window=5).std()\n",
    "    tech_df[f'{etf}_Mom_3']   = returns[etf].rolling(window=3).mean()\n",
    "\n",
    "    # Lagged returns (shifted so only past information is used)\n",
    "    for lag in [1, 2, 3]:\n",
    "        tech_df[f'{etf}_LagRet_{lag}'] = returns[etf].shift(lag)\n",
    "\n",
    "    all_tech_features.append(tech_df)\n",
    "\n",
    "# Concatenate technical indicators for all ETFs\n",
    "technical_features = pd.concat(all_tech_features, axis=1)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2. Create lagged factor features\n",
    "# ------------------------------------------------------------------------\n",
    "for factor in ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']:\n",
    "    for lag in [1, 2, 3]:\n",
    "        factors[f'{factor}_lag_{lag}'] = factors[factor].shift(lag)\n",
    "\n",
    "# Drop rows with NA values arising from lagging\n",
    "factors = factors.dropna()\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3. Combine factors, technical features, and VIX change\n",
    "# ------------------------------------------------------------------------\n",
    "features = pd.concat([factors, technical_features], axis=1).dropna()\n",
    "vix = pd.read_csv(\"VIX_History.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Align VIX to our feature dates and compute lagged change\n",
    "vix_aligned = vix['CLOSE'].reindex(features.index).ffill()\n",
    "features['VIX'] = vix_aligned.pct_change(fill_method=None).shift(1)\n",
    "features['VIX'] = features['VIX'].fillna(0)\n",
    "\n",
    "# Define the target: next-day return per ETF\n",
    "target_returns = returns.shift(-1).loc[features.index].dropna()\n",
    "features = features.loc[target_returns.index]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4. Define rolling window parameters\n",
    "# ------------------------------------------------------------------------\n",
    "train_years = 12      # years used for training\n",
    "valid_years = 1       # years used for validation\n",
    "test_years  = 1       # years used for testing/prediction\n",
    "retrain_frequency = 1 # years between retrainings\n",
    "start_year = 2009\n",
    "end_year   = 2024\n",
    "\n",
    "# List generic features used for SHAP importance ranking\n",
    "all_generic_features = [\n",
    "    'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA',\n",
    "    'Mkt-RF_lag_1', 'Mkt-RF_lag_2', 'Mkt-RF_lag_3',\n",
    "    'SMB_lag_1', 'SMB_lag_2', 'SMB_lag_3',\n",
    "    'HML_lag_1', 'HML_lag_2', 'HML_lag_3',\n",
    "    'RMW_lag_1', 'RMW_lag_2', 'RMW_lag_3',\n",
    "    'CMA_lag_1', 'CMA_lag_2', 'CMA_lag_3',\n",
    "    'SMA_5', 'EMA_12', 'RSI_7', 'MACD',\n",
    "    'Vol_5', 'Mom_3',\n",
    "    'LagRet_1', 'LagRet_2', 'LagRet_3', 'VIX'\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 5. Compute generic feature importance via SHAP\n",
    "#    (aggregated across ETFs, using only the initial training window)\n",
    "# ------------------------------------------------------------------------\n",
    "shap_importances = pd.DataFrame(0.0, index=all_generic_features, columns=['SHAP_Value'])\n",
    "\n",
    "# Use a fixed period (e.g. up to year 2009) for computing importances\n",
    "base_train_start = pd.Timestamp(start_year - train_years, 1, 1)\n",
    "base_train_end   = pd.Timestamp(start_year - valid_years - 1, 12, 31)\n",
    "\n",
    "for etf in returns.columns:\n",
    "    print(f\"Computing SHAP importances for ETF: {etf}\")\n",
    "    # Filter columns relevant to this ETF (generic + factor features)\n",
    "    etf_cols = [\n",
    "        col for col in features.columns\n",
    "        if (etf in col and any(k in col for k in ['SMA_5', 'EMA_12', 'RSI_7',\n",
    "                                                  'MACD', 'Vol_5', 'Mom_3',\n",
    "                                                  'LagRet_1','LagRet_2','LagRet_3', 'VIX']))\n",
    "        or col in ['Mkt-RF','SMB','HML','RMW','CMA',\n",
    "                   'Mkt-RF_lag_1','Mkt-RF_lag_2','Mkt-RF_lag_3',\n",
    "                   'SMB_lag_1','SMB_lag_2','SMB_lag_3',\n",
    "                   'HML_lag_1','HML_lag_2','HML_lag_3',\n",
    "                   'RMW_lag_1','RMW_lag_2','RMW_lag_3',\n",
    "                   'CMA_lag_1','CMA_lag_2','CMA_lag_3']\n",
    "    ]\n",
    "    X_base  = features.loc[base_train_start:base_train_end, etf_cols]\n",
    "    y_base  = target_returns[etf].loc[base_train_start:base_train_end]\n",
    "\n",
    "    # Fit a quick model to compute SHAP\n",
    "    model_base = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        tree_method='hist',\n",
    "        random_state=GLOBAL_SEED,\n",
    "        seed=GLOBAL_SEED,\n",
    "        device='cuda'\n",
    "    ).fit(X_base, y_base)\n",
    "\n",
    "    explainer_base = shap.Explainer(model_base)\n",
    "    shap_vals = explainer_base(X_base)\n",
    "\n",
    "    # Aggregate SHAP values per generic feature\n",
    "    for gen_feat in all_generic_features:\n",
    "        cols = [c for c in X_base.columns if gen_feat in c]\n",
    "        if cols:\n",
    "            idx = [X_base.columns.get_loc(c) for c in cols]\n",
    "            shap_importances.loc[gen_feat] += np.mean(np.abs(shap_vals.values[:, idx]))\n",
    "\n",
    "# Average importance across ETFs and select top N\n",
    "shap_importances /= len(returns.columns)\n",
    "top_generic_features = (\n",
    "    shap_importances.sort_values('SHAP_Value', ascending=False)\n",
    "                    .head(40)\n",
    "                    .index\n",
    "                    .tolist()\n",
    ")\n",
    "\n",
    "# returns = returns.iloc[:,:2]\n",
    "# ------------------------------------------------------------------------\n",
    "# 6. Retrain models using the selected generic features in rolling windows\n",
    "# ------------------------------------------------------------------------\n",
    "all_predictions = []\n",
    "\n",
    "# for etf in returns.columns:\n",
    "#     print(f\"\\n==== Training models for ETF: {etf} ====\")\n",
    "#     # Select columns containing any of the top_generic_features or factor names\n",
    "#     selected_features = [\n",
    "#         f for f in features.columns\n",
    "#         if any(gen in f for gen in top_generic_features) or f in factors.columns\n",
    "#     ]\n",
    "\n",
    "for etf in returns.columns:\n",
    "    print(f\"\\n==== Training models for ETF: {etf} ====\")\n",
    "\n",
    "    # Select features explicitly relevant to current ETF\n",
    "    # selected_features = [\n",
    "    #     f for f in features.columns\n",
    "    #     if (\n",
    "    #         # Include ETF-specific technical indicators explicitly\n",
    "    #         (any(gen in f for gen in top_generic_features) and (etf in f))\n",
    "    #         # Include ONLY explicitly selected generic FF factors or their lags\n",
    "    #         or (any(gen == f for gen in top_generic_features))\n",
    "    #     )\n",
    "    # ]\n",
    "    \n",
    "    selected_features = []\n",
    "\n",
    "    for feature in top_generic_features:\n",
    "        # Clearly check if the feature is ETF-specific (technical indicators)\n",
    "        etf_specific_feature_name = f'{etf}_{feature}'\n",
    "        \n",
    "        # Add ETF-specific feature explicitly if present in columns\n",
    "        if etf_specific_feature_name in features.columns:\n",
    "            selected_features.append(etf_specific_feature_name)\n",
    "        \n",
    "        # If not ETF-specific, explicitly add generic factor features directly\n",
    "        elif feature in features.columns:\n",
    "            selected_features.append(feature)\n",
    "    \n",
    "    # Sanity check to ensure you have valid selected features\n",
    "    if not selected_features:\n",
    "        raise ValueError(f\"No features selected for {etf}, please verify feature names.\")\n",
    "\n",
    "    print(f\"Selected features for {etf}: {selected_features}\")\n",
    "    \n",
    "    year = start_year\n",
    "    while year <= end_year - test_years + 1:\n",
    "        print(f\"\\nTraining window starting {year}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Define periods\n",
    "        train_start = pd.Timestamp(year - train_years, 1, 1)\n",
    "        train_end   = pd.Timestamp(year - valid_years - 1, 12, 31)\n",
    "        valid_start = pd.Timestamp(year - valid_years, 1, 1)\n",
    "        valid_end   = pd.Timestamp(year - 1, 12, 31)\n",
    "        test_start  = pd.Timestamp(year, 1, 1)\n",
    "        test_end    = pd.Timestamp(year + test_years - 1, 12, 31)\n",
    "\n",
    "        # Extract data\n",
    "        X_train = features.loc[train_start:train_end, selected_features]\n",
    "        y_train = target_returns[etf].loc[train_start:train_end]\n",
    "        X_valid = features.loc[valid_start:valid_end, selected_features]\n",
    "        y_valid = target_returns[etf].loc[valid_start:valid_end]\n",
    "        X_test  = features.loc[test_start:test_end, selected_features]\n",
    "        y_test  = target_returns[etf].loc[test_start:test_end]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_valid_scaled = scaler.transform(X_valid)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # # Base model with early stopping\n",
    "        # base_model = xgb.XGBRegressor(\n",
    "        #     objective='reg:squarederror',\n",
    "        #     tree_method='hist',\n",
    "        #     device='cuda',\n",
    "        #     random_state=42,\n",
    "        #     n_jobs=4,\n",
    "        #     # eval_metric='rmse', # The metric to monitor for early stopping\n",
    "        #     # early_stopping_rounds=50\n",
    "        # )\n",
    "        # \n",
    "        # param_grid = {\n",
    "        #     'n_estimators': [200, 400],\n",
    "        #     'max_depth': [3, 4, 5],\n",
    "        #     'learning_rate': [0.03, 0.05],\n",
    "        #     'subsample': [0.7, 0.8],\n",
    "        #     'colsample_bytree': [0.7, 0.8]\n",
    "        # }\n",
    "        # \n",
    "        # tscv = TimeSeriesSplit(n_splits=3)\n",
    "        # \n",
    "        # grid_search = GridSearchCV(\n",
    "        #     base_model,\n",
    "        #     param_grid,\n",
    "        #     cv=tscv,\n",
    "        #     scoring='neg_mean_squared_error',\n",
    "        #     verbose=0,\n",
    "        #     n_jobs=4\n",
    "        # )\n",
    "        # \n",
    "        # # Fit with early stopping on the explicit validation set\n",
    "        # # fit_params = {\n",
    "        # #     \"eval_set\": [(X_valid, y_valid)],\n",
    "        # #     \"verbose\": False\n",
    "        # # }\n",
    "        # # \n",
    "        # # # grid_search.fit(X_train, y_train, **fit_params)\n",
    "        # # grid_search.fit(X_train_scaled, y_train)\n",
    "        # \n",
    "        # fit_params = {\n",
    "        #     'eval_set': [(X_valid_scaled, y_valid)],\n",
    "        #     'eval_metric': 'rmse',\n",
    "        #     'early_stopping_rounds': 50,\n",
    "        #     'verbose': False\n",
    "        # }\n",
    "        # \n",
    "        # grid_search.fit(X_train_scaled, y_train, **fit_params)\n",
    "        # \n",
    "        # best_model = grid_search.best_estimator_\n",
    "        # \n",
    "        # # Predict on the test period\n",
    "        # preds = best_model.predict(X_test_scaled)\n",
    "\n",
    "        \n",
    "        # Define your parameter grid explicitly\n",
    "        param_grid = {\n",
    "            'n_estimators': [200, 400],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'learning_rate': [0.03, 0.05],\n",
    "            'subsample': [0.7, 0.8],\n",
    "            'colsample_bytree': [0.7, 0.8]\n",
    "        }\n",
    "        \n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        \n",
    "        best_score = float('inf')\n",
    "        best_params = None\n",
    "        best_model = None\n",
    "        \n",
    "        # Explicit loop for parameter search and cross-validation\n",
    "        for params in ParameterGrid(param_grid):\n",
    "            cv_rmse = []\n",
    "        \n",
    "            for train_idx, val_idx in tscv.split(X_train_scaled):\n",
    "                X_fold_train, X_fold_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "                y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "                # DMatrix explicitly required by XGBoost native API\n",
    "                dtrain = xgb.DMatrix(X_fold_train, label=y_fold_train)\n",
    "                dval = xgb.DMatrix(X_fold_val, label=y_fold_val)\n",
    "        \n",
    "                # Setup watchlist explicitly for early stopping\n",
    "                watchlist = [(dtrain, 'train'), (dval, 'validation')]\n",
    "        \n",
    "                xgb_params = {\n",
    "                    'objective': 'reg:squarederror',\n",
    "                    'tree_method': 'hist',\n",
    "                    'device': 'cuda',\n",
    "                    'eval_metric': 'rmse',\n",
    "                    'seed': 42,\n",
    "                    'max_depth': params['max_depth'],\n",
    "                    'learning_rate': params['learning_rate'],\n",
    "                    'subsample': params['subsample'],\n",
    "                    'colsample_bytree': params['colsample_bytree']\n",
    "                }\n",
    "        \n",
    "                # Explicitly train with early stopping\n",
    "                model = xgb.train(\n",
    "                    xgb_params,\n",
    "                    dtrain,\n",
    "                    num_boost_round=params['n_estimators'],\n",
    "                    evals=watchlist,\n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose_eval=False\n",
    "                )\n",
    "        \n",
    "                preds = model.predict(dval)\n",
    "                rmse = np.sqrt(mean_squared_error(y_fold_val, preds))\n",
    "                cv_rmse.append(rmse)\n",
    "        \n",
    "            avg_rmse = np.mean(cv_rmse)\n",
    "            # print(f\"Params: {params}, CV Avg RMSE: {avg_rmse:.6f}\")\n",
    "        \n",
    "            if avg_rmse < best_score:\n",
    "                best_score = avg_rmse\n",
    "                best_params = params\n",
    "                best_model = model\n",
    "        \n",
    "        # Train final model explicitly with best parameters on full training data\n",
    "        dtrain_full = xgb.DMatrix(X_train_scaled, label=y_train)\n",
    "        dvalid_full = xgb.DMatrix(X_valid_scaled, label=y_valid)\n",
    "        \n",
    "        watchlist_full = [(dtrain_full, 'train'), (dvalid_full, 'validation')]\n",
    "        \n",
    "        final_xgb_params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'tree_method': 'hist',\n",
    "            'device': 'cuda',\n",
    "            'eval_metric': 'rmse',\n",
    "            'seed': 42,\n",
    "            'max_depth': best_params['max_depth'],\n",
    "            'learning_rate': best_params['learning_rate'],\n",
    "            'subsample': best_params['subsample'],\n",
    "            'colsample_bytree': best_params['colsample_bytree']\n",
    "        }\n",
    "        \n",
    "        best_model = xgb.train(\n",
    "            final_xgb_params,\n",
    "            dtrain_full,\n",
    "            num_boost_round=best_params['n_estimators'],\n",
    "            evals=watchlist_full,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Predict explicitly on test data\n",
    "        dtest = xgb.DMatrix(X_test_scaled)\n",
    "        preds = best_model.predict(dtest)\n",
    "        \n",
    "        # Metrics clearly\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        # print(f\"Test RMSE: {test_rmse:.6f}\")\n",
    "        \n",
    "        # Compute evaluation metrics\n",
    "        mse  = mean_squared_error(y_test, preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae  = mean_absolute_error(y_test, preds)\n",
    "        r2   = r2_score(y_test, preds)\n",
    "        dir_acc = np.mean((np.sign(y_test) == np.sign(preds)).astype(int))\n",
    "\n",
    "        print(f\"MSE: {mse:.6f}  RMSE: {rmse:.6f}  MAE: {mae:.6f}  \"\n",
    "              f\"R²: {r2:.6f}  DirAcc: {dir_acc:.2%}\")\n",
    "\n",
    "        # Save the model for reproducibility\n",
    "        joblib.dump(best_model, f\"best_model_{etf}_{year}.joblib\")\n",
    "\n",
    "        # Save predictions\n",
    "        preds_df = pd.DataFrame({\n",
    "            'Date': X_test.index,\n",
    "            'ETF': etf,\n",
    "            'Year': year,\n",
    "            'Actual_Return': y_test,\n",
    "            'Predicted_Return': preds\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "        # Compute SHAP values on the test set\n",
    "        explainer_test = shap.Explainer(best_model, feature_names=X_test.columns)\n",
    "        shap_vals_test = explainer_test(X_test_scaled)\n",
    "\n",
    "        clean_shap_cols = [\n",
    "            f'SHAP_{col.replace(f\"{etf}_\", \"\")}' if col.startswith(f'{etf}_') else f'SHAP_{col}'\n",
    "            for col in X_test.columns\n",
    "        ]\n",
    "        \n",
    "        shap_df = pd.DataFrame(\n",
    "            shap_vals_test.values,\n",
    "            columns=clean_shap_cols,\n",
    "            # columns=[f'SHAP_{col}' for col in X_test.columns],\n",
    "            index=X_test.index\n",
    "        ).reset_index().rename(columns={'index': 'Date'})\n",
    "\n",
    "        # Merge SHAP values with predictions\n",
    "        preds_df = preds_df.merge(shap_df, on='Date', how='left')\n",
    "\n",
    "        all_predictions.append(preds_df)\n",
    "\n",
    "        # Advance the window\n",
    "        year += retrain_frequency\n",
    "        print(f\"Window processed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Concatenate and save all predictions and SHAP values\n",
    "final_predictions_df = pd.concat(all_predictions, ignore_index=True)\n",
    "final_predictions_df.to_csv(\"stage1_predictions_with_shap_10ETFs.csv\", index=False)\n",
    "\n",
    "print(\"Stage 1 completed and data saved for Stage 2.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T03:43:03.303147Z",
     "start_time": "2025-08-04T01:56:21.552413Z"
    }
   },
   "id": "8f0500c59fdc3167",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['EMA_12',\n 'RSI_7',\n 'SMB_lag_3',\n 'RMW_lag_3',\n 'SMA_5',\n 'Mkt-RF_lag_2',\n 'Mkt-RF_lag_3',\n 'SMB_lag_2',\n 'Mkt-RF',\n 'SMB']"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_generic_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-03T18:23:01.287994Z",
     "start_time": "2025-08-03T18:23:01.270976Z"
    }
   },
   "id": "c449d0418b87114f",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3797229969.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[36], line 2\u001B[1;36m\u001B[0m\n\u001B[1;33m    if etf in f and if f in top_generic_features]\u001B[0m\n\u001B[1;37m                    ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T00:45:03.359619Z",
     "start_time": "2025-08-04T00:45:03.348617Z"
    }
   },
   "id": "46b46b2e610dc12b",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features for VZ: ['VZ_LagRet_1', 'HML_lag_2', 'VZ_LagRet_2', 'VZ_Vol_5', 'VZ_Mom_3', 'VZ_LagRet_3', 'SMB_lag_2', 'VZ_MACD', 'Mkt-RF', 'HML']\n"
     ]
    }
   ],
   "source": [
    "selected_features = []\n",
    "\n",
    "for feature in top_generic_features:\n",
    "    # Clearly check if the feature is ETF-specific (technical indicators)\n",
    "    etf_specific_feature_name = f'{etf}_{feature}'\n",
    "    \n",
    "    # Add ETF-specific feature explicitly if present in columns\n",
    "    if etf_specific_feature_name in features.columns:\n",
    "        selected_features.append(etf_specific_feature_name)\n",
    "    \n",
    "    # If not ETF-specific, explicitly add generic factor features directly\n",
    "    elif feature in features.columns:\n",
    "        selected_features.append(feature)\n",
    "\n",
    "# Sanity check to ensure you have valid selected features\n",
    "if not selected_features:\n",
    "    raise ValueError(f\"No features selected for {etf}, please verify feature names.\")\n",
    "\n",
    "print(f\"Selected features for {etf}: {selected_features}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T00:48:54.452966Z",
     "start_time": "2025-08-04T00:48:54.402955Z"
    }
   },
   "id": "ea958cf59209ccad",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                 XLB\nDate                \n1999-01-05  0.024531\n1999-01-06  0.017606\n1999-01-07 -0.007612\n1999-01-08  0.034170\n1999-01-11  0.002023\n...              ...\n2024-11-21  0.012224\n2024-11-22  0.005557\n2024-11-25  0.010309\n2024-11-26 -0.007469\n2024-11-27 -0.000530\n\n[6518 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>XLB</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1999-01-05</th>\n      <td>0.024531</td>\n    </tr>\n    <tr>\n      <th>1999-01-06</th>\n      <td>0.017606</td>\n    </tr>\n    <tr>\n      <th>1999-01-07</th>\n      <td>-0.007612</td>\n    </tr>\n    <tr>\n      <th>1999-01-08</th>\n      <td>0.034170</td>\n    </tr>\n    <tr>\n      <th>1999-01-11</th>\n      <td>0.002023</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2024-11-21</th>\n      <td>0.012224</td>\n    </tr>\n    <tr>\n      <th>2024-11-22</th>\n      <td>0.005557</td>\n    </tr>\n    <tr>\n      <th>2024-11-25</th>\n      <td>0.010309</td>\n    </tr>\n    <tr>\n      <th>2024-11-26</th>\n      <td>-0.007469</td>\n    </tr>\n    <tr>\n      <th>2024-11-27</th>\n      <td>-0.000530</td>\n    </tr>\n  </tbody>\n</table>\n<p>6518 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-03T18:03:33.363860Z",
     "start_time": "2025-08-03T18:03:33.353858Z"
    }
   },
   "id": "4bc45fc95e8410dc",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'colsample_bytree': 0.8,\n 'learning_rate': 0.03,\n 'max_depth': 3,\n 'n_estimators': 200,\n 'subsample': 0.7}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-03T03:13:35.192475Z",
     "start_time": "2025-08-03T03:13:35.187458Z"
    }
   },
   "id": "9a1973a72e42310b",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6efb3e1a4faf0b68",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated DataFrame shape: (3999, 100)\n",
      "Aggregated DataFrame summary stats:\n",
      "                       count                           mean  \\\n",
      "Date                    3999  2016-12-17 07:15:20.930232832   \n",
      "Predicted_Return_XLB  3999.0                       0.000149   \n",
      "Actual_Return_XLB     3999.0                       0.000439   \n",
      "Volatility_XLB        3999.0                       0.011625   \n",
      "Predicted_Return_XLE  3999.0                       0.000886   \n",
      "...                      ...                            ...   \n",
      "Rank_PredRet_XLK      3999.0                       0.528799   \n",
      "Rank_PredRet_XLP      3999.0                       0.492234   \n",
      "Rank_PredRet_XLY      3999.0                       0.685088   \n",
      "Rank_PredRet_XLV      3999.0                       0.536384   \n",
      "Rank_PredRet_XLU      3999.0                        0.53669   \n",
      "\n",
      "                                      min                  25%  \\\n",
      "Date                  2009-01-08 00:00:00  2012-12-27 12:00:00   \n",
      "Predicted_Return_XLB            -0.028701             0.000124   \n",
      "Actual_Return_XLB               -0.110084            -0.006411   \n",
      "Volatility_XLB                   0.001177             0.006721   \n",
      "Predicted_Return_XLE            -0.022169            -0.000019   \n",
      "...                                   ...                  ...   \n",
      "Rank_PredRet_XLK                 0.111111             0.222222   \n",
      "Rank_PredRet_XLP                 0.111111             0.222222   \n",
      "Rank_PredRet_XLY                 0.111111             0.555556   \n",
      "Rank_PredRet_XLV                 0.111111             0.222222   \n",
      "Rank_PredRet_XLU                 0.111111             0.333333   \n",
      "\n",
      "                                      50%                  75%  \\\n",
      "Date                  2016-12-15 00:00:00  2020-12-05 12:00:00   \n",
      "Predicted_Return_XLB             0.000293             0.000396   \n",
      "Actual_Return_XLB                0.000831             0.007916   \n",
      "Volatility_XLB                   0.009804             0.014372   \n",
      "Predicted_Return_XLE             0.000373             0.000624   \n",
      "...                                   ...                  ...   \n",
      "Rank_PredRet_XLK                 0.555556             0.777778   \n",
      "Rank_PredRet_XLP                 0.444444             0.666667   \n",
      "Rank_PredRet_XLY                 0.777778             0.888889   \n",
      "Rank_PredRet_XLV                 0.555556             0.777778   \n",
      "Rank_PredRet_XLU                 0.555556             0.777778   \n",
      "\n",
      "                                      max       std  \n",
      "Date                  2024-11-26 00:00:00       NaN  \n",
      "Predicted_Return_XLB             0.022071  0.001942  \n",
      "Actual_Return_XLB                0.117601  0.013831  \n",
      "Volatility_XLB                   0.087182  0.007675  \n",
      "Predicted_Return_XLE             0.062615  0.004786  \n",
      "...                                   ...       ...  \n",
      "Rank_PredRet_XLK                      1.0  0.304443  \n",
      "Rank_PredRet_XLP                      1.0  0.257654  \n",
      "Rank_PredRet_XLY                      1.0  0.228208  \n",
      "Rank_PredRet_XLV                      1.0   0.29847  \n",
      "Rank_PredRet_XLU                      1.0  0.248351  \n",
      "\n",
      "[100 rows x 8 columns]\n",
      "Optimized Stage 2 RL dataset successfully saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load stage 1 predictions with SHAP values explicitly\n",
    "stage1_df = pd.read_csv(\"stage1_predictions_with_shap_10ETFs.csv\", parse_dates=['Date'])\n",
    "etfs = stage1_df['ETF'].unique()\n",
    "\n",
    "# Initialize DataFrame explicitly for aggregated daily data\n",
    "dates = sorted(stage1_df['Date'].unique())\n",
    "aggregated_data = pd.DataFrame({'Date': dates})\n",
    "\n",
    "# Pivot tables for efficient cross-sectional computations\n",
    "predicted_returns = stage1_df.pivot(index='Date', columns='ETF', values='Predicted_Return')\n",
    "actual_returns = stage1_df.pivot(index='Date', columns='ETF', values='Actual_Return')\n",
    "\n",
    "# Compute ETF-specific volatility (rolling 5-day window)\n",
    "volatility = actual_returns.rolling(window=5).std()\n",
    "\n",
    "# Merge explicitly into aggregated_data\n",
    "for etf in etfs:\n",
    "    aggregated_data[f'Predicted_Return_{etf}'] = aggregated_data['Date'].map(predicted_returns[etf])\n",
    "    aggregated_data[f'Actual_Return_{etf}'] = aggregated_data['Date'].map(actual_returns[etf])\n",
    "    aggregated_data[f'Volatility_{etf}'] = aggregated_data['Date'].map(volatility[etf])\n",
    "\n",
    "# Dynamically load the top generic features from Stage 1 explicitly to maintain consistency\n",
    "# generic_shap_features = ['LagRet_1',\n",
    "#  'HML_lag_2',\n",
    "#  'LagRet_2',\n",
    "#  'Vol_5',\n",
    "#  'Mom_3',\n",
    "#  'LagRet_3',\n",
    "#  'SMB_lag_2',\n",
    "#  'MACD',\n",
    "#  'Mkt-RF',\n",
    "#  'HML']\n",
    "generic_shap_features = top_generic_features\n",
    "\n",
    "# Aggregate SHAP values (mean and std across ETFs) explicitly by generic feature\n",
    "shap_aggregated_features = {}\n",
    "\n",
    "for feature in generic_shap_features:\n",
    "    matching_cols = [col for col in stage1_df.columns \n",
    "                     if col.startswith('SHAP_') and col.endswith(feature)]\n",
    "\n",
    "    if matching_cols:\n",
    "        shap_means = stage1_df.groupby('Date')[matching_cols].mean().mean(axis=1)\n",
    "        shap_stds = stage1_df.groupby('Date')[matching_cols].std().mean(axis=1)\n",
    "\n",
    "        shap_aggregated_features[f'Avg_SHAP_{feature}'] = shap_means\n",
    "        shap_aggregated_features[f'Std_SHAP_{feature}'] = shap_stds\n",
    "    else:\n",
    "        print(f\"Warning: No matches found for SHAP feature: {feature}\")\n",
    "\n",
    "# Convert aggregated SHAP features explicitly to DataFrame\n",
    "shap_aggregated_df = pd.DataFrame(shap_aggregated_features).reset_index()\n",
    "\n",
    "# Merge aggregated SHAP features explicitly\n",
    "aggregated_data = pd.merge(aggregated_data, shap_aggregated_df, on='Date', how='left')\n",
    "\n",
    "# Explicitly compute additional cross-sectional signals for richer Stage 2 observations\n",
    "# Cross-sectional mean and std of predicted returns\n",
    "aggregated_data['CrossSec_Mean_PredRet'] = predicted_returns.mean(axis=1).values\n",
    "aggregated_data['CrossSec_Std_PredRet'] = predicted_returns.std(axis=1).values\n",
    "\n",
    "# Cross-sectional mean volatility\n",
    "aggregated_data['CrossSec_Mean_Volatility'] = volatility.mean(axis=1).values\n",
    "\n",
    "# Rank ETFs by predicted return explicitly (percentile ranks)\n",
    "ranked_preds = predicted_returns.rank(axis=1, pct=True)\n",
    "for etf in etfs:\n",
    "    aggregated_data[f'Rank_PredRet_{etf}'] = aggregated_data['Date'].map(ranked_preds[etf])\n",
    "\n",
    "# Handle missing values explicitly and robustly:\n",
    "# Forward-fill only SHAP and cross-sectional features explicitly\n",
    "shap_and_crosssec_cols = [col for col in aggregated_data.columns if 'SHAP' in col or 'CrossSec' in col]\n",
    "aggregated_data[shap_and_crosssec_cols] = aggregated_data[shap_and_crosssec_cols].ffill()\n",
    "\n",
    "# Drop rows explicitly where ETF volatility calculations have initial NaNs\n",
    "vol_cols = [f'Volatility_{etf}' for etf in etfs]\n",
    "aggregated_data.dropna(subset=vol_cols, inplace=True)\n",
    "\n",
    "# Final sanity checks explicitly for data quality assurance\n",
    "if aggregated_data.empty:\n",
    "    raise ValueError(\"Aggregated dataset is empty after preprocessing. Verify your input data.\")\n",
    "else:\n",
    "    # Quick summary statistics explicitly for diagnostics\n",
    "    print(\"Aggregated DataFrame shape:\", aggregated_data.shape)\n",
    "    print(\"Aggregated DataFrame summary stats:\")\n",
    "    print(aggregated_data.describe().transpose())\n",
    "\n",
    "    # Save optimized data explicitly for Stage 2\n",
    "    aggregated_data.to_csv(\"stage2_rl_observations_optimized_10ETFs.csv\", index=False)\n",
    "    print(\"Optimized Stage 2 RL dataset successfully saved.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T03:45:24.832889Z",
     "start_time": "2025-08-04T03:45:23.471545Z"
    }
   },
   "id": "626373434f64d0c4",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d8f3f0813e659ead",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting iteration 1/25 (Seed: 42) at 2025-08-03 23:47:49\n",
      "  - Starting window 1/8 at 2025-08-03 23:47:49\n"
     ]
    }
   ],
   "source": [
    "# fine tune to use change‑based actions\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "import time\n",
    "from gymnasium import spaces\n",
    "import gc\n",
    "# ---------------------------------------------------------------------\n",
    "# Configuration dataclass\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    train_window_days: int = 252 * 7      # 10 years for training\n",
    "    validation_window_days: int = 252      # ~6 months for validation\n",
    "    prediction_window_days: int = 252      # ~6 months for prediction\n",
    "    lookback_period: int = 21              # lookback for observations\n",
    "    rebalance_period: int = 21             # rebalance every 10 days\n",
    "    n_iter_tuning: int = 20                # number of hyperparameter samples\n",
    "    tuning_timesteps: int = 10_000          # timesteps for each tune\n",
    "    incremental_timesteps: int = 10_000     # PPO training step size\n",
    "    max_timesteps: int = 50_000            # maximum PPO timesteps\n",
    "    patience: int = 3                      # early stopping patience\n",
    "    policy_arch: Tuple[int, int] = (256, 256)  # network architecture\n",
    "    num_iterations: int = 25                # number of outer iterations (seeds)\n",
    "    base_seed: int = 42                    # base random seed\n",
    "    default_risk_coeff: float = 0.5        # default risk coefficient\n",
    "    desired_long: float = 1.0       # Default no leverage, 100% allocation\n",
    "    desired_short: float = 0.0      # Default no short selling\n",
    "    weight_bounds: Tuple[float, float] = (0.0, 1.0)  # Default bounds [0,1] for no shorts\n",
    "    lambda_hhi: float = 0.1\n",
    "    lambda_turnover: float = 0.005\n",
    "    transaction_cost_rate: float = 0.0\n",
    "    model_retrain: bool  = False\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Seed-setting utility\n",
    "# ---------------------------------------------------------------------\n",
    "def set_global_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Feature engineering\n",
    "# ---------------------------------------------------------------------\n",
    "def add_stable_features(df: pd.DataFrame, etf_list: List[str]) -> pd.DataFrame:\n",
    "    data = df.copy()\n",
    "    for etf in etf_list:\n",
    "        price_col = f'Price_{etf}'\n",
    "        data[f'Volatility_{etf}'] = data[price_col].pct_change().rolling(20).std()\n",
    "        data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
    "        data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
    "        data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
    "        data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
    "        data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
    "        data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "def filter_features(df: pd.DataFrame,\n",
    "                    include_predicted_returns: bool = True,\n",
    "                    include_shap_metrics: bool = True) -> pd.DataFrame:\n",
    "    df_filtered = df.copy()\n",
    "    if not include_predicted_returns:\n",
    "        pred_cols = [c for c in df_filtered.columns if 'Predicted_Return' in c]\n",
    "        df_filtered.drop(columns=pred_cols, inplace=True)\n",
    "    if not include_shap_metrics:\n",
    "        shap_cols = [c for c in df_filtered.columns if 'SHAP' in c]\n",
    "        df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "    return df_filtered\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Custom Gym environment\n",
    "# ---------------------------------------------------------------------\n",
    "class PortfolioEnv(gym.Env):\n",
    "    metadata = {'render_modes': []}\n",
    "\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar',\n",
    "                 risk_coefficient=0.5, rebalance_period=21,\n",
    "                 lookback_period=21, weight_bounds=(0.0, 1.0),\n",
    "                 desired_long=1.0, desired_short=0.0,\n",
    "                 use_baseline=False, baseline_fn=None,\n",
    "                 transaction_cost_rate=0.0,\n",
    "                 lambda_turnover=0.001,   # <- Add explicitly\n",
    "                 lambda_hhi=0.1):         # <- Add explicitly\n",
    "        super().__init__()\n",
    "        self.transaction_cost_rate = transaction_cost_rate\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "        self.weight_bounds = weight_bounds\n",
    "        self.desired_long = desired_long       # Add this explicitly\n",
    "        self.desired_short = desired_short     # Add this explicitly\n",
    "        self.use_baseline = use_baseline\n",
    "        self.baseline_fn = baseline_fn\n",
    "        self.transaction_cost_rate = transaction_cost_rate\n",
    "        self.lambda_turnover = lambda_turnover\n",
    "        self.lambda_hhi = lambda_hhi\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0,\n",
    "                                       shape=(len(etf_list),), dtype=np.float32)\n",
    "        self.feature_cols = [c for c in data.columns\n",
    "                             if c != 'Date' and not c.startswith('Actual_Return')]\n",
    "        self.num_features_per_day = len(self.feature_cols)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.num_features_per_day * lookback_period,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = lookback_period\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(etf_list)] * len(etf_list))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(self.etf_list)] * len(self.etf_list))\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs_window = self.data.iloc[self.current_step - self.lookback_period : self.current_step]\n",
    "        obs_values = obs_window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "        \n",
    "        if np.isnan(obs_values).any() or np.isinf(obs_values).any():\n",
    "            obs_values = np.nan_to_num(obs_values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return obs_values\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns, turnover):\n",
    "        hhi = np.sum(np.square(np.abs(self.current_weights)))\n",
    "    \n",
    "        if np.isnan(portfolio_return) or np.isinf(portfolio_return):\n",
    "            portfolio_return = 0.0  # safeguard explicitly\n",
    "    \n",
    "        portfolio_return = np.clip(portfolio_return, -0.5, 0.5)  # explicitly clip returns\n",
    "    \n",
    "        if self.reward_type == 'cumulative_return':\n",
    "            base_reward = portfolio_return\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            base_reward = np.log(max(1 + portfolio_return, 1e-8))\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            base_reward = portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100 * alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            base_reward = portfolio_return - self.risk_coefficient * cvar\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid reward type: {self.reward_type}\")\n",
    "    \n",
    "        reward = base_reward \\\n",
    "                 - self.lambda_turnover * turnover \\\n",
    "                 - self.lambda_hhi * hhi\n",
    "    \n",
    "        if np.isnan(reward) or np.isinf(reward):\n",
    "            reward = -1.0  # explicit fallback\n",
    "    \n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        next_step = self.current_step + 1\n",
    "        prev_weights = self.current_weights.copy()\n",
    "    \n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            if self.use_baseline and self.baseline_fn is not None:\n",
    "                current_date = self.data.loc[self.current_step, 'Date']\n",
    "                baseline_w = self.baseline_fn(current_date)\n",
    "                raw = baseline_w * (1.0 + action)\n",
    "            else:\n",
    "                raw = self.current_weights + action\n",
    "    \n",
    "            long_w = np.maximum(raw, 0.0)\n",
    "            short_w = np.abs(np.minimum(raw, 0.0))\n",
    "    \n",
    "            has_long = long_w.sum() > 0\n",
    "            has_short = short_w.sum() > 0\n",
    "    \n",
    "            if has_long and has_short:\n",
    "                norm_long = self.desired_long * long_w / long_w.sum()\n",
    "                norm_short = self.desired_short * short_w / short_w.sum()\n",
    "                combined = norm_long - norm_short\n",
    "            elif has_long and not has_short:\n",
    "                # explicitly no leverage if no shorts\n",
    "                combined = long_w / long_w.sum()\n",
    "            elif not has_long and has_short:\n",
    "                # explicitly full short if no longs\n",
    "                combined = -short_w / short_w.sum()\n",
    "            else:\n",
    "                # fallback explicitly to equal weights\n",
    "                combined = np.ones(len(raw)) / len(raw)\n",
    "    \n",
    "            clipped = np.clip(combined, self.weight_bounds[0], self.weight_bounds[1])\n",
    "    \n",
    "            # After clipping explicitly re-normalize\n",
    "            long_c = np.maximum(clipped, 0.0)\n",
    "            short_c = np.abs(np.minimum(clipped, 0.0))\n",
    "    \n",
    "            if long_c.sum() > 0 and short_c.sum() > 0:\n",
    "                final_long = self.desired_long * long_c / long_c.sum()\n",
    "                final_short = self.desired_short * short_c / short_c.sum()\n",
    "                self.current_weights = final_long - final_short\n",
    "            elif long_c.sum() > 0:\n",
    "                self.current_weights = long_c / long_c.sum()\n",
    "            elif short_c.sum() > 0:\n",
    "                self.current_weights = -short_c / short_c.sum()\n",
    "            else:\n",
    "                self.current_weights = np.ones(len(raw)) / len(raw)\n",
    "    \n",
    "            turnover = np.sum(np.abs(self.current_weights - prev_weights))\n",
    "        else:\n",
    "            # Passive reweighting between rebalances\n",
    "            returns_today = np.array([\n",
    "                self.data.loc[self.current_step, f\"Actual_Return_{etf}\"]\n",
    "                for etf in self.etf_list\n",
    "            ])\n",
    "            self.current_weights *= (1.0 + returns_today)\n",
    "            self.current_weights /= np.sum(np.abs(self.current_weights))\n",
    "            turnover = 0.0\n",
    "    \n",
    "        # Check for termination\n",
    "        if next_step >= len(self.data):\n",
    "            reward = 0.0\n",
    "            terminated = True\n",
    "        else:\n",
    "            asset_returns = np.array([\n",
    "                self.data.loc[next_step, f\"Actual_Return_{etf}\"]\n",
    "                for etf in self.etf_list\n",
    "            ])\n",
    "            portfolio_return = np.dot(self.current_weights, asset_returns)\n",
    "            portfolio_return = np.nan_to_num(portfolio_return, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "            self.cumulative_wealth *= (1.0 + portfolio_return)\n",
    "            reward = self.calculate_reward(portfolio_return, asset_returns, turnover)\n",
    "            reward -= self.transaction_cost_rate * turnover\n",
    "            terminated = next_step >= len(self.data) - 1\n",
    "    \n",
    "        self.current_step += 1\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "    \n",
    "        # advance time and return observation, reward, termination flags\n",
    "        self.current_step += 1\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Hyperparameter tuning function\n",
    "# ---------------------------------------------------------------------\n",
    "def equal_weight_baseline(date):\n",
    "    return np.ones(len(etf_list)) / len(etf_list)\n",
    "\n",
    "def validate_and_tune(train_data: pd.DataFrame, val_data: pd.DataFrame,\n",
    "                      etf_list: List[str], cfg: TrainingConfig,\n",
    "                      random_seed: int) -> Dict[str, float]:\n",
    "\n",
    "    param_dist = {\n",
    "        'learning_rate': [5e-4, 1e-5, 5e-5],\n",
    "        'n_steps': [20, 40],\n",
    "        'batch_size': [10, 20],\n",
    "        'gamma': [0.95, 0.98],\n",
    "        'risk_coefficient': [0.1, 0.5, 1.0, 5.0],\n",
    "        'lambda_turnover': [0.005, 0.01, 0.05],\n",
    "        'lambda_hhi': [0.5, 1, 5],\n",
    "        'seed': [random_seed, random_seed + 11, random_seed + 23]  # explicitly vary seeds\n",
    "    }\n",
    "\n",
    "    # Crucial: explicitly pass random_seed to ParameterSampler\n",
    "    sampled_params = list(ParameterSampler(\n",
    "        param_dist, n_iter=cfg.n_iter_tuning, random_state=random_seed\n",
    "    ))\n",
    "\n",
    "    best_reward = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for params in sampled_params:\n",
    "        seed = params.pop('seed')\n",
    "        risk_coeff = params.pop('risk_coefficient', cfg.default_risk_coeff)\n",
    "        lambda_turnover = params.pop('lambda_turnover', cfg.lambda_turnover)\n",
    "        lambda_hhi = params.pop('lambda_hhi', cfg.lambda_hhi)\n",
    "        \n",
    "        set_global_seed(seed)\n",
    "\n",
    "\n",
    "        env = make_vec_env(lambda: PortfolioEnv(\n",
    "            train_data, etf_list, 'mean_cvar', risk_coeff,\n",
    "            cfg.rebalance_period, cfg.lookback_period,\n",
    "            use_baseline=True, baseline_fn=equal_weight_baseline,\n",
    "            transaction_cost_rate=0.0005,\n",
    "            lambda_turnover=lambda_turnover,\n",
    "            lambda_hhi=lambda_hhi\n",
    "        ), n_envs=1, seed=seed)\n",
    "\n",
    "        model = PPO('MlpPolicy', env, ent_coef=0.01, clip_range=0.2, seed=seed, **params, verbose=0)\n",
    "        model.learn(total_timesteps=cfg.tuning_timesteps)\n",
    "\n",
    "        # Evaluate explicitly on validation set\n",
    "        val_env = PortfolioEnv(\n",
    "            val_data, etf_list, 'mean_cvar', risk_coeff,\n",
    "            cfg.rebalance_period, cfg.lookback_period,\n",
    "            use_baseline=True, baseline_fn=equal_weight_baseline,\n",
    "            transaction_cost_rate=0.0005,\n",
    "            lambda_turnover=lambda_turnover,\n",
    "            lambda_hhi=lambda_hhi\n",
    "        )\n",
    "\n",
    "        obs, _ = val_env.reset(seed=seed)\n",
    "        done, total_reward = False, 0.0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_params = params.copy()\n",
    "            best_params.update({\n",
    "                'risk_coefficient': risk_coeff,\n",
    "                'lambda_turnover': lambda_turnover,\n",
    "                'lambda_hhi': lambda_hhi,\n",
    "                'seed': seed\n",
    "            })\n",
    "\n",
    "    return best_params\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Training and prediction function\n",
    "# ---------------------------------------------------------------------\n",
    "def train_and_predict(train_df: pd.DataFrame, val_df: pd.DataFrame,\n",
    "                      pred_df: pd.DataFrame, etf_list: List[str],\n",
    "                      cfg: TrainingConfig, best_params: Dict[str, float],\n",
    "                      model_path: str) -> Tuple[List[List[float]], List[pd.Timestamp]]:\n",
    "    risk_coeff = best_params.pop('risk_coefficient')\n",
    "    seed = best_params.pop('seed')\n",
    "    set_global_seed(seed)\n",
    "\n",
    "    # Initialize training environment\n",
    "    env_train = make_vec_env(\n",
    "        lambda: PortfolioEnv(\n",
    "            train_df, etf_list,\n",
    "            'mean_cvar', risk_coeff,\n",
    "            cfg.rebalance_period,\n",
    "            cfg.lookback_period,\n",
    "            use_baseline=True,\n",
    "            baseline_fn=equal_weight_baseline,\n",
    "            transaction_cost_rate=cfg.transaction_cost_rate,\n",
    "\t\t\tdesired_long=cfg.desired_long,\n",
    "\t\t    desired_short=cfg.desired_short,\n",
    "\t\t    weight_bounds=cfg.weight_bounds,\n",
    "            lambda_turnover=cfg.lambda_turnover,\n",
    "            lambda_hhi=cfg.lambda_hhi\n",
    "        ),\n",
    "        n_envs=1, seed=seed\n",
    "    )\n",
    "\n",
    "    policy_kwargs = dict(net_arch=list(cfg.policy_arch))\n",
    "    model = PPO(\n",
    "        'MlpPolicy', env_train,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        ent_coef=0.01,\n",
    "        clip_range=0.2,\n",
    "        seed=seed,\n",
    "        verbose=0,\n",
    "        **best_params\n",
    "    )\n",
    "\n",
    "    best_val_reward = -np.inf\n",
    "    no_improve = 0\n",
    "\n",
    "    # Early stopping loop\n",
    "    for step in range(0, cfg.max_timesteps, cfg.incremental_timesteps):\n",
    "        model.learn(total_timesteps=cfg.incremental_timesteps)\n",
    "\n",
    "        # Initialize validation environment\n",
    "        val_env = PortfolioEnv(\n",
    "            val_df, etf_list,\n",
    "            'mean_cvar', risk_coeff,\n",
    "            cfg.rebalance_period,\n",
    "            cfg.lookback_period,\n",
    "            use_baseline=True,\n",
    "            baseline_fn=equal_weight_baseline,\n",
    "            transaction_cost_rate=cfg.transaction_cost_rate,\n",
    "\t\t\tdesired_long=cfg.desired_long,\n",
    "\t\t    desired_short=cfg.desired_short,\n",
    "\t\t    weight_bounds=cfg.weight_bounds,\n",
    "            lambda_turnover=cfg.lambda_turnover,\n",
    "            lambda_hhi=cfg.lambda_hhi\n",
    "        )\n",
    "\n",
    "        obs, _ = val_env.reset(seed=seed)\n",
    "        done = False\n",
    "        val_reward = 0.0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            val_reward += reward\n",
    "\n",
    "        if val_reward > best_val_reward:\n",
    "            best_val_reward = val_reward\n",
    "            no_improve = 0\n",
    "            model.save(model_path)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= cfg.patience:\n",
    "                break\n",
    "\n",
    "    # Load best model and predict on pred_df\n",
    "    best_model = PPO.load(model_path)\n",
    "\n",
    "    env_pred = PortfolioEnv(\n",
    "        pred_df, etf_list,\n",
    "        reward_type='mean_cvar',  # Use Mean-CVaR reward explicitly\n",
    "        risk_coefficient=risk_coeff,\n",
    "        rebalance_period=cfg.rebalance_period,\n",
    "        lookback_period=cfg.lookback_period,\n",
    "        use_baseline=False,  # Set baseline to False for delta actions\n",
    "        transaction_cost_rate=cfg.transaction_cost_rate,\n",
    "\t\tdesired_long=cfg.desired_long,\n",
    "\t\tdesired_short=cfg.desired_short,\n",
    "\t\tweight_bounds=cfg.weight_bounds,\n",
    "        lambda_turnover=cfg.lambda_turnover,\n",
    "        lambda_hhi=cfg.lambda_hhi\n",
    "    )\n",
    "\n",
    "    obs, _ = env_pred.reset()\n",
    "    done = False\n",
    "    weights_list, dates_list = [], []\n",
    "\n",
    "    while not done:\n",
    "        if env_pred.current_step >= cfg.lookback_period and (\n",
    "            env_pred.current_step % cfg.rebalance_period == 0\n",
    "        ):\n",
    "            action, _ = best_model.predict(obs, deterministic=True)\n",
    "            obs, _, done, _, _ = env_pred.step(action)  # step first, then record\n",
    "\n",
    "            # Record weights AFTER applying the action\n",
    "            weights_list.append(env_pred.current_weights.tolist())\n",
    "            dates_list.append(env_pred.data.loc[env_pred.current_step, 'Date'])\n",
    "        else:\n",
    "            obs, _, done, _, _ = env_pred.step(np.zeros(len(etf_list), dtype=np.float32))\n",
    "\n",
    "    return weights_list, dates_list\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Data loading and overall training loop\n",
    "# ---------------------------------------------------------------------\n",
    "cfg = TrainingConfig(model_retrain=False)\n",
    "\n",
    "# Load your prepared Stage‑2 dataset and price data\n",
    "data = pd.read_csv('stage2_rl_observations_optimized_10ETFs.csv', parse_dates=['Date'])\n",
    "price_data = pd.read_csv('stock_prices_10ETFs.csv')\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'], utc=True).dt.tz_localize(None)\n",
    "price_cols = {col: f'Price_{col}' for col in price_data.columns if col != 'Date'}\n",
    "price_data.rename(columns=price_cols, inplace=True)\n",
    "\n",
    "merged_data = pd.merge(data, price_data, on='Date', how='inner').reset_index(drop=True)\n",
    "if len(merged_data) != len(data):\n",
    "    print(\"Warning: data length mismatch after merge.\")\n",
    "\n",
    "etf_list = ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU']\n",
    "# etf_list = ['BA',\t'AMGN',\t'DIS',\t'NKE',\t'HON',\t'MMM',\t'CAT',\t'KO',\t'PG',\t'AXP',\t'JPM',\t'MCD',\t'HD',\t'AAPL',\t'CSCO',\t'IBM',\t'MSFT',\t'TRV',\t'UNH',\t'CVX',\t'JNJ',\t'MRK',\t'AMZN',\t'WMT',\t'INTC',\t'VZ']\n",
    "\n",
    "feature_data = add_stable_features(merged_data, etf_list)\n",
    "feature_data = filter_features(feature_data, include_predicted_returns=True, include_shap_metrics=True)\n",
    "\n",
    "# Rolling windows\n",
    "total_len = len(feature_data)\n",
    "# start_indices = range(0,\n",
    "#                       total_len - (cfg.train_window_days + cfg.validation_window_days + cfg.prediction_window_days),\n",
    "#                       cfg.prediction_window_days)\n",
    "\n",
    "start_indices = []\n",
    "current_start = 0\n",
    "\n",
    "while True:\n",
    "    train_start = current_start\n",
    "    train_end = train_start + cfg.train_window_days\n",
    "    val_end = train_end + cfg.validation_window_days\n",
    "    pred_end = val_end + cfg.prediction_window_days\n",
    "    \n",
    "    if pred_end > total_len:\n",
    "        break\n",
    "    \n",
    "    start_indices.append(current_start)\n",
    "    \n",
    "    # move to next window ensuring continuity without gap\n",
    "    current_start += cfg.prediction_window_days - cfg.rebalance_period\n",
    "\n",
    "# Prepare directory for outputs\n",
    "output_dir = 'stage2_iterations'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Collect metrics for all iterations\n",
    "summary_records = []\n",
    "for iter_num in range(cfg.num_iterations):\n",
    "\t\n",
    "    iter_seed = cfg.base_seed + iter_num\n",
    "    set_global_seed(iter_seed)\n",
    "    tuned_seed = iter_seed\n",
    "    iter_dir = os.path.join(output_dir, f'iteration_{iter_num:02d}')\n",
    "    os.makedirs(iter_dir, exist_ok=True)\n",
    "    \n",
    "    previous_model_path = None\n",
    "    iter_returns = []\n",
    "    print(f\"\\nStarting iteration {iter_num+1}/{cfg.num_iterations} (Seed: {tuned_seed}) at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\t\n",
    "    for idx, start_idx in enumerate(start_indices):\n",
    "        window_start_time = time.time()\n",
    "        print(f\"  - Starting window {idx+1}/{len(start_indices)} at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "        train_start = start_idx\n",
    "        train_end = train_start + cfg.train_window_days\n",
    "        val_start = train_end\n",
    "        val_end = val_start + cfg.validation_window_days\n",
    "        pred_start = val_end\n",
    "        pred_end = pred_start + cfg.prediction_window_days\n",
    "\n",
    "        train_df = feature_data.iloc[train_start:train_end].reset_index(drop=True)\n",
    "        val_df = feature_data.iloc[val_start:val_end].reset_index(drop=True)\n",
    "        pred_df = feature_data.iloc[pred_start:pred_end].reset_index(drop=True)\n",
    "        \n",
    "        train_df.ffill(inplace=True)\n",
    "        train_df.bfill(inplace=True)\n",
    "        \n",
    "        val_df.ffill(inplace=True)\n",
    "        val_df.bfill(inplace=True)\n",
    "        \n",
    "        pred_df.ffill(inplace=True)\n",
    "        pred_df.bfill(inplace=True)\n",
    "        \n",
    "\n",
    "        feature_cols = [c for c in train_df.columns if c != 'Date' and not c.startswith('Actual_Return')]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_df[feature_cols])\n",
    "\t\t\n",
    "        scale = scaler.scale_\n",
    "        scale[scale < 1e-8] = 1.0\n",
    "        scaler.scale_ = scale\n",
    "        \n",
    "        train_scaled = train_df.copy()\n",
    "        train_scaled[feature_cols] = scaler.transform(train_df[feature_cols])\n",
    "        val_scaled = val_df.copy()\n",
    "        val_scaled[feature_cols] = scaler.transform(val_df[feature_cols])\n",
    "        pred_scaled = pred_df.copy()\n",
    "        pred_scaled[feature_cols] = scaler.transform(pred_df[feature_cols])\n",
    "\n",
    "        # Explicit seed handling\n",
    "        if idx == 0:\n",
    "            best_params = validate_and_tune(\n",
    "                train_scaled, val_scaled, etf_list, cfg, random_seed=iter_seed\n",
    "            )\n",
    "            \n",
    "            # Fetch tuned seed explicitly from best_params\n",
    "            tuned_seed = best_params.get('seed', iter_seed)\n",
    "        else:\n",
    "            # Keep explicitly using previously found best_params\n",
    "            best_params = best_params.copy()\n",
    "\n",
    "        seed = tuned_seed\n",
    "        set_global_seed(seed)\n",
    "\n",
    "        window_dir = os.path.join(iter_dir, f'window_{idx:02d}')\n",
    "        os.makedirs(window_dir, exist_ok=True)\n",
    "        model_path = os.path.join(window_dir, 'best_ppo.zip')\n",
    "\n",
    "        # Use tuned parameters explicitly\n",
    "        risk_coeff = best_params.get('risk_coefficient', cfg.default_risk_coeff)\n",
    "        lambda_turnover = best_params.get('lambda_turnover', cfg.lambda_turnover)\n",
    "        lambda_hhi = best_params.get('lambda_hhi', cfg.lambda_hhi)\n",
    "        \n",
    "\n",
    "        env_train = make_vec_env(lambda: PortfolioEnv(\n",
    "                train_scaled, etf_list, 'mean_cvar', risk_coeff, cfg.rebalance_period, cfg.lookback_period,\n",
    "                use_baseline=True,\n",
    "                baseline_fn=equal_weight_baseline,\n",
    "                transaction_cost_rate=0.0005,\n",
    "                desired_long=cfg.desired_long,\n",
    "                desired_short=cfg.desired_short,\n",
    "                weight_bounds=cfg.weight_bounds,\n",
    "                lambda_turnover=lambda_turnover,\n",
    "                lambda_hhi=lambda_hhi\n",
    "            ), n_envs=1, seed=seed)\n",
    "\n",
    "        policy_kwargs = dict(net_arch=list(cfg.policy_arch))\n",
    "\n",
    "        if previous_model_path and os.path.exists(previous_model_path) and not cfg.model_retrain:\n",
    "            print(f'load the exising model from {previous_model_path} and retrain')\n",
    "            model = PPO.load(previous_model_path, env=env_train)\n",
    "            model.set_env(env_train)\n",
    "        else:\n",
    "            print(f'triam new model and saved under {model_path}')\n",
    "            model = PPO('MlpPolicy', env_train, policy_kwargs=policy_kwargs,\n",
    "                ent_coef=0.01,\n",
    "                clip_range=0.2,\n",
    "                seed=seed,\n",
    "                learning_rate=best_params.get('learning_rate', 1e-4),\n",
    "                n_steps=best_params.get('n_steps', 20),\n",
    "                batch_size=best_params.get('batch_size', 10),\n",
    "                gamma=best_params.get('gamma', 0.98),\n",
    "                verbose=0)\n",
    "            \n",
    "        best_val_reward = -np.inf\n",
    "        no_improve = 0\n",
    "        training_log = []\n",
    "\n",
    "        for step in range(0, cfg.max_timesteps, cfg.incremental_timesteps):\n",
    "            model.learn(total_timesteps=cfg.incremental_timesteps)\n",
    "\n",
    "            val_env = PortfolioEnv(val_scaled, etf_list, 'mean_cvar', risk_coeff,\n",
    "                                   cfg.rebalance_period, cfg.lookback_period,\n",
    "                                   use_baseline=True,\n",
    "                                   baseline_fn=equal_weight_baseline,\n",
    "                                   transaction_cost_rate=0.0005, \t\t\t\n",
    "\t\t\t\t\t\t\t\t   desired_long=cfg.desired_long,\n",
    "\t\t\t\t\t\t\t\t   desired_short=cfg.desired_short,\n",
    "\t\t\t\t\t\t\t\t   weight_bounds=cfg.weight_bounds, lambda_turnover=lambda_turnover,\n",
    "                lambda_hhi=lambda_hhi)\n",
    "            obs, _ = val_env.reset(seed=seed)\n",
    "            done, val_reward = False, 0.0\n",
    "\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, _, _ = val_env.step(action)\n",
    "                val_reward += reward\n",
    "\n",
    "            training_log.append({'training_step': step + cfg.incremental_timesteps, 'validation_reward': val_reward})\n",
    "\n",
    "            if val_reward > best_val_reward:\n",
    "                best_val_reward = val_reward\n",
    "                no_improve = 0\n",
    "                model.save(model_path)\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= cfg.patience:\n",
    "                    break\n",
    "\n",
    "        pd.DataFrame(training_log).to_csv(os.path.join(window_dir, 'training_validation_log.csv'), index=False)\n",
    "        previous_model_path = model_path\n",
    "\n",
    "        best_model = PPO.load(model_path)\n",
    "        env_pred = PortfolioEnv(\n",
    "            pred_scaled, etf_list,\n",
    "            reward_type='mean_cvar',\n",
    "            risk_coefficient=risk_coeff,\n",
    "            rebalance_period=cfg.rebalance_period,\n",
    "            lookback_period=cfg.lookback_period,\n",
    "            use_baseline=False,\n",
    "            transaction_cost_rate=0.0005,\n",
    "\t\t\tdesired_long=cfg.desired_long,\n",
    "\t\t    desired_short=cfg.desired_short,\n",
    "\t\t    weight_bounds=cfg.weight_bounds, lambda_turnover=lambda_turnover,\n",
    "                lambda_hhi=lambda_hhi\n",
    "        )\n",
    "\n",
    "        obs, _ = env_pred.reset()\n",
    "        done = False\n",
    "        weights_list, dates_list = [], []\n",
    "\n",
    "        while not done:\n",
    "            if env_pred.current_step >= cfg.lookback_period and (\n",
    "                env_pred.current_step % cfg.rebalance_period == 0\n",
    "            ):\n",
    "                action, _ = best_model.predict(obs, deterministic=True)\n",
    "                obs, _, done, _, _ = env_pred.step(action)\n",
    "\n",
    "                weights_list.append(env_pred.current_weights.tolist())\n",
    "                dates_list.append(env_pred.data.loc[env_pred.current_step, 'Date'])\n",
    "            else:\n",
    "                obs, _, done, _, _ = env_pred.step(np.zeros(len(etf_list), dtype=np.float32))\n",
    "\n",
    "        weights_df = pd.DataFrame(weights_list, columns=etf_list)\n",
    "        weights_df.insert(0, 'Date', dates_list)\n",
    "        weights_df.to_csv(os.path.join(window_dir, 'weights.csv'), index=False)\n",
    "\n",
    "        cum_wealth = 1.0\n",
    "        returns_log = []\n",
    "\n",
    "        for t, w in zip(dates_list, weights_list):\n",
    "            step_idx = pred_scaled[pred_scaled['Date'] == t].index[0]\n",
    "            asset_returns = np.array([\n",
    "                pred_scaled.loc[step_idx + 1, f'Actual_Return_{etf}']\n",
    "                for etf in etf_list\n",
    "            ])\n",
    "            port_ret = np.dot(w, asset_returns)\n",
    "            cum_wealth *= (1 + port_ret)\n",
    "            returns_log.append({'Date': t, 'Portfolio_Return': port_ret, 'Cumulative_Wealth': cum_wealth})\n",
    "\n",
    "        iter_returns.append(cum_wealth - 1.0)\n",
    "\n",
    "        pd.DataFrame(returns_log).to_csv(os.path.join(window_dir, 'returns_log.csv'), index=False)\n",
    "        window_end_time = time.time()\n",
    "        elapsed_window_time = window_end_time - window_start_time\n",
    "        print(f\"  - Completed window {idx+1}/{len(start_indices)} in {elapsed_window_time/60:.2f} minutes.\")\n",
    "\n",
    "    mean_ret = np.mean(iter_returns)\n",
    "    std_ret = np.std(iter_returns, ddof=1)\n",
    "    sharpe = (mean_ret / std_ret) * np.sqrt(len(iter_returns)) if std_ret != 0 else np.nan\n",
    "    summary_records.append({\n",
    "        'iteration': iter_num,\n",
    "        'seed': iter_seed,\n",
    "        'mean_return': mean_ret,\n",
    "        'sharpe': sharpe\n",
    "    })\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()  # Only if you're using GPU explicitly\n",
    "\n",
    "summary_df = pd.DataFrame(summary_records)\n",
    "summary_df.to_csv(os.path.join(output_dir, 'iterations_summary.csv'), index=False)\n",
    "\n",
    "t_stat, p_val = ttest_1samp(summary_df['mean_return'], 0.0)\n",
    "with open(os.path.join(output_dir, 't_test_result.csv'), 'w') as f:\n",
    "    f.write(f\"t-statistic,{t_stat}\\np-value,{p_val}\\n\")\n",
    "\n",
    "print(summary_df)\n",
    "print(f\"Overall t-statistic={t_stat:.3f}, p-value={p_val:.3f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()  # Only if you're using GPU explicitly\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-08-04T03:47:49.360681Z"
    }
   },
   "id": "cfe656732378c6e7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-01T04:34:59.094636Z",
     "start_time": "2025-08-01T04:34:59.087630Z"
    }
   },
   "id": "7aba3a43020d11ca",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-01T04:35:15.621027Z",
     "start_time": "2025-08-01T04:35:15.603008Z"
    }
   },
   "id": "93ecdbb778338c51",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'stage2_iterations\\\\iteration_01\\\\window_15\\\\best_ppo.zip'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-30T02:01:42.443625Z",
     "start_time": "2025-07-30T02:01:42.412378Z"
    }
   },
   "id": "5556d1f98d4833ec",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined weights saved to: stage2_iterations\\combined_weights.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "output_dir = 'stage2_iterations'  # Adjust if your path is different\n",
    "pattern = os.path.join(output_dir, 'iteration_*', 'window_*', 'weights.csv')\n",
    "\n",
    "# Find all weight files matching the pattern\n",
    "files = glob.glob(pattern)\n",
    "\n",
    "# Initialize an empty list to collect DataFrames\n",
    "all_weights = []\n",
    "\n",
    "for file_path in files:\n",
    "    # Extract iteration and window numbers\n",
    "    parts = file_path.split(os.sep)\n",
    "    iteration = int(parts[-3].split('_')[1])\n",
    "    window = int(parts[-2].split('_')[1])\n",
    "\n",
    "    # Load weights file\n",
    "    df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "    # Add columns for iteration and window\n",
    "    df.insert(0, 'Window', window)\n",
    "    df.insert(0, 'Iteration', iteration)\n",
    "\n",
    "    # Append to the list\n",
    "    all_weights.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(all_weights, ignore_index=True)\n",
    "\n",
    "# Sort by iteration, window, and date\n",
    "combined_df.sort_values(['Iteration', 'Window', 'Date'], inplace=True)\n",
    "\n",
    "# Save combined data\n",
    "combined_df.to_csv(os.path.join(output_dir, 'combined_weights.csv'), index=False)\n",
    "\n",
    "print(f\"Combined weights saved to: {os.path.join(output_dir, 'combined_weights.csv')}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-02T23:54:25.897625Z",
     "start_time": "2025-08-02T23:54:25.856106Z"
    }
   },
   "id": "9b01351041b154b6",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2b0f510c1ff23295",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "10563904fd258e2f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "637f2b485e42e5a6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# start of stage 2 training\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "SEED = 42\n",
    "def set_global_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    set_random_seed(seed)\n",
    "\n",
    "set_global_seed(SEED)\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar', risk_coefficient=0.5, rebalance_period=21, lookback_period=21):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(etf_list),), dtype=np.float32)\n",
    "\n",
    "        # Explicitly select feature columns (excluding Date and returns used only for calculating reward)\n",
    "        self.feature_cols = [col for col in data.columns if col not in ['Date'] and not col.startswith('Actual_Return')]\n",
    "        self.num_features_per_day = len(self.feature_cols)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.num_features_per_day * self.lookback_period,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(etf_list)] * len(etf_list))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(self.etf_list)] * len(self.etf_list))\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        next_step = self.current_step + 1\n",
    "\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            # v2 long short\n",
    "            desired_long = 1.20  # 120% long exposure explicitly\n",
    "            desired_short = 0.20  # 20% short exposure explicitly\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                # Normal 120/20 explicitly0\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                # Only long explicitly: default realistically to 100% long\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                # Only short explicitly (unrealistic), fallback clearly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                # All zeros explicitly: fallback explicitly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            # Apply explicit clipping\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate explicitly after clipping\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            # Final explicit normalization after clipping\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)  # exactly 100% long\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                # Realistic fallback explicitly: equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "            self.current_weights = final_weights\n",
    "            \n",
    "            # v1 softmax normalization\n",
    "            \n",
    "            # temperature = 0.5  # Explicitly lower for higher concentration (try 0.2 to 0.8)\n",
    "            # scaled_action = action / temperature\n",
    "            # self.current_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "\n",
    "        else:\n",
    "            returns_today = np.array([self.data.loc[self.current_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            self.current_weights *= (1 + returns_today)\n",
    "            self.current_weights /= np.sum(self.current_weights)\n",
    "\n",
    "        if next_step >= len(self.data):\n",
    "            terminated = True\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            returns = np.array([self.data.loc[next_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            portfolio_return = np.dot(self.current_weights, returns)\n",
    "            self.cumulative_wealth *= (1 + portfolio_return)\n",
    "            reward = self.calculate_reward(portfolio_return, returns)\n",
    "            terminated = next_step >= len(self.data) - 1\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "        # def _get_obs(self):\n",
    "        #     obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        #     obs_window = obs_window.drop(columns=['Date']).values.flatten().astype(np.float32)\n",
    "        #     return obs_window\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        obs_window = obs_window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "        return obs_window\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns):\n",
    "        if self.reward_type == 'cumulative_return':\n",
    "            return self.cumulative_wealth - 1.0\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            return np.log(self.cumulative_wealth)\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100 * alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            return portfolio_return - self.risk_coefficient * cvar\n",
    "        else:\n",
    "            raise ValueError('Invalid reward type')\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_stable_features(df, etf_list):\n",
    "    data = df.copy()\n",
    "\n",
    "    for etf in etf_list:\n",
    "        price_col = f'Price_{etf}'\n",
    "\n",
    "        # Volatility (20-day)\n",
    "        data[f'Volatility_{etf}'] = data[price_col].pct_change().rolling(20).std()\n",
    "\n",
    "        # Momentum indicators (returns over 5, 10, 20 days)\n",
    "        data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
    "        data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
    "        data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
    "\n",
    "        # Moving averages (5-day and 20-day)\n",
    "        data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
    "        data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
    "\n",
    "        # Moving average crossover (5-day MA - 20-day MA)\n",
    "        data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
    "\n",
    "    # Drop NaN values due to rolling calculations\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # Explicit patterns to identify columns\n",
    "    predicted_return_pattern = 'Predicted_Return'\n",
    "    shap_metric_pattern = 'SHAP'\n",
    "\n",
    "    # Exclude Predicted Returns explicitly if requested\n",
    "    if not include_predicted_returns:\n",
    "        predicted_cols = [col for col in df_filtered.columns if predicted_return_pattern in col]\n",
    "        df_filtered.drop(columns=predicted_cols, inplace=True)\n",
    "        print(f\"Excluded predicted return columns: {predicted_cols}\")\n",
    "\n",
    "    # Exclude SHAP-related metrics explicitly if requested\n",
    "    if not include_shap_metrics:\n",
    "        shap_cols = [col for col in df_filtered.columns if shap_metric_pattern in col]\n",
    "        df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "        print(f\"Excluded SHAP-related columns: {shap_cols}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# ETFs\n",
    "etf_list = ['XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLY', 'XLV', 'XLU']\n",
    "\n",
    "# etf_list = ['BA',\n",
    "# 'AMGN',\n",
    "# 'DIS',\n",
    "# 'NKE',\n",
    "# 'HON',\n",
    "# 'MMM',\n",
    "# 'CAT',\n",
    "# 'KO',\n",
    "# 'PG',\n",
    "# 'AXP',\n",
    "# 'JPM',\n",
    "# 'MCD',\n",
    "# 'HD',\n",
    "# 'AAPL',\n",
    "# 'CSCO',\n",
    "# 'IBM',\n",
    "# 'MSFT',\n",
    "# 'TRV',\n",
    "# 'UNH',\n",
    "# 'CVX',\n",
    "# 'JNJ',\n",
    "# 'MRK',\n",
    "# 'AMZN',\n",
    "# 'WMT',\n",
    "# 'INTC',\n",
    "# 'VZ']\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 5e-5],\n",
    "    'n_steps': [20, 40],\n",
    "    'batch_size': [5, 10],\n",
    "    'gamma': [0.98, 0.99]\n",
    "}\n",
    "consolidated_file = 'stage2_rl_observations_optimized_10ETFs.csv'\n",
    "reward_type = 'mean_cvar'\n",
    "# data = pd.read_csv(consolidated_file, parse_dates=['Date'])\n",
    "# data = data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "data = pd.read_csv('stage2_rl_observations_optimized_10ETFs.csv', parse_dates=['Date'])\n",
    "price_data = pd.read_csv('stock_prices_10ETFs.csv')\n",
    "# price_data = pd.read_csv('stock_prices_10ETFs.csv')\n",
    "# Convert the Date column in price data, handling the timezone correctly\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'], utc=True)\n",
    "price_data['Date'] = price_data['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Rename price columns explicitly to 'price_{ticker}'\n",
    "price_cols = {col: f'Price_{col}' for col in price_data.columns if col != 'Date'}\n",
    "price_data.rename(columns=price_cols, inplace=True)\n",
    "\n",
    "# Merge datasets on Date\n",
    "merged_data = pd.merge(data, price_data, on='Date', how='inner')\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "# Check if merge was successful\n",
    "if len(merged_data) != len(data):\n",
    "    print(f\"Warning: Data length mismatch after merging (Original: {len(data)}, Merged: {len(merged_data)}).\")\n",
    "else:\n",
    "    print(\"Merged successfully with aligned dates.\")\n",
    "\n",
    "data_with_features_raw = add_stable_features(merged_data, etf_list)\n",
    "data_with_features_raw.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Usage Example clearly for benchmark (only price metrics, no predicted return or SHAP):\n",
    "data_with_features = filter_features(data_with_features_raw, \n",
    "                                 include_predicted_returns=True, \n",
    "                                 include_shap_metrics=True)\n",
    "################### override data to use SHAP only\n",
    "# data_with_features = data\n",
    "################### END override \n",
    "\n",
    "# Define your rolling window lengths clearly:\n",
    "train_window_days = 252 * 10\n",
    "validation_window_days = 126\n",
    "prediction_window_days = 126\n",
    "lookback_period = 10\n",
    "rebalance_period = 10\n",
    "\n",
    "start_indices = range(0, len(data) - (train_window_days + validation_window_days + prediction_window_days), prediction_window_days)\n",
    "all_weights = []\n",
    "model_path = 'ppo_single_train_best_model_10ETFs.zip'\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "def validate_and_tune(train_data, val_data, reward_type, rebalance_period=10, lookback_period=10, n_iter=8, timesteps=5000):\n",
    "    best_reward, best_params = -np.inf, None\n",
    "\n",
    "    # Narrow and meaningful parameter distribution\n",
    "    param_dist = {\n",
    "        'learning_rate': [3e-4, 1e-4],\n",
    "        'n_steps': [20, 40],\n",
    "        'batch_size': [10, 20],\n",
    "        'gamma': [0.95, 0.98],\n",
    "        'risk_coefficient': [0.1, 0.5, 1.0] if reward_type in ['mean_var', 'mean_cvar'] else [0.5],\n",
    "        'seed': [42, 100, 2024, 12345, 579]\n",
    "    }\n",
    "\n",
    "    sampled_params = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=SEED))\n",
    "\n",
    "    for params in sampled_params:\n",
    "        seed = params.pop('seed')\n",
    "        risk_coeff = params.pop('risk_coefficient', 0.5)\n",
    "        set_global_seed(seed)\n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1, seed=seed)\n",
    "        model = PPO('MlpPolicy', env,\n",
    "                    ent_coef=0.01,    # explicitly encourages exploration\n",
    "                    clip_range=0.2,\n",
    "                    seed=seed,\n",
    "                    **params, verbose=0)\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        val_env = PortfolioEnv(val_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        obs, _ = val_env.reset(seed=seed)\n",
    "        done, total_reward = False, 0\n",
    "\n",
    "        while not done:\n",
    "            # num_samples = 100  # Recommended starting point\n",
    "            # action_samples = []\n",
    "            # \n",
    "            # for _ in range(num_samples):\n",
    "            #     sampled_action, _ = model.predict(obs, deterministic=False)  # obs directly\n",
    "            #     action_samples.append(sampled_action)\n",
    "            # \n",
    "            # action = np.mean(action_samples, axis=0)\n",
    "            \n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_params = {**params, 'risk_coefficient': risk_coeff, 'seed': seed}\n",
    "    with open('best_params.json', 'w') as f:\n",
    "        json.dump(best_params, f)\n",
    "    return best_params\n",
    "\n",
    "def scale_data(df, feature_cols, scaler):\n",
    "    scaled_features = scaler.transform(df[feature_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=feature_cols, index=df.index)\n",
    "\n",
    "    # Re-add columns that were not scaled (e.g., Date, Actual_Return_*)\n",
    "    for col in df.columns:\n",
    "        if col not in feature_cols:\n",
    "            scaled_df[col] = df[col].values\n",
    "\n",
    "    # Keep original column order\n",
    "    scaled_df = scaled_df[df.columns]\n",
    "    return scaled_df\n",
    "\n",
    "# Main execution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for idx, start_idx in enumerate(start_indices):\n",
    "    # for start_idx in range(0, 252*2, 252):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Explicit indices for training, validation, and prediction datasets\n",
    "    train_start_idx = start_idx\n",
    "    train_end_idx = train_start_idx + train_window_days\n",
    "\n",
    "    val_start_idx = train_end_idx\n",
    "    val_end_idx = val_start_idx + validation_window_days\n",
    "\n",
    "    pred_start_idx = val_end_idx\n",
    "    pred_end_idx = pred_start_idx + prediction_window_days\n",
    "\n",
    "    # Corresponding dates explicitly\n",
    "    train_start_date = data_with_features.loc[train_start_idx, 'Date']\n",
    "    train_end_date = data_with_features.loc[train_end_idx - 1, 'Date']\n",
    "\n",
    "    val_start_date = data_with_features.loc[val_start_idx, 'Date']\n",
    "    val_end_date = data_with_features.loc[val_end_idx - 1, 'Date']\n",
    "\n",
    "    pred_start_date = data_with_features.loc[pred_start_idx, 'Date']\n",
    "    pred_end_date = data_with_features.loc[pred_end_idx - 1, 'Date']\n",
    "\n",
    "    # Clearly print ranges for clarity\n",
    "    print(f\"Training period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "    print(f\"Validation period: {val_start_date.date()} to {val_end_date.date()}\")\n",
    "    print(f\"Prediction period: {pred_start_date.date()} to {pred_end_date.date()}\")\n",
    "\n",
    "    # Explicitly subset data accordingly\n",
    "    train_data = data_with_features.iloc[train_start_idx:train_end_idx].reset_index(drop=True)\n",
    "    val_data = data_with_features.iloc[val_start_idx:val_end_idx].reset_index(drop=True)\n",
    "    pred_data = data_with_features.iloc[pred_start_idx:pred_end_idx].reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [col for col in train_data.columns if col != 'Date' and not col.startswith('Actual_Return')]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data[feature_cols])\n",
    "\n",
    "    train_data_scaled = scale_data(train_data, feature_cols, scaler)\n",
    "    val_data_scaled = scale_data(val_data, feature_cols, scaler)\n",
    "    pred_data_scaled = scale_data(pred_data, feature_cols, scaler)\n",
    "\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    best_params = validate_and_tune(train_data_scaled, val_data_scaled, reward_type)\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "    incremental_timesteps = 3000    \n",
    "    max_timesteps = 30000\n",
    "    patience = 3\n",
    "    \n",
    "    best_val_reward = -np.inf\n",
    "    no_improve_steps = 0\n",
    "\n",
    "    # risk_coeff = best_params.pop('risk_coefficient',0.5)\n",
    "    policy_kwargs = dict(net_arch=[256, 256])\n",
    "\n",
    "    with open('best_params.json', 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "    \n",
    "    risk_coeff = best_params.pop('risk_coefficient')\n",
    "    seed = best_params.pop('seed')\n",
    "    \n",
    "    set_global_seed(seed)\n",
    "    env = make_vec_env(lambda: PortfolioEnv(train_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1, seed=seed)\n",
    "    \n",
    "    # Load previous model if exists\n",
    "    # if idx > 0 and os.path.exists(model_path):\n",
    "    #     print(f\"Loading previous model from {model_path}...\")\n",
    "    #     model = PPO.load(model_path, env=env)\n",
    "    #     model.set_env(env)\n",
    "    # else:\n",
    "    #     print(\"Initializing new PPO model...\")\n",
    "    #     model = PPO('MlpPolicy', env,\n",
    "    #                 policy_kwargs=policy_kwargs,\n",
    "    #                 ent_coef=0.01,\n",
    "    #                 clip_range=0.2,\n",
    "    #                 seed=seed, \n",
    "    #                 **best_params, verbose=0)\n",
    "     # always retrain\n",
    "    model = PPO('MlpPolicy', env,\n",
    "                    policy_kwargs=policy_kwargs,\n",
    "                    ent_coef=0.01,\n",
    "                    clip_range=0.2,\n",
    "                    seed=seed, \n",
    "                    **best_params, verbose=0)\n",
    "    # model.learn(total_timesteps=20000)\n",
    "    print(\"Starting model training with early stopping...\")\n",
    "\n",
    "    for step in range(0, max_timesteps, incremental_timesteps):\n",
    "        model.learn(total_timesteps=incremental_timesteps)\n",
    "    \n",
    "        # Evaluate on validation environment\n",
    "        val_env = PortfolioEnv(val_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        val_obs, _ = val_env.reset()\n",
    "        val_done = False\n",
    "        val_total_reward = 0.0\n",
    "    \n",
    "        while not val_done:\n",
    "            val_action, _ = model.predict(val_obs, deterministic=True)\n",
    "            # num_samples = 100  # Recommended\n",
    "            # value_action_samples = []\n",
    "            # \n",
    "            # for _ in range(num_samples):\n",
    "            #     value_sampled_action, _ = model.predict(val_obs, deterministic=False)\n",
    "            #     value_action_samples.append(value_sampled_action)\n",
    "            # \n",
    "            # val_action = np.mean(value_action_samples, axis=0)    \n",
    "            \n",
    "            val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "            val_total_reward += val_reward\n",
    "    \n",
    "        print(f\"Step: {step + incremental_timesteps}, Validation Total Reward: {val_total_reward:.4f}\")\n",
    "    \n",
    "        # Early stopping check\n",
    "        if val_total_reward > best_val_reward:\n",
    "            best_val_reward = val_total_reward\n",
    "            no_improve_steps = 0\n",
    "            # model.save(\"best_ppo_model.zip\")\n",
    "            model.save(model_path)\n",
    "            print(f\"Improved validation reward; model saved at step {step + incremental_timesteps}\")\n",
    "        else:\n",
    "            no_improve_steps += 1\n",
    "            print(f\"No improvement ({no_improve_steps}/{patience})\")\n",
    "    \n",
    "            if no_improve_steps >= patience:\n",
    "                print(\"Early stopping explicitly triggered.\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model explicitly\n",
    "    model = PPO.load(model_path)\n",
    "    print(\"Loaded the best PPO model explicitly for prediction.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Ensure historical context explicitly available in prediction\n",
    "    full_data = pd.concat([train_data_scaled, val_data_scaled, pred_data_scaled])\n",
    "    pred_data_with_history = full_data[full_data['Date'] >= (pred_start_date - pd.Timedelta(days=lookback_period))].reset_index(drop=True)\n",
    "\n",
    "    pred_env = PortfolioEnv(pred_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "    # pred_env = PortfolioEnv(pred_data_with_history, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "\n",
    "    obs, info = pred_env.reset()\n",
    "    done = False\n",
    "\n",
    "    action = np.zeros(len(etf_list), dtype=np.float32)\n",
    "\n",
    "    while not done:\n",
    "        if pred_env.current_step >= lookback_period and pred_env.current_step % pred_env.rebalance_period == 0:\n",
    "            # obs_for_agent = pred_data_with_history.drop(columns=['Date']).iloc[pred_env.current_step - lookback_period:pred_env.current_step].values.flatten().astype(np.float32)\n",
    "            # action, _ = model.predict(obs_for_agent, deterministic=True)\n",
    "\n",
    "            # v1 normalize weight\n",
    "            # action, _ = model.predict(obs, deterministic=True)\n",
    "            # use determinstic = FALSE       \n",
    "            # num_samples = 100  # Recommended\n",
    "            # action_samples = []\n",
    "            # for _ in range(num_samples):\n",
    "            #     sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "            #     action_samples.append(sampled_action)\n",
    "            # action = np.mean(action_samples, axis=0)    \n",
    "            # \n",
    "            # temperature = 0.5\n",
    "            # scaled_action = action / temperature\n",
    "            # weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "            # rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "            # all_weights.append([rebalance_date] + weights.tolist())\n",
    "\n",
    "\n",
    "            # v2 long short normalization\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # uncomment this for predictopm\n",
    "            # num_samples = 100  # Recommended\n",
    "            # action_samples = []\n",
    "            # \n",
    "            # for _ in range(num_samples):\n",
    "            #     sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "            #     action_samples.append(sampled_action)\n",
    "            # \n",
    "            # action = np.mean(action_samples, axis=0)    \n",
    "\n",
    "            # Explicitly apply your new 120/20 normalization logic (to match environment step)\n",
    "            desired_long = 1.20  # Explicitly 120% long exposure\n",
    "            desired_short = 0.20  # Explicitly 20% short exposure\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate after clipping explicitly\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "\n",
    "            rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "            all_weights.append([rebalance_date] + final_weights.tolist())\n",
    "\n",
    "        obs, _, done, _, _ = pred_env.step(action)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "columns = ['Date'] + etf_list\n",
    "weights_df = pd.DataFrame(all_weights, columns=columns)\n",
    "weights_df.to_csv('ppo_multi_year_weights_10ETFs.csv', index=False)\n",
    "print(\"Saved predictions to ppo_multi_year_weights_10ETFs.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c770edf2acf6b5a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "############################## This is start to run 25 iterations ##############################\n",
    "########################################################################################################################"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8acf6abff959b252",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ITERATION - final variable: 128/20 - retrain - 50kx30k sample - mean cvar - determinstic false with 50 - 7 yr train by 21 day test\n",
    "# start of stage 2 training\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "def set_global_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    set_random_seed(seed)\n",
    "\n",
    "set_global_seed(SEED)\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar', risk_coefficient=0.5, rebalance_period=21, lookback_period=21):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(etf_list),), dtype=np.float32)\n",
    "\n",
    "        # Explicitly select feature columns (excluding Date and returns used only for calculating reward)\n",
    "        self.feature_cols = [col for col in data.columns if col not in ['Date'] and not col.startswith('Actual_Return')]\n",
    "        self.num_features_per_day = len(self.feature_cols)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.num_features_per_day * self.lookback_period,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(etf_list)] * len(etf_list))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(self.etf_list)] * len(self.etf_list))\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        next_step = self.current_step + 1\n",
    "\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            # v2 long short\n",
    "            desired_long = 1.20  # 120% long exposure explicitly\n",
    "            desired_short = 0.20  # 20% short exposure explicitly\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                # Normal 120/20 explicitly0\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                # Only long explicitly: default realistically to 100% long\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                # Only short explicitly (unrealistic), fallback clearly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                # All zeros explicitly: fallback explicitly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            # Apply explicit clipping\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate explicitly after clipping\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            # Final explicit normalization after clipping\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)  # exactly 100% long\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                # Realistic fallback explicitly: equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "            self.current_weights = final_weights\n",
    "            \n",
    "            # v1 softmax normalization\n",
    "            \n",
    "            # temperature = 0.5  # Explicitly lower for higher concentration (try 0.2 to 0.8)\n",
    "            # scaled_action = action / temperature\n",
    "            # self.current_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "\n",
    "        else:\n",
    "            returns_today = np.array([self.data.loc[self.current_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            self.current_weights *= (1 + returns_today)\n",
    "            self.current_weights /= np.sum(self.current_weights)\n",
    "\n",
    "        if next_step >= len(self.data):\n",
    "            terminated = True\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            returns = np.array([self.data.loc[next_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            portfolio_return = np.dot(self.current_weights, returns)\n",
    "            self.cumulative_wealth *= (1 + portfolio_return)\n",
    "            reward = self.calculate_reward(portfolio_return, returns)\n",
    "            terminated = next_step >= len(self.data) - 1\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "        # def _get_obs(self):\n",
    "        #     obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        #     obs_window = obs_window.drop(columns=['Date']).values.flatten().astype(np.float32)\n",
    "        #     return obs_window\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        obs_window = obs_window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "        return obs_window\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns):\n",
    "        if self.reward_type == 'cumulative_return':\n",
    "            return self.cumulative_wealth - 1.0\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            return np.log(self.cumulative_wealth)\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100 * alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            return portfolio_return - self.risk_coefficient * cvar\n",
    "        else:\n",
    "            raise ValueError('Invalid reward type')\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_stable_features(df, etf_list):\n",
    "    data = df.copy()\n",
    "\n",
    "    for etf in etf_list:\n",
    "        price_col = f'Price_{etf}'\n",
    "\n",
    "        # Volatility (20-day)\n",
    "        data[f'Volatility_{etf}'] = data[price_col].pct_change().rolling(20).std()\n",
    "\n",
    "        # Momentum indicators (returns over 5, 10, 20 days)\n",
    "        data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
    "        data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
    "        data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
    "\n",
    "        # Moving averages (5-day and 20-day)\n",
    "        data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
    "        data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
    "\n",
    "        # Moving average crossover (5-day MA - 20-day MA)\n",
    "        data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
    "\n",
    "    # Drop NaN values due to rolling calculations\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # Explicit patterns to identify columns\n",
    "    predicted_return_pattern = 'Predicted_Return'\n",
    "    shap_metric_pattern = 'SHAP'\n",
    "\n",
    "    # Exclude Predicted Returns explicitly if requested\n",
    "    if not include_predicted_returns:\n",
    "        predicted_cols = [col for col in df_filtered.columns if predicted_return_pattern in col]\n",
    "        df_filtered.drop(columns=predicted_cols, inplace=True)\n",
    "        print(f\"Excluded predicted return columns: {predicted_cols}\")\n",
    "\n",
    "    # Exclude SHAP-related metrics explicitly if requested\n",
    "    if not include_shap_metrics:\n",
    "        shap_cols = [col for col in df_filtered.columns if shap_metric_pattern in col]\n",
    "        df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "        print(f\"Excluded SHAP-related columns: {shap_cols}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# ETFs\n",
    "etf_list = ['XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLY', 'XLV', 'XLU']\n",
    "# etf_list = ['BA',\n",
    "# 'AMGN',\n",
    "# 'DIS',\n",
    "# 'NKE',\n",
    "# 'HON',\n",
    "# 'MMM',\n",
    "# 'CAT',\n",
    "# 'KO',\n",
    "# 'PG',\n",
    "# 'AXP',\n",
    "# 'JPM',\n",
    "# 'MCD',\n",
    "# 'HD',\n",
    "# 'AAPL',\n",
    "# 'CSCO',\n",
    "# 'IBM',\n",
    "# 'MSFT',\n",
    "# 'TRV',\n",
    "# 'UNH',\n",
    "# 'CVX',\n",
    "# 'JNJ',\n",
    "# 'MRK',\n",
    "# 'AMZN',\n",
    "# 'WMT',\n",
    "# 'INTC',\n",
    "# 'VZ']\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 3e-4, 5e-4],\n",
    "    'gamma': [0.90, 0.95, 0.98],\n",
    "    'clip_range': [0.1, 0.2, 0.25],\n",
    "    'gae_lambda': [0.8, 0.9, 0.95]\n",
    "}\n",
    "consolidated_file = 'stage2_rl_observations_optimized_10ETFs.csv'\n",
    "reward_type = 'mean_cvar'\n",
    "# data = pd.read_csv(consolidated_file, parse_dates=['Date'])\n",
    "# data = data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "data = pd.read_csv('stage2_rl_observations_optimized_DIA_ETF.csv', parse_dates=['Date'])\n",
    "price_data = pd.read_csv('stock_prices_DIA_ETF.csv')\n",
    "\n",
    "# Convert the Date column in price data, handling the timezone correctly\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'], utc=True)\n",
    "price_data['Date'] = price_data['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Rename price columns explicitly to 'price_{ticker}'\n",
    "price_cols = {col: f'Price_{col}' for col in price_data.columns if col != 'Date'}\n",
    "price_data.rename(columns=price_cols, inplace=True)\n",
    "\n",
    "# Merge datasets on Date\n",
    "merged_data = pd.merge(data, price_data, on='Date', how='inner')\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "# Check if merge was successful\n",
    "if len(merged_data) != len(data):\n",
    "    print(f\"Warning: Data length mismatch after merging (Original: {len(data)}, Merged: {len(merged_data)}).\")\n",
    "else:\n",
    "    print(\"Merged successfully with aligned dates.\")\n",
    "\n",
    "data_with_features_raw = add_stable_features(merged_data, etf_list)\n",
    "data_with_features_raw.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Usage Example clearly for benchmark (only price metrics, no predicted return or SHAP):\n",
    "data_with_features = filter_features(data_with_features_raw, \n",
    "                                 include_predicted_returns=True, \n",
    "                                 include_shap_metrics=True)\n",
    "################### override data to use SHAP only\n",
    "# data_with_features = data\n",
    "################### END override \n",
    "\n",
    "# Define your rolling window lengths clearly:\n",
    "train_window_days = 252 * 7\n",
    "validation_window_days = 252\n",
    "prediction_window_days = 252\n",
    "lookback_period = 21\n",
    "rebalance_period = 21\n",
    "\n",
    "start_indices = range(0, len(data) - (train_window_days + validation_window_days + prediction_window_days), prediction_window_days)\n",
    "all_weights = []\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "def validate_and_tune(train_data, val_data, reward_type, rebalance_period=10, lookback_period=10, n_iter=8, timesteps=5000):\n",
    "    best_reward, best_params = -np.inf, None\n",
    "\n",
    "    # Narrow and meaningful parameter distribution\n",
    "    param_dist = {\n",
    "        'learning_rate': [3e-4, 1e-4],\n",
    "        'n_steps': [20, 40],\n",
    "        'batch_size': [10, 20],\n",
    "        'gamma': [0.95, 0.98],\n",
    "        'risk_coefficient': [0.1, 0.5, 1.0] if reward_type in ['mean_var', 'mean_cvar'] else [0.5],\n",
    "    }\n",
    "\n",
    "    sampled_params = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=42))\n",
    "\n",
    "    for params in sampled_params:\n",
    "        risk_coeff = params.pop('risk_coefficient', 0.5)\n",
    "\n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "        model = PPO('MlpPolicy', env,\n",
    "                    ent_coef=0.01,    # explicitly encourages exploration\n",
    "                    clip_range=0.2,\n",
    "                    **params, verbose=0)\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        val_env = PortfolioEnv(val_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        obs, _ = val_env.reset()\n",
    "        done, total_reward = False, 0\n",
    "        \n",
    "        # while not done:\n",
    "        #     action, _ = model.predict(obs, deterministic=True)\n",
    "        #     obs, reward, done, _, _ = val_env.step(action)\n",
    "        #     total_reward += reward\n",
    "        \n",
    "        while not done:\n",
    "            num_samples = 100  # Recommended starting point\n",
    "            action_samples = []\n",
    "        \n",
    "            for _ in range(num_samples):\n",
    "                sampled_action, _ = model.predict(obs, deterministic=False)  # obs directly\n",
    "                action_samples.append(sampled_action)\n",
    "        \n",
    "            action = np.mean(action_samples, axis=0)\n",
    "        \n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_params = {**params, 'risk_coefficient': risk_coeff}\n",
    "\n",
    "    return best_params\n",
    "\n",
    "def scale_data(df, feature_cols, scaler):\n",
    "    scaled_features = scaler.transform(df[feature_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=feature_cols, index=df.index)\n",
    "\n",
    "    # Re-add columns that were not scaled (e.g., Date, Actual_Return_*)\n",
    "    for col in df.columns:\n",
    "        if col not in feature_cols:\n",
    "            scaled_df[col] = df[col].values\n",
    "\n",
    "    # Keep original column order\n",
    "    scaled_df = scaled_df[df.columns]\n",
    "    return scaled_df\n",
    "\n",
    "# Main execution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "iterations = 10\n",
    "all_weights_iterations = []\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    print(f\"\\n==== Starting Iteration {iteration + 1}/{iterations} ====\")\n",
    "    model_path = f\"ppo_train_best_model_iteration_{iteration}.zip\"\n",
    "    for idx, start_idx in enumerate(start_indices):\n",
    "        # for start_idx in range(0, 252*2, 252):\n",
    "        start_time = time.time()\n",
    "    \n",
    "        # Explicit indices for training, validation, and prediction datasets\n",
    "        train_start_idx = start_idx\n",
    "        train_end_idx = train_start_idx + train_window_days\n",
    "    \n",
    "        val_start_idx = train_end_idx\n",
    "        val_end_idx = val_start_idx + validation_window_days\n",
    "    \n",
    "        pred_start_idx = val_end_idx\n",
    "        pred_end_idx = pred_start_idx + prediction_window_days\n",
    "    \n",
    "        # Corresponding dates explicitly\n",
    "        train_start_date = data_with_features.loc[train_start_idx, 'Date']\n",
    "        train_end_date = data_with_features.loc[train_end_idx - 1, 'Date']\n",
    "    \n",
    "        val_start_date = data_with_features.loc[val_start_idx, 'Date']\n",
    "        val_end_date = data_with_features.loc[val_end_idx - 1, 'Date']\n",
    "    \n",
    "        pred_start_date = data_with_features.loc[pred_start_idx, 'Date']\n",
    "        pred_end_date = data_with_features.loc[pred_end_idx - 1, 'Date']\n",
    "    \n",
    "        # Clearly print ranges for clarity\n",
    "        print(f\"Training period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "        print(f\"Validation period: {val_start_date.date()} to {val_end_date.date()}\")\n",
    "        print(f\"Prediction period: {pred_start_date.date()} to {pred_end_date.date()}\")\n",
    "    \n",
    "        # Explicitly subset data accordingly\n",
    "        train_data = data_with_features.iloc[train_start_idx:train_end_idx].reset_index(drop=True)\n",
    "        val_data = data_with_features.iloc[val_start_idx:val_end_idx].reset_index(drop=True)\n",
    "        pred_data = data_with_features.iloc[pred_start_idx:pred_end_idx].reset_index(drop=True)\n",
    "    \n",
    "        feature_cols = [col for col in train_data.columns if col != 'Date' and not col.startswith('Actual_Return')]\n",
    "    \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_data[feature_cols])\n",
    "    \n",
    "        train_data_scaled = scale_data(train_data, feature_cols, scaler)\n",
    "        val_data_scaled = scale_data(val_data, feature_cols, scaler)\n",
    "        pred_data_scaled = scale_data(pred_data, feature_cols, scaler)\n",
    "    \n",
    "        print(\"Starting hyperparameter tuning...\")\n",
    "        best_params = validate_and_tune(train_data_scaled, val_data_scaled, reward_type)\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "        incremental_timesteps = 5000\n",
    "        max_timesteps = 30000\n",
    "        patience = 3\n",
    "        \n",
    "        best_val_reward = -np.inf\n",
    "        no_improve_steps = 0\n",
    "    \n",
    "        risk_coeff = best_params.pop('risk_coefficient',0.5)\n",
    "        policy_kwargs = dict(net_arch=[256, 256])\n",
    "    \n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "        \n",
    "        # Load previous model if exists\n",
    "        if idx > 0 and os.path.exists(model_path):\n",
    "            print(f\"Loading previous model from {model_path}...\")\n",
    "            model = PPO.load(model_path, env=env)\n",
    "            model.set_env(env)\n",
    "        else:\n",
    "            print(\"Initializing new PPO model...\")\n",
    "            model = PPO('MlpPolicy', env,\n",
    "                        policy_kwargs=policy_kwargs,\n",
    "                        ent_coef=0.01,\n",
    "                        clip_range=0.2,\n",
    "                        **best_params, verbose=0)\n",
    "         # always retrain\n",
    "        # model = PPO('MlpPolicy', env,\n",
    "        #             policy_kwargs=policy_kwargs,\n",
    "        #             ent_coef=0.01,    # explicitly encourages exploration\n",
    "        #             clip_range=0.2,\n",
    "        #             **best_params, verbose=0)\n",
    "        # print(\"Starting model training...\")\n",
    "        # model.learn(total_timesteps=20000)\n",
    "        print(\"Starting model training with early stopping...\")\n",
    "        \n",
    "        # model = PPO('MlpPolicy', env,\n",
    "        #             policy_kwargs=policy_kwargs,\n",
    "        #             ent_coef=0.01,    # explicitly encourages exploration\n",
    "        #             clip_range=0.2,\n",
    "        #             **best_params, verbose=0)\n",
    "        # print(\"Starting model training...\")\n",
    "        # model.learn(total_timesteps=20000)\n",
    "    \n",
    "        for step in range(0, max_timesteps, incremental_timesteps):\n",
    "            model.learn(total_timesteps=incremental_timesteps)\n",
    "        \n",
    "            # Evaluate on validation environment\n",
    "            val_env = PortfolioEnv(val_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "            val_obs, _ = val_env.reset()\n",
    "            val_done = False\n",
    "            val_total_reward = 0.0\n",
    "        \n",
    "            while not val_done:\n",
    "                # val_action, _ = model.predict(val_obs, deterministic=True)\n",
    "                # val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "                # val_total_reward += val_reward\n",
    "                \n",
    "                num_samples = 100  # Recommended\n",
    "                value_action_samples = []\n",
    "        \n",
    "                for _ in range(num_samples):\n",
    "                    value_sampled_action, _ = model.predict(val_obs, deterministic=False)\n",
    "                    value_action_samples.append(value_sampled_action)\n",
    "            \n",
    "                val_action = np.mean(value_action_samples, axis=0)    \n",
    "                \n",
    "                val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "                val_total_reward += val_reward\n",
    "        \n",
    "            print(f\"Step: {step + incremental_timesteps}, Validation Total Reward: {val_total_reward:.4f}\")\n",
    "        \n",
    "            # Early stopping check\n",
    "            if val_total_reward > best_val_reward:\n",
    "                best_val_reward = val_total_reward\n",
    "                no_improve_steps = 0\n",
    "                # model.save(\"best_ppo_model.zip\")\n",
    "                model.save(model_path)\n",
    "                print(f\"Improved validation reward; model saved at step {step + incremental_timesteps}\")\n",
    "            else:\n",
    "                no_improve_steps += 1\n",
    "                print(f\"No improvement ({no_improve_steps}/{patience})\")\n",
    "        \n",
    "                if no_improve_steps >= patience:\n",
    "                    print(\"Early stopping explicitly triggered.\")\n",
    "                    break\n",
    "        \n",
    "        # Load the best model explicitly\n",
    "        # model = PPO.load(\"best_ppo_model.zip\")\n",
    "        model = PPO.load(model_path)\n",
    "        \n",
    "        print(\"Loaded the best PPO model explicitly for prediction.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "        # Ensure historical context explicitly available in prediction\n",
    "        full_data = pd.concat([train_data_scaled, val_data_scaled, pred_data_scaled])\n",
    "        pred_data_with_history = full_data[full_data['Date'] >= (pred_start_date - pd.Timedelta(days=lookback_period))].reset_index(drop=True)\n",
    "    \n",
    "        pred_env = PortfolioEnv(pred_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        # pred_env = PortfolioEnv(pred_data_with_history, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "    \n",
    "        obs, info = pred_env.reset()\n",
    "        done = False\n",
    "    \n",
    "        action = np.zeros(len(etf_list), dtype=np.float32)\n",
    "    \n",
    "        while not done:\n",
    "            if pred_env.current_step >= lookback_period and pred_env.current_step % pred_env.rebalance_period == 0:\n",
    "                # obs_for_agent = pred_data_with_history.drop(columns=['Date']).iloc[pred_env.current_step - lookback_period:pred_env.current_step].values.flatten().astype(np.float32)\n",
    "                # action, _ = model.predict(obs_for_agent, deterministic=True)\n",
    "    \n",
    "                # v1 normalize weight\n",
    "                # action, _ = model.predict(obs, deterministic=True)\n",
    "                \n",
    "                # num_samples = 100  # Recommended\n",
    "                # action_samples = []\n",
    "                # \n",
    "                # for _ in range(num_samples):\n",
    "                #     sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "                #     action_samples.append(sampled_action)\n",
    "                # \n",
    "                # action = np.mean(action_samples, axis=0)    \n",
    "                # \n",
    "                # temperature = 0.5\n",
    "                # scaled_action = action / temperature\n",
    "                # final_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "                # rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "                # # all_weights.append([rebalance_date] + weights.tolist())\n",
    "                # all_weights_iterations.append([iteration + 1, rebalance_date] + final_weights.tolist())\n",
    "    \n",
    "                # v2 long short normalization\n",
    "                # action, _ = model.predict(obs, deterministic=True)\n",
    "                \n",
    "                num_samples = 100  # Recommended\n",
    "                action_samples = []\n",
    "\n",
    "                for _ in range(num_samples):\n",
    "                    sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "                    action_samples.append(sampled_action)\n",
    "\n",
    "                action = np.mean(action_samples, axis=0)    \n",
    "\n",
    "                # Explicitly apply your new 120/20 normalization logic (to match environment step)\n",
    "                desired_long = 1.20  # Explicitly 120% long exposure\n",
    "                desired_short = 0.20  # Explicitly 20% short exposure\n",
    "                clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "                raw_weights = action.copy()\n",
    "\n",
    "                # Separate explicitly positive (long) and negative (short) actions\n",
    "                long_weights = np.maximum(raw_weights, 0)\n",
    "                short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "                has_longs = np.sum(long_weights) > 0\n",
    "                has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "                if has_longs and has_shorts:\n",
    "                    normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                    normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "                elif has_longs and not has_shorts:\n",
    "                    normalized_long = long_weights / np.sum(long_weights)\n",
    "                    normalized_short = np.zeros_like(short_weights)\n",
    "                elif not has_longs and has_shorts:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    normalized_long = np.ones(num_assets) / num_assets\n",
    "                    normalized_short = np.zeros(num_assets)\n",
    "                else:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    normalized_long = np.ones(num_assets) / num_assets\n",
    "                    normalized_short = np.zeros(num_assets)\n",
    "\n",
    "                combined_weights = normalized_long - normalized_short\n",
    "                clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "                # Re-separate after clipping explicitly\n",
    "                long_clipped = np.maximum(clipped_weights, 0)\n",
    "                short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "                has_long_clipped = np.sum(long_clipped) > 0\n",
    "                has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "                if has_long_clipped and has_short_clipped:\n",
    "                    final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                    final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "                elif has_long_clipped and not has_short_clipped:\n",
    "                    final_long = long_clipped / np.sum(long_clipped)\n",
    "                    final_short = np.zeros_like(short_clipped)\n",
    "                else:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    final_long = np.ones(num_assets) / num_assets\n",
    "                    final_short = np.zeros(num_assets)\n",
    "\n",
    "                final_weights = final_long - final_short\n",
    "\n",
    "                rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "                # all_weights.append([rebalance_date] + final_weights.tolist())\n",
    "                all_weights_iterations.append([iteration + 1, rebalance_date] + final_weights.tolist())\n",
    "                # \n",
    "            obs, _, done, _, _ = pred_env.step(action)\n",
    "    \n",
    "        end_time = time.time()\n",
    "        print(f\"Iteration {iteration + 1}, start index {start_idx} completed in {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "columns = ['Iteration', 'Date'] + etf_list\n",
    "weights_df = pd.DataFrame(all_weights_iterations, columns=columns)\n",
    "weights_df.to_csv('ppo_allocations_multiple_iterations_DIA_ETF.csv', index=False)\n",
    "print(\"Saved all iterations' allocations to ppo_allocations_multiple_iterations_DIA_ETF.csv\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86d1a98b60121652",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Stage 2 PPO training with recommended enhancements\n",
    "# ==================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utility functions and seeding\n",
    "# -------------------------------------------------------------------\n",
    "SEED = 42\n",
    "\n",
    "def set_global_seed(seed):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_global_seed(SEED)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Environment definition with softmax normalisation and Mean‑CVaR reward\n",
    "# -------------------------------------------------------------------\n",
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gym environment for portfolio allocation.\n",
    "    Observations are flattened windows of features; actions are unconstrained\n",
    "    real numbers that are converted to portfolio weights via softmax.\n",
    "    Reward is computed at each rebalance period as mean minus λ × CVaR.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar',\n",
    "                 risk_coefficient=1.0, rebalance_period=21,\n",
    "                 lookback_period=60):\n",
    "        super().__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "\n",
    "        # Action space: one unbounded action per asset\n",
    "        self.action_space = spaces.Box(low=-10, high=10, shape=(len(etf_list),), dtype=np.float32)\n",
    "\n",
    "        # Observation space: flatten last lookback_period days of features\n",
    "        self.feature_cols = [c for c in data.columns\n",
    "                             if c not in ['Date'] and not c.startswith('Actual_Return')]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(len(self.feature_cols)*lookback_period,),\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "        self.current_step = self.lookback_period\n",
    "        self.current_weights = np.array([1/len(etf_list)]*len(etf_list), dtype=float)\n",
    "        self.cumulative_wealth = 1.0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.current_weights = np.array([1/len(self.etf_list)]*len(self.etf_list),\n",
    "                                        dtype=float)\n",
    "        self.cumulative_wealth = 1.0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Return a flattened window of recent features.\"\"\"\n",
    "        window = self.data.iloc[\n",
    "            self.current_step - self.lookback_period : self.current_step\n",
    "        ]\n",
    "        return window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "\n",
    "    def _action_to_weights(self, action):\n",
    "        \"\"\"\n",
    "        Convert raw action outputs into a valid long‑only weight vector via softmax.\n",
    "        This implements the 'continuous 10‑dimensional weights with softmax normalisation'\n",
    "        specification from your methodology (Step 4).\n",
    "        \"\"\"\n",
    "        # temperature scaling – adjust if you want more/less concentration\n",
    "        temperature = 1.0\n",
    "        scaled = action / temperature\n",
    "        exp_vals = np.exp(scaled - np.max(scaled))\n",
    "        return exp_vals / exp_vals.sum()\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns):\n",
    "        \"\"\"Compute reward according to the chosen risk measure.\"\"\"\n",
    "        if self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100*alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            return portfolio_return - self.risk_coefficient * cvar\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            return np.log(self.cumulative_wealth)\n",
    "        elif self.reward_type == 'cumulative_return':\n",
    "            return self.cumulative_wealth - 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown reward_type {self.reward_type}\")\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Update portfolio and compute reward.\"\"\"\n",
    "        next_step = self.current_step + 1\n",
    "\n",
    "        # rebalance portfolio at rebalance dates\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            self.current_weights = self._action_to_weights(action)\n",
    "        else:\n",
    "            # drift weights using actual returns\n",
    "            daily_rets = np.array([\n",
    "                self.data.loc[self.current_step, f'Actual_Return_{t}']\n",
    "                for t in self.etf_list\n",
    "            ])\n",
    "            self.current_weights *= (1 + daily_rets)\n",
    "            self.current_weights /= self.current_weights.sum()\n",
    "\n",
    "        # compute reward on the next day\n",
    "        if next_step >= len(self.data):\n",
    "            done = True\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            asset_returns = np.array([\n",
    "                self.data.loc[next_step, f'Actual_Return_{t}']\n",
    "                for t in self.etf_list\n",
    "            ])\n",
    "            portfolio_ret = float(np.dot(self.current_weights, asset_returns))\n",
    "            self.cumulative_wealth *= (1 + portfolio_ret)\n",
    "            reward = self.calculate_reward(portfolio_ret, asset_returns)\n",
    "            done = (next_step >= len(self.data) - 1)\n",
    "\n",
    "        self.current_step += 1\n",
    "        return self._get_obs(), reward, done, False, {}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Data preparation (Step 1)\n",
    "# -------------------------------------------------------------------\n",
    "# Load your Stage 2 RL observations (predicted returns, SHAP, etc.)\n",
    "stage2_file = 'stage2_rl_observations_optimized_10ETFs.csv'\n",
    "price_file = 'stock_prices_10ETFs.csv'\n",
    "\n",
    "stage2 = pd.read_csv(stage2_file, parse_dates=['Date'])\n",
    "prices = pd.read_csv(price_file)\n",
    "prices['Date'] = pd.to_datetime(prices['Date'], utc=True).dt.tz_localize(None)\n",
    "\n",
    "# Align data on Date\n",
    "prices.rename(columns={c: f'Price_{c}' for c in prices.columns if c != 'Date'},\n",
    "              inplace=True)\n",
    "data_merged = pd.merge(stage2, prices, on='Date', how='inner')\n",
    "\n",
    "# Compute technical indicators (volatility, momentum, moving averages)\n",
    "# as outlined in the methodology (20‑day volatility, 5/10/20‑day momentum,\n",
    "# 5‑ and 20‑day moving averages and crossover).\n",
    "\n",
    "\n",
    "def add_features(df, etfs):\n",
    "    df2 = df.copy()\n",
    "    for etf in etfs:\n",
    "        price_col = f'Price_{etf}'\n",
    "        returns = df2[price_col].pct_change()\n",
    "        df2[f'Volatility_{etf}'] = returns.rolling(20).std()\n",
    "        df2[f'Momentum_5d_{etf}'] = returns.rolling(5).sum()\n",
    "        df2[f'Momentum_10d_{etf}'] = returns.rolling(10).sum()\n",
    "        df2[f'Momentum_20d_{etf}'] = returns.rolling(20).sum()\n",
    "        df2[f'MA_5d_{etf}'] = df2[price_col].rolling(5).mean()\n",
    "        df2[f'MA_20d_{etf}'] = df2[price_col].rolling(20).mean()\n",
    "        df2[f'MA_Crossover_{etf}'] = df2[f'MA_5d_{etf}'] - df2[f'MA_20d_{etf}']\n",
    "    return df2.dropna()\n",
    "\n",
    "data_with_features = add_features(data_merged, ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'])\n",
    "\n",
    "# Optionally, filter out predicted returns or SHAP metrics; here we include both\n",
    "# because they are key inputs in Step 4’s observation space.\n",
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df2 = df.copy()\n",
    "    if not include_predicted_returns:\n",
    "        cols = [c for c in df2.columns if 'Predicted_Return' in c]\n",
    "        df2.drop(columns=cols, inplace=True)\n",
    "    if not include_shap_metrics:\n",
    "        cols = [c for c in df2.columns if 'SHAP' in c]\n",
    "        df2.drop(columns=cols, inplace=True)\n",
    "    return df2\n",
    "\n",
    "data_with_features = filter_features(data_with_features, True, True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Rolling window splits (Stage 2 Initial Training, Validation, Test)\n",
    "# -------------------------------------------------------------------\n",
    "# Use 10 years for training, 2 years for validation, 3 years for test\n",
    "train_days = 252*10\n",
    "val_days   = 252*2\n",
    "test_days  = 252*3\n",
    "\n",
    "lookback  = 60         # 60‑day lookback (recommended)\n",
    "rebalance = 21         # monthly rebalance (21 trading days)\n",
    "\n",
    "# In a real implementation you would loop over many start dates; here we take the first one\n",
    "start_idx = 0\n",
    "train_data = data_with_features.iloc[start_idx:start_idx+train_days].reset_index(drop=True)\n",
    "val_data   = data_with_features.iloc[start_idx+train_days:\n",
    "                                     start_idx+train_days+val_days].reset_index(drop=True)\n",
    "test_data  = data_with_features.iloc[start_idx+train_days+val_days:\n",
    "                                     start_idx+train_days+val_days+test_days].reset_index(drop=True)\n",
    "\n",
    "# Standardise features\n",
    "feature_cols = [c for c in train_data.columns if c not in ['Date']\n",
    "                and not c.startswith('Actual_Return')]\n",
    "scaler = StandardScaler().fit(train_data[feature_cols])\n",
    "\n",
    "def scale(df):\n",
    "    x = scaler.transform(df[feature_cols])\n",
    "    df_scaled = pd.DataFrame(x, columns=feature_cols, index=df.index)\n",
    "    for col in df.columns:\n",
    "        if col not in feature_cols:\n",
    "            df_scaled[col] = df[col]\n",
    "    return df_scaled[df.columns]\n",
    "\n",
    "train_scaled = scale(train_data)\n",
    "val_scaled   = scale(val_data)\n",
    "test_scaled  = scale(test_data)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# PPO Training with improved hyper‑parameters (Step 5)\n",
    "# -------------------------------------------------------------------\n",
    "def linear_schedule(initial_value, final_value):\n",
    "    def schedule(progress_remaining):\n",
    "        return final_value + progress_remaining * (initial_value - final_value)\n",
    "    return schedule\n",
    "\n",
    "# Use a vectorised environment with 10 parallel instances for faster training\n",
    "n_envs = 10\n",
    "def make_env():\n",
    "    return PortfolioEnv(train_scaled, \n",
    "                        ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                        reward_type='mean_cvar',\n",
    "                        risk_coefficient=1.0,\n",
    "                        rebalance_period=rebalance,\n",
    "                        lookback_period=lookback)\n",
    "\n",
    "vec_env = SubprocVecEnv([make_env for _ in range(n_envs)], start_method='spawn')\n",
    "\n",
    "# PPO hyper‑parameters inspired by recent research:contentReference[oaicite:0]{index=0}\n",
    "# n_steps collects 3 months of daily data per environment: 252 * 3 ≈ 756\n",
    "n_steps = 252 * 3\n",
    "ppo_model = PPO(\n",
    "    policy='MlpPolicy',\n",
    "    env=vec_env,\n",
    "    learning_rate=linear_schedule(3e-4, 1e-5),\n",
    "    n_steps=n_steps,\n",
    "    batch_size=1260,           # 252 * 5\n",
    "    n_epochs=16,\n",
    "    gamma=0.9,                 # lower discount to focus on near‑term returns\n",
    "    gae_lambda=0.9,\n",
    "    clip_range=0.25,\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=[64, 64],\n",
    "        activation_fn=torch.nn.Tanh,\n",
    "        log_std_init=-1.0\n",
    "    ),\n",
    "    ent_coef=0.01,\n",
    "    seed=SEED,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train for 7.5 million timesteps (≈600 episodes × 10 envs × 252×5 steps)\n",
    "total_timesteps = int(7.5e6)\n",
    "ppo_model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "# Optionally save the model\n",
    "ppo_model.save('ppo_stage2_best_model.zip')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Validation and early stopping (Step 6)\n",
    "# -------------------------------------------------------------------\n",
    "# After training, evaluate on the validation set without updating parameters\n",
    "val_env = PortfolioEnv(val_scaled,\n",
    "                       ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                       reward_type='mean_cvar',\n",
    "                       risk_coefficient=1.0,\n",
    "                       rebalance_period=rebalance,\n",
    "                       lookback_period=lookback)\n",
    "obs, _ = val_env.reset(seed=SEED)\n",
    "done = False\n",
    "val_reward = 0.0\n",
    "while not done:\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, _ = val_env.step(action)\n",
    "    val_reward += reward\n",
    "print(f\"Total validation reward: {val_reward:.4f}\")\n",
    "\n",
    "# If necessary, you can adjust hyperparameters and re‑train based on validation performance.\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Out‑of‑sample testing (2022–2024) and performance metrics (Step 6)\n",
    "# -------------------------------------------------------------------\n",
    "test_env = PortfolioEnv(test_scaled,\n",
    "                        ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                        reward_type='mean_cvar',\n",
    "                        risk_coefficient=1.0,\n",
    "                        rebalance_period=rebalance,\n",
    "                        lookback_period=lookback)\n",
    "\n",
    "obs, _ = test_env.reset()\n",
    "done = False\n",
    "rebalance_dates = []\n",
    "weights_history = []\n",
    "while not done:\n",
    "    # produce an action every step; env will apply it only on rebalance dates\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    obs, _, done, _, _ = test_env.step(action)\n",
    "    # record weights at rebalance points\n",
    "    if test_env.current_step % rebalance == 0:\n",
    "        date = test_scaled.loc[test_env.current_step-1, 'Date']\n",
    "        weights_history.append([date] + test_env.current_weights.tolist())\n",
    "\n",
    "# Save monthly weights to CSV\n",
    "weights_df = pd.DataFrame(weights_history, columns=['Date'] + ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'])\n",
    "weights_df.to_csv('ppo_stage2_weights.csv', index=False)\n",
    "\n",
    "# Compute drifted daily returns and compare to equal weights\n",
    "# (similar to your existing evaluation code)\n",
    "def compute_returns(weights, price_df):\n",
    "    # Explicitly define price columns by removing \"Price_\" prefix\n",
    "    price_df.columns = [c.replace('Price_', '') for c in price_df.columns]\n",
    "\n",
    "    common = [c for c in weights.columns if c in price_df.columns]\n",
    "    if len(common) == 0:\n",
    "        raise ValueError(\"No common ETFs found between weights and prices DataFrames.\")\n",
    "\n",
    "    price_df = price_df[common]\n",
    "    daily_returns = price_df.pct_change().dropna()\n",
    "    weights = weights.set_index('Date')\n",
    "    start = weights.index.min()\n",
    "    end = weights.index.max() + timedelta(days=rebalance)\n",
    "    daily_returns = daily_returns.loc[start:end]\n",
    "\n",
    "    eq_weight = np.array([1/len(common)]*len(common))\n",
    "    drifted = pd.DataFrame(index=daily_returns.index, columns=common)\n",
    "    eq_drift = pd.DataFrame(index=daily_returns.index, columns=common)\n",
    "    cur_w = weights.iloc[0].values\n",
    "    cur_eq = eq_weight\n",
    "\n",
    "    returns_df = pd.DataFrame(index=daily_returns.index, columns=['RL', 'Equal'])\n",
    "\n",
    "    for d in daily_returns.index:\n",
    "        rets = daily_returns.loc[d]  # <-- Define this here explicitly every loop iteration\n",
    "\n",
    "        if d in weights.index:\n",
    "            cur_w = weights.loc[d].values\n",
    "            cur_eq = eq_weight\n",
    "        else:\n",
    "            cur_w = (cur_w * (1 + rets.values))\n",
    "            cur_w /= cur_w.sum()\n",
    "            cur_eq = (cur_eq * (1 + rets.values))\n",
    "            cur_eq /= cur_eq.sum()\n",
    "\n",
    "        drifted.loc[d] = cur_w\n",
    "        eq_drift.loc[d] = cur_eq\n",
    "\n",
    "        shifted_rl = drifted.shift(1).loc[d]\n",
    "        shifted_eq = eq_drift.shift(1).loc[d]\n",
    "\n",
    "        if d == daily_returns.index[0]:\n",
    "            returns_df.loc[d, 'RL'] = np.dot(cur_w, rets)\n",
    "            returns_df.loc[d, 'Equal'] = np.dot(cur_eq, rets)\n",
    "        else:\n",
    "            returns_df.loc[d, 'RL'] = np.dot(shifted_rl, rets)\n",
    "            returns_df.loc[d, 'Equal'] = np.dot(shifted_eq, rets)\n",
    "\n",
    "    return returns_df.dropna()\n",
    "\n",
    "# Compute test returns\n",
    "test_returns = compute_returns(weights_df, prices.set_index('Date'))\n",
    "cum_rl    = (1 + test_returns['RL']).prod() - 1\n",
    "cum_equal = (1 + test_returns['Equal']).prod() - 1\n",
    "print(f\"Out‑of‑sample cumulative return (RL):    {cum_rl:.4%}\")\n",
    "print(f\"Out‑of‑sample cumulative return (Equal): {cum_equal:.4%}\")\n",
    "\n",
    "# You can also compute annualised return, volatility, Sharpe ratio and max drawdown\n",
    "def performance_metrics(returns, freq=252):\n",
    "    ann_return = (1 + returns).prod()**(freq/len(returns)) - 1\n",
    "    ann_vol    = returns.std() * np.sqrt(freq)\n",
    "    sharpe     = ann_return / ann_vol if ann_vol != 0 else np.nan\n",
    "    cum_pnl    = (1+returns).cumprod()\n",
    "    max_dd     = (cum_pnl / cum_pnl.cummax() - 1).min()\n",
    "    return ann_return, ann_vol, sharpe, max_dd\n",
    "\n",
    "rl_ann, rl_vol, rl_sharpe, rl_dd = performance_metrics(test_returns['RL'])\n",
    "eq_ann, eq_vol, eq_sharpe, eq_dd = performance_metrics(test_returns['Equal'])\n",
    "print(f\"RL annualised return:    {rl_ann:.4%}, Sharpe: {rl_sharpe:.3f}, Max Drawdown: {rl_dd:.4%}\")\n",
    "print(f\"Equal annualised return: {eq_ann:.4%}, Sharpe: {eq_sharpe:.3f}, Max Drawdown: {eq_dd:.4%}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6947d247eaecd896",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Validation and early stopping (Step 6)\n",
    "# -------------------------------------------------------------------\n",
    "# After training, evaluate on the validation set without updating parameters\n",
    "val_env = PortfolioEnv(val_scaled,\n",
    "                       ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                       reward_type='mean_cvar',\n",
    "                       risk_coefficient=1.0,\n",
    "                       rebalance_period=rebalance,\n",
    "                       lookback_period=lookback)\n",
    "obs, _ = val_env.reset(seed=SEED)\n",
    "done = False\n",
    "val_reward = 0.0\n",
    "while not done:\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, _ = val_env.step(action)\n",
    "    val_reward += reward\n",
    "print(f\"Total validation reward: {val_reward:.4f}\")\n",
    "\n",
    "# If necessary, you can adjust hyperparameters and re‑train based on validation performance.\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Out‑of‑sample testing (2022–2024) and performance metrics (Step 6)\n",
    "# -------------------------------------------------------------------\n",
    "test_env = PortfolioEnv(test_scaled,\n",
    "                        ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                        reward_type='mean_cvar',\n",
    "                        risk_coefficient=1.0,\n",
    "                        rebalance_period=rebalance,\n",
    "                        lookback_period=lookback)\n",
    "\n",
    "obs, _ = test_env.reset()\n",
    "done = False\n",
    "rebalance_dates = []\n",
    "weights_history = []\n",
    "while not done:\n",
    "    # produce an action every step; env will apply it only on rebalance dates\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    obs, _, done, _, _ = test_env.step(action)\n",
    "    # record weights at rebalance points\n",
    "    if test_env.current_step % rebalance == 0:\n",
    "        date = test_scaled.loc[test_env.current_step-1, 'Date']\n",
    "        weights_history.append([date] + test_env.current_weights.tolist())\n",
    "\n",
    "# Save monthly weights to CSV\n",
    "weights_df = pd.DataFrame(weights_history, columns=['Date'] + ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'])\n",
    "weights_df.to_csv('ppo_stage2_weights.csv', index=False)\n",
    "\n",
    "# Compute drifted daily returns and compare to equal weights\n",
    "# (similar to your existing evaluation code)\n",
    "def compute_returns(weights, price_df):\n",
    "    # Explicitly define price columns by removing \"Price_\" prefix\n",
    "    price_df.columns = [c.replace('Price_', '') for c in price_df.columns]\n",
    "\n",
    "    common = [c for c in weights.columns if c in price_df.columns]\n",
    "    if len(common) == 0:\n",
    "        raise ValueError(\"No common ETFs found between weights and prices DataFrames.\")\n",
    "\n",
    "    price_df = price_df[common]\n",
    "    daily_returns = price_df.pct_change().dropna()\n",
    "    weights = weights.set_index('Date')\n",
    "    start = weights.index.min()\n",
    "    end = weights.index.max() + timedelta(days=rebalance)\n",
    "    daily_returns = daily_returns.loc[start:end]\n",
    "\n",
    "    eq_weight = np.array([1/len(common)]*len(common))\n",
    "    drifted = pd.DataFrame(index=daily_returns.index, columns=common)\n",
    "    eq_drift = pd.DataFrame(index=daily_returns.index, columns=common)\n",
    "    cur_w = weights.iloc[0].values\n",
    "    cur_eq = eq_weight\n",
    "\n",
    "    returns_df = pd.DataFrame(index=daily_returns.index, columns=['RL', 'Equal'])\n",
    "\n",
    "    for d in daily_returns.index:\n",
    "        rets = daily_returns.loc[d]  # <-- Define this here explicitly every loop iteration\n",
    "\n",
    "        if d in weights.index:\n",
    "            cur_w = weights.loc[d].values\n",
    "            cur_eq = eq_weight\n",
    "        else:\n",
    "            cur_w = (cur_w * (1 + rets.values))\n",
    "            cur_w /= cur_w.sum()\n",
    "            cur_eq = (cur_eq * (1 + rets.values))\n",
    "            cur_eq /= cur_eq.sum()\n",
    "\n",
    "        drifted.loc[d] = cur_w\n",
    "        eq_drift.loc[d] = cur_eq\n",
    "\n",
    "        shifted_rl = drifted.shift(1).loc[d]\n",
    "        shifted_eq = eq_drift.shift(1).loc[d]\n",
    "\n",
    "        if d == daily_returns.index[0]:\n",
    "            returns_df.loc[d, 'RL'] = np.dot(cur_w, rets)\n",
    "            returns_df.loc[d, 'Equal'] = np.dot(cur_eq, rets)\n",
    "        else:\n",
    "            returns_df.loc[d, 'RL'] = np.dot(shifted_rl, rets)\n",
    "            returns_df.loc[d, 'Equal'] = np.dot(shifted_eq, rets)\n",
    "\n",
    "    return returns_df.dropna()\n",
    "\n",
    "# Compute test returns\n",
    "test_returns = compute_returns(weights_df, prices.set_index('Date'))\n",
    "cum_rl    = (1 + test_returns['RL']).prod() - 1\n",
    "cum_equal = (1 + test_returns['Equal']).prod() - 1\n",
    "print(f\"Out‑of‑sample cumulative return (RL):    {cum_rl:.4%}\")\n",
    "print(f\"Out‑of‑sample cumulative return (Equal): {cum_equal:.4%}\")\n",
    "\n",
    "# You can also compute annualised return, volatility, Sharpe ratio and max drawdown\n",
    "def performance_metrics(returns, freq=252):\n",
    "    ann_return = (1 + returns).prod()**(freq/len(returns)) - 1\n",
    "    ann_vol    = returns.std() * np.sqrt(freq)\n",
    "    sharpe     = ann_return / ann_vol if ann_vol != 0 else np.nan\n",
    "    cum_pnl    = (1+returns).cumprod()\n",
    "    max_dd     = (cum_pnl / cum_pnl.cummax() - 1).min()\n",
    "    return ann_return, ann_vol, sharpe, max_dd\n",
    "\n",
    "rl_ann, rl_vol, rl_sharpe, rl_dd = performance_metrics(test_returns['RL'])\n",
    "eq_ann, eq_vol, eq_sharpe, eq_dd = performance_metrics(test_returns['Equal'])\n",
    "print(f\"RL annualised return:    {rl_ann:.4%}, Sharpe: {rl_sharpe:.3f}, Max Drawdown: {rl_dd:.4%}\")\n",
    "print(f\"Equal annualised return: {eq_ann:.4%}, Sharpe: {eq_sharpe:.3f}, Max Drawdown: {eq_dd:.4%}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3462ab258866c09e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data explicitly\n",
    "port_wts = pd.read_csv('ppo_allocations_multiple_iterations_DIA_ETF.csv', parse_dates=['Date'], index_col='Date')\n",
    "daily_returns = pd.read_csv('daily_returns_10ETFs.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "common_tickers = [col for col in port_wts.columns if col in daily_returns.columns]\n",
    "daily_returns = daily_returns[common_tickers]\n",
    "\n",
    "# Explicitly filter daily returns to the date range covered by portfolio weights\n",
    "start_date, end_date = port_wts.index.min(), port_wts.index.max() + pd.Timedelta(days=21)\n",
    "daily_returns = daily_returns.loc[start_date:end_date]\n",
    "\n",
    "# Initialize drifted weights with the first available rebalance weights\n",
    "initial_weights = port_wts.loc[start_date].values\n",
    "\n",
    "equal_weight = np.array([1.0 / len(common_tickers)] * len(common_tickers))\n",
    "\n",
    "# Create drifted weights DataFrame explicitly initialized\n",
    "drifted_weights = pd.DataFrame(index=daily_returns.index, columns=common_tickers)\n",
    "equal_weights = pd.DataFrame(index=daily_returns.index, columns=common_tickers)\n",
    "\n",
    "current_weights = initial_weights\n",
    "current_equal_weights = equal_weight\n",
    "\n",
    "# Initialize returns DataFrame explicitly\n",
    "returns_df = pd.DataFrame(index=daily_returns.index, columns=['Optimal_Portfolio_Return', 'Equal_Weight_Return'])\n",
    "\n",
    "for current_date in daily_returns.index:\n",
    "    if current_date in port_wts.index:\n",
    "        # Explicit rebalance date: assign new weights\n",
    "        current_weights = port_wts.loc[current_date].values\n",
    "        current_equal_weights = equal_weight\n",
    "    else:\n",
    "        # Explicitly drift weights using previous day's return\n",
    "        prev_day_return = daily_returns.loc[current_date]\n",
    "\n",
    "        drifted_wts_numerator = current_weights * (1 + prev_day_return.values)\n",
    "        current_weights = drifted_wts_numerator / np.sum(drifted_wts_numerator)\n",
    "\n",
    "        equal_drifted_numerator = current_equal_weights * (1 + prev_day_return.values)\n",
    "        current_equal_weights = equal_drifted_numerator / np.sum(equal_drifted_numerator)\n",
    "\n",
    "    drifted_weights.loc[current_date] = current_weights\n",
    "    equal_weights.loc[current_date] = current_equal_weights\n",
    "    shifted_drifted_weights = drifted_weights.shift(1)\n",
    "    shifted_equal_weights = equal_weights.shift(1)\n",
    "    if current_date == daily_returns.index[0]:\n",
    "        # On the first day, use initial weights directly\n",
    "        returns_df.loc[current_date, 'Optimal_Portfolio_Return'] = np.dot(\n",
    "            drifted_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "        returns_df.loc[current_date, 'Equal_Weight_Return'] = np.dot(\n",
    "            equal_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "    else:\n",
    "        # Explicitly use previous day's weights\n",
    "        returns_df.loc[current_date, 'Optimal_Portfolio_Return'] = np.dot(\n",
    "            shifted_drifted_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "        returns_df.loc[current_date, 'Equal_Weight_Return'] = np.dot(\n",
    "            shifted_equal_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "\n",
    "# Check explicitly\n",
    "print(\"Drifted weights (head):\\n\", drifted_weights.head())\n",
    "print(\"\\nPortfolio returns (head):\\n\", returns_df.head())\n",
    "\n",
    "# Save explicitly\n",
    "drifted_weights.to_csv('drifted_weights_corrected.csv')\n",
    "equal_weights.to_csv('equal_weights.csv')\n",
    "returns_df.to_csv('portfolio_returns_combined.csv')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9e17792fa6118e4",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
