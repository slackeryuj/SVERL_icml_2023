{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install shap torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8689c0d5db117cff",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-14T02:28:00.888314Z",
     "start_time": "2025-07-14T02:27:50.697717Z"
    }
   },
   "id": "eba7f3a3ab7361f8",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "input_dir = \"E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\"  # Use the specified directory\n",
    "ticker_file = os.path.join(input_dir, \"tickers.txt\")  # Text file containing tickers, one per line\n",
    "factor_file = os.path.join(input_dir, \"FF_factors.csv\")  # CSV file containing factor returns\n",
    "start_date = \"1999-01-01\"\n",
    "end_date = \"2025-03-31\"\n",
    "output_csv = os.path.join(input_dir, \"stock_prices.csv\")\n",
    "\n",
    "try:\n",
    "    tickers = load_tickers_from_file(ticker_file)\n",
    "    price_data = download_stock_prices(tickers, start_date, end_date, output_csv)\n",
    "    daily_returns = calculate_daily_returns(price_data, return_type='arithmetic')\n",
    "    daily_returns.to_csv(os.path.join(input_dir, \"daily_returns.csv\"))\n",
    "    factor_returns = read_factor_returns(factor_file, daily_returns)\n",
    "    factor_returns.to_csv(os.path.join(input_dir, \"aligned_factors.csv\"))\n",
    "    print(\"Factor returns alignment completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "factors = pd.read_csv(f\"{input_dir}/aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns_raw = pd.read_csv(f\"{input_dir}/daily_returns.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors = factors.dropna()\n",
    "returns = returns_raw.dropna()\n",
    "\n",
    "# Align dates between factors and returns\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors_raw = factors.loc[dates]\n",
    "returns_raw = returns.loc[dates]\n",
    "factors_raw = factors_raw.iloc[:, :-1]\n",
    "factors_raw"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81f54119dd995ccd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for BA...\n",
      "Downloading data for AMGN...\n",
      "Downloading data for DIS...\n",
      "Downloading data for NKE...\n",
      "Downloading data for HON...\n",
      "Downloading data for MMM...\n",
      "Downloading data for CAT...\n",
      "Downloading data for KO...\n",
      "Downloading data for PG...\n",
      "Downloading data for AXP...\n",
      "Downloading data for GS...\n",
      "Downloading data for JPM...\n",
      "Downloading data for MCD...\n",
      "Downloading data for HD...\n",
      "Downloading data for AAPL...\n",
      "Downloading data for CRM...\n",
      "Downloading data for CSCO...\n",
      "Downloading data for IBM...\n",
      "Downloading data for MSFT...\n",
      "Downloading data for TRV...\n",
      "Downloading data for UNH...\n",
      "Downloading data for CVX...\n",
      "Downloading data for JNJ...\n",
      "Downloading data for MRK...\n",
      "Downloading data for AMZN...\n",
      "Downloading data for WMT...\n",
      "Downloading data for INTC...\n",
      "Downloading data for VZ...\n",
      "Removed tickers with insufficient data: CRM, GS\n",
      "Data saved to E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\\stock_prices.csv\n",
      "Factor returns alignment completed.\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\"  # Use the specified directory\n",
    "ticker_file = os.path.join(input_dir, \"tickers.txt\")  # Text file containing tickers, one per line\n",
    "factor_file = os.path.join(input_dir, \"FF_factors.csv\")  # CSV file containing factor returns\n",
    "start_date = \"1999-01-01\"\n",
    "end_date = \"2024-11-29\"\n",
    "output_csv = os.path.join(input_dir, \"stock_prices.csv\")\n",
    "\n",
    "try:\n",
    "    tickers = load_tickers_from_file(ticker_file)\n",
    "    price_data = download_stock_prices(tickers, start_date, end_date, output_csv)\n",
    "    daily_returns = calculate_daily_returns(price_data, return_type='arithmetic')\n",
    "    daily_returns.to_csv(os.path.join(input_dir, \"daily_returns_DIA_ETF.csv\"))\n",
    "    factor_returns = read_factor_returns(factor_file, daily_returns)\n",
    "    factor_returns.to_csv(os.path.join(input_dir, \"aligned_factors.csv\"))\n",
    "    print(\"Factor returns alignment completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-14T02:28:10.815497Z",
     "start_time": "2025-07-14T02:28:00.890316Z"
    }
   },
   "id": "9cffc0a75f01028",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "factor_file = os.path.join(input_dir, \"FF_factors.csv\")  # CSV file containing factor returns\n",
    "daily_returns = pd.read_csv(f\"{input_dir}/daily_returns_DIA_ETF.csv\", index_col=0, parse_dates=True)\n",
    "factor_returns = read_factor_returns(factor_file, daily_returns)\n",
    "factor_returns.to_csv(os.path.join(input_dir, \"aligned_factors.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-14T02:33:10.725059Z",
     "start_time": "2025-07-14T02:33:10.569025Z"
    }
   },
   "id": "48c7098402de2de5",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def download_stock_prices(tickers, start_date, end_date, output_file):\n",
    "    \"\"\"\n",
    "    Download historical stock prices for the given tickers with rate-limit handling.\n",
    "\n",
    "    Parameters:\n",
    "    tickers (list): List of stock tickers.\n",
    "    start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "    end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "    output_file (str): Path to save the resulting CSV file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the daily price data with tickers as columns and dates as index.\n",
    "    \"\"\"\n",
    "    if not tickers:\n",
    "        raise ValueError(\"Ticker list is empty.\")\n",
    "\n",
    "    all_data = {}\n",
    "\n",
    "    for ticker in tickers:\n",
    "        print(f\"Downloading data for {ticker}...\")\n",
    "\n",
    "        success = False\n",
    "        retries = 5  # Retry up to 5 times if rate-limited\n",
    "\n",
    "        while not success and retries > 0:\n",
    "            try:\n",
    "                data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "                if 'Close' in data.columns:\n",
    "                    data = data[['Close']]\n",
    "                    data.columns = [ticker]  # Rename column to ticker\n",
    "                    all_data[ticker] = data\n",
    "                    success = True\n",
    "                else:\n",
    "                    print(f\"No 'Close' data available for {ticker}. Skipping.\")\n",
    "\n",
    "            except yf.YFRateLimitError as e:\n",
    "                print(f\"Rate limit hit for {ticker}, retrying in 10 seconds...\")\n",
    "                time.sleep(10)  # Wait for 10 seconds before retrying\n",
    "                retries -= 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download data for {ticker}: {e}\")\n",
    "                break  # Break if error is not rate limit\n",
    "\n",
    "        time.sleep(2)  # Add 2-second delay to prevent frequent requests\n",
    "\n",
    "    if not all_data:\n",
    "        raise ValueError(\"No stock data was downloaded.\")\n",
    "\n",
    "    combined_data = pd.concat(all_data.values(), axis=1)\n",
    "    combined_data.to_csv(output_file)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "    return combined_data\n",
    "tickers = load_tickers_from_file(ticker_file)\n",
    "combined_data = download_stock_prices(tickers, start_date, end_date, output_csv)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74462fb4861e4ff9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# returns = returns_raw[[\"MMM\"]]\n",
    "\n",
    "assert not factors_raw.isnull().values.any(), \"Factors data contains NaN values\"\n",
    "assert not returns_raw.isnull().values.any(), \"Returns data contains NaN values\"\n",
    "assert np.isfinite(factors_raw.values).all(), \"Factors data contains infinity\"\n",
    "assert np.isfinite(returns_raw.values).all(), \"Returns data contains infinity\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d705145e1d42231",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Nday_diff = 60  # Define the future prediction horizon for shift days\n",
    "returns_raw = returns_raw.iloc[:, :1]\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors_raw = factors_raw.dropna()\n",
    "returns_raw = returns_raw.dropna()\n",
    "# factors_raw = pd.concat([factors_raw, returns_raw], axis=1)\n",
    "# Align dates between factors_raw and returns_raw\n",
    "dates = factors_raw.index.intersection(returns_raw.index)\n",
    "factors_raw = factors_raw.loc[dates]\n",
    "returns_raw = returns_raw.loc[dates]\n",
    "\n",
    "# Shift returns to generate target variable (N-day future return)\n",
    "returns_raw_Y = returns_raw.shift(-Nday_diff).dropna()\n",
    "\n",
    "# Align all datasets with available dates in returns_raw_Y\n",
    "dates_final = factors_raw.index.intersection(returns_raw_Y.index)\n",
    "factors_raw = factors_raw.loc[dates_final]\n",
    "returns_raw = returns_raw.loc[dates_final]\n",
    "returns_raw_Y = returns_raw_Y.loc[dates_final]\n",
    "\n",
    "N = 60 \n",
    "sequence_length = 20\n",
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "rolling_window_train = 240  # Number of training samples\n",
    "rolling_window_test = 60   # Number of testing samples\n",
    "prediction_window = 60     # Number of prediction samples\n",
    "\n",
    "\n",
    "# sequence_length = 20\n",
    "# batch_size = 32\n",
    "# num_epochs = 25\n",
    "# learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Record results\n",
    "results = []\n",
    "prediction_records = []\n",
    "shapley_records = []\n",
    "shapley_metrics_df = []\n",
    "all_test_losses = []\n",
    "test_results =[]\n",
    "prediction_results = []\n",
    "combined_test_result = []\n",
    "\n",
    "for stock in returns_raw.columns:\n",
    "    print(f\"Training for stock: {stock}\")\n",
    "    stock_returns = returns_raw_Y[[stock]]  # Select the stock column\n",
    "    factors_raw_X =  pd.concat([factors_raw, stock_returns], axis=1)\n",
    "    # Rolling split: Train (M days), Test (N days), Predict (W days)\n",
    "    total_samples = len(factors_raw_X)\n",
    "        # Initialize the rolling split index\n",
    "    start = 0\n",
    "\n",
    "    while start + rolling_window_train + prediction_window + prediction_window <= total_samples:\n",
    "        train_start = start\n",
    "        train_end = start + rolling_window_train\n",
    "\n",
    "        test_start = train_end\n",
    "        test_end = test_start + rolling_window_test\n",
    "\n",
    "        predict_start = test_end  # Predictions start right after training\n",
    "        predict_end = predict_start + prediction_window\n",
    "\n",
    "        # Extract the actual date indices\n",
    "        train_dates = (factors_raw_X.iloc[train_start].name, factors_raw_X.iloc[train_end - 1].name)\n",
    "        test_dates = (factors_raw_X.iloc[test_start].name, factors_raw_X.iloc[test_end - 1].name)\n",
    "        predict_dates = (factors_raw_X.iloc[predict_start].name, factors_raw_X.iloc[predict_end - 1].name)\n",
    "\n",
    "        # Print the selected date ranges\n",
    "        print(f\"Training dates: {train_dates}\")\n",
    "        print(f\"Testing dates: {test_dates}\")\n",
    "        print(f\"Prediction dates: {predict_dates}\")\n",
    "\n",
    "        # # Split data into training, testing, and prediction sets\n",
    "        train_factors = factors_raw_X.iloc[train_start:train_end]\n",
    "        train_returns = stock_returns.iloc[train_start:train_end]\n",
    "        test_factors = factors_raw_X.iloc[test_start:test_end]\n",
    "        test_returns = stock_returns.iloc[test_start:test_end]\n",
    "        predict_factors = factors_raw_X.iloc[predict_start:predict_end]\n",
    "        predict_returns = stock_returns.iloc[predict_start:predict_end]\n",
    "        # \n",
    "        # # Normalize train and test data\n",
    "        train_factors_mean, train_factors_std = train_factors.mean(), train_factors.std()\n",
    "        train_returns_mean, train_returns_std = train_returns.mean(), train_returns.std()\n",
    "\n",
    "        # Avoid division by zero\n",
    "        train_factors_std.replace(0, 1e-8, inplace=True)\n",
    "        train_returns_std.replace(0, 1e-8, inplace=True)\n",
    "        \n",
    "        train_factors_norm = (train_factors - train_factors_mean) / train_factors_std\n",
    "        train_returns_norm = (train_returns - train_returns_mean) / train_returns_std\n",
    "        test_factors_norm = (test_factors - train_factors_mean) / train_factors_std\n",
    "        test_returns_norm = (test_returns - train_returns_mean) / train_returns_std\n",
    "        predict_factors_norm = (predict_factors - train_factors_mean) / train_factors_std\n",
    "        predict_returns_norm = (predict_returns - train_returns_mean) / train_returns_std\n",
    "        print(f'test_factors_norm dim: {test_factors_norm.shape}')\n",
    "        \n",
    "        # # Initialize dataset and dataloader\n",
    "        dataset = StockDataset(train_factors_norm, train_returns_norm, sequence_length, N=N)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        # \n",
    "        # # Model initialization\n",
    "        d_model = train_factors.shape[1]\n",
    "        print(f'd_model: {d_model}')\n",
    "\n",
    "        num_heads = min(4, d_model)  # Ensure num_heads does not exceed d_model and d_model is divisible by num_heads\n",
    "        while d_model % num_heads != 0:\n",
    "            num_heads -= 1\n",
    "        # print(f\"d_model: {d_model} num_heads: {num_heads} sequence_length: {sequence_length} and train factor return dim: {train_factors.shape[1]} {train_returns.shape[1]}\")\n",
    "        model = StockPredictAgent(d_model=d_model, num_heads=num_heads, sequence_length=sequence_length, N=N).to(device)\n",
    "        # \n",
    "        # # Optimizer and loss function\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "        criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss for stability        \n",
    "        # \n",
    "        # # Training the model\n",
    "        # print(f\"Training window: {train_start} to {train_end}\")\n",
    "        # print(f\"Expected FC Layer Input Shape: {sequence_length * d_model}\")\n",
    "        # print(f\"Flattened Tensor Shape: {batch_size, sequence_length * d_model}\")\n",
    "        train_model_with_logging(model, dataloader, optimizer, criterion,scheduler, num_epochs, device)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        test_factors_tensor = torch.tensor(test_factors_norm.values, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "        predict_factors_tensor = torch.tensor(predict_factors_norm.values, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "        \n",
    "        # print(f'test_factors_tensor: {test_factors_tensor.shape}')\n",
    "        # Run inference on test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_predictions_norm = model(test_factors_tensor).cpu().numpy().flatten()\n",
    "            future_predictions_norm = model(predict_factors_tensor).cpu().numpy().flatten()\n",
    "            \n",
    "            # test_output = model(torch.tensor(test_factors_np, dtype=torch.float32).to(device))\n",
    "            # print(f\"Model output shape: {test_output.shape}\")  # Expected: (1, 20) for 20 future returns\n",
    "        # print(f'test_factors_tensor: {test_factors_tensor.shape}')\n",
    "        # Revert normalization for test and predicted returns\n",
    "        test_predictions = test_predictions_norm * train_returns_std.item() + train_returns_mean.item()\n",
    "        \n",
    "        \n",
    "        # \n",
    "        # # start SHAP\n",
    "        # Prepare your input: (1 sample, 20 days, 6 features)\n",
    "        test_factors_np = test_factors_norm.values.astype('float32')\n",
    "        test_factors_np = test_factors_np.reshape(1, rolling_window_test, d_model)  # Reshaping for 1 sample with 20 days and 7 features\n",
    "        # print(f'test_factors_np dim: {test_factors_np.shape}')\n",
    "        # \n",
    "        # # Use a subset of the training data as the background dataset\n",
    "        background_data = torch.tensor(train_factors_norm.values.astype('float32')).to(device)\n",
    "        background_data = background_data.reshape(-1, rolling_window_test, d_model)  # Ensure same shape as model input\n",
    "        # print(f'background_data dim: {background_data.shape}')\n",
    "        # Ensure background_data is a tensor and avoid redundant torch.tensor() calls\n",
    "        if not isinstance(background_data, torch.Tensor):\n",
    "            background_data = torch.tensor(background_data, dtype=torch.float32).to(device)\n",
    "        else:\n",
    "            background_data = background_data.to(device)  # Just move to device without re-wrapping\n",
    "        \n",
    "        # Initialize SHAP GradientExplainer\n",
    "        explainer = shap.GradientExplainer(model, background_data)\n",
    "\n",
    "        # Compute SHAP values for the test input\n",
    "        shap_values = explainer.shap_values(torch.tensor(test_factors_np, dtype=torch.float32).to(device), ranked_outputs=1)\n",
    "        # Aggregate SHAP values across outputs and time\n",
    "        # Verify SHAP values shape\n",
    "        # print(f\"SHAP values shape: {shap_values[0].shape}\")  # Expected: (1, 20, 6)\n",
    "        # print(f\"test factor shape: {test_factors_norm.shape}\")  # Expected: (1, 20, 6)\n",
    "        \n",
    "        ############# train using combined SHAP measures and test factor against binary test return \n",
    "        # Normalize SHAP values using the same mean and std as training factors\n",
    "        shap_values_squeezed = np.squeeze(shap_values[0], axis=(0, -1))  # Shape: (60, 6)\n",
    "        shap_values_normalized = (shap_values_squeezed - train_factors_mean.values) / train_factors_std.values\n",
    "        \n",
    "        # Concatenate normalized SHAP values with normalized test factors\n",
    "        test_factors_np = test_factors_norm.values.astype('float32')  # Shape: (60, 6)\n",
    "        augmented_features = np.concatenate([test_factors_np, shap_values_normalized], axis=1)  # Shape: (60, 12)\n",
    "        \n",
    "        # Convert test_returns_norm into binary (above/below mean)\n",
    "        returns_mean = train_factors_mean.values\n",
    "        test_returns_binary = (test_returns_norm > returns_mean).astype(int)\n",
    "\n",
    "        # Train the Model Using Augmented Features\n",
    "        \n",
    "        d_model2 = augmented_features.shape[1]  # Number of features (6 original + 6 SHAP)\n",
    "        num_heads = min(4, d_model2)\n",
    "        while d_model2 % num_heads != 0:\n",
    "            num_heads -= 1\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = StockDatasetBinary(augmented_features, test_returns_norm, sequence_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Initialize the model for binary classification\n",
    "        model = StockPredictAgentBinary(d_model=d_model2, num_heads=num_heads, sequence_length=sequence_length).to(device)\n",
    "        \n",
    "        # Optimizer and binary cross-entropy loss\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "        criterion = nn.BCELoss()  # Binary cross-entropy for up/down classification\n",
    "        \n",
    "        # Training loop\n",
    "        def train_model(model, dataloader, optimizer, criterion, num_epochs, device):\n",
    "            model.train()\n",
    "            for epoch in range(num_epochs):\n",
    "                total_loss = 0.0\n",
    "                for factors, target in dataloader:\n",
    "                    factors, target = factors.to(device), target.to(device).unsqueeze(1)  # Ensure correct shape\n",
    "        \n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(factors)\n",
    "        \n",
    "                    loss = criterion(output, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "        \n",
    "                    total_loss += loss.item()\n",
    "        \n",
    "                avg_loss = total_loss / len(dataloader)\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Train the model\n",
    "        train_model(model, dataloader, optimizer, criterion, num_epochs, device)\n",
    "        # Convert augmented test features to tensor\n",
    "        augmented_features_tensor = torch.tensor(augmented_features, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Get model predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_predictions = model(augmented_features_tensor).cpu().numpy().flatten()\n",
    "        \n",
    "        # Convert probabilities to binary labels (0 = down, 1 = up)\n",
    "        test_predictions_binary = (test_predictions > 0.5).astype(int)\n",
    "        \n",
    "        # Print predictions\n",
    "        print(f\"Predicted Stock Directions: {test_predictions_binary}\")\n",
    "\n",
    "        \n",
    "        ########################## end of Shap test data training and use those model for prediction phase\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ###### print SHAP RELATED METRICS\n",
    "        # # shap_values_squeeze = shap_values[0].reshape(1,rolling_window_test,d_model)\n",
    "        # \n",
    "        # # 1. Mean Absolute SHAP Values (Feature Importance)\n",
    "        # mean_abs_shap_values = np.mean(np.abs(shap_values[0]), axis=(0, 1))  # Shape: (6,)\n",
    "        # # print(f'mean_abs_shap_values: {mean_abs_shap_values}')\n",
    "        # # 2. SHAP Value Variance (Variability of Feature Contributions)\n",
    "        # shap_value_variance = np.var(shap_values[0], axis=(0, 1))  # Shape: (6,)\n",
    "        # # print(f'shap_value_variance: {shap_value_variance}')\n",
    "        # # 3. SHAP Interaction Values (Feature Interdependencies)\n",
    "        # # interaction_values = shap.TreeExplainer(model).shap_interaction_values(test_factors_tensor)\n",
    "        # \n",
    "        # # 4. Create DataFrame to Display Metrics\n",
    "        # feature_importance_df = pd.DataFrame({\n",
    "        #     'Date': test_factors.index[-1],\n",
    "        #     'Feature': factors_raw_X.columns,\n",
    "        #     'Mean_Abs_SHAP_Value': mean_abs_shap_values.flatten(),\n",
    "        #     'SHAP_Value_Variance': shap_value_variance.flatten()\n",
    "        #     # 'SHAP_Value_MEAN_DIV_VAR': mean_abs_shap_values.flatten() / shap_value_variance.flatten()\n",
    "        #     \n",
    "        # })\n",
    "        # \n",
    "        # feature_importance_df_pivot = feature_importance_df.pivot(\n",
    "        #         index='Date', \n",
    "        #         columns='Feature', \n",
    "        #         values=['Mean_Abs_SHAP_Value', 'SHAP_Value_Variance']\n",
    "        #     ).reset_index()\n",
    "        # # Sort by Feature Importance\n",
    "        # # feature_importance_df = feature_importance_df.sort_values(by='Mean_Abs_SHAP_Value', ascending=False)\n",
    "        # \n",
    "        # # Flatten MultiIndex columns for readability\n",
    "        # feature_importance_df_pivot.columns = ['_'.join(col).strip() if col[1] else col[0] for col in feature_importance_df_pivot.columns.values]\n",
    "        # \n",
    "        # \n",
    "        # \n",
    "        # print(\"Feature Importance Metrics (Mean SHAP & Variance):\")\n",
    "        # print(feature_importance_df_pivot.to_string(index=False))\n",
    "        # # print(f'feature_importance_df: {feature_importance_df}')\n",
    "        # \n",
    "        # # # end shap\n",
    "        # ###### print SHAP RELATED METRICS\n",
    "        \n",
    "        \n",
    "        \n",
    "        future_predictions = future_predictions_norm * train_returns_std.item() + train_returns_mean.item()\n",
    "        actual_test_returns = test_returns.values.flatten()\n",
    "        actual_predict_returns = predict_returns.values.flatten()\n",
    "        # print(f'test_predictions dim:{test_predictions.shape} actual_test_returns dim: {actual_test_returns.shape}')\n",
    "\n",
    "        # Compute loss for test phase\n",
    "        test_loss = np.mean((test_predictions - actual_test_returns) ** 2)  # MSE Loss\n",
    "        all_test_losses.append(test_loss)\n",
    "        print(f'test_loss: {test_loss}')\n",
    "        # Store test results\n",
    "        combined_test_result = {\n",
    "            \"Stock\": stock,\n",
    "            \"Test Dates\": (test_start, test_end),\n",
    "            \"Predicted Returns (Test Set)\": test_predictions.tolist(),\n",
    "            \"Actual Returns (Test Set)\": actual_test_returns.tolist(),\n",
    "            \"Loss\": test_loss\n",
    "        }\n",
    "        \n",
    "        # Add SHAP feature importance metrics to the test results\n",
    "        combined_test_result.update(feature_importance_df_pivot.iloc[0].to_dict())\n",
    "        \n",
    "        # Append the combined results to the test_results list\n",
    "        test_results.append(combined_test_result)\n",
    "\n",
    "\n",
    "        # Store prediction results\n",
    "        # prediction_results.append({\n",
    "        #     \"Stock\": stock,\n",
    "        #     \"Prediction Dates\": (predict_start, predict_end),\n",
    "        #     \"Predicted Returns\": future_predictions.tolist(),\n",
    "        #     \"Actual Returns\": actual_predict_returns.tolist()\n",
    "        # })\n",
    "        # Match dates with predictions and actual returns\n",
    "        # date_range = pd.date_range(start=test_start, periods=len(actual_predict_returns), freq=\"D\")\n",
    "    \n",
    "        for date, predicted, actual in zip(predict_returns.index, future_predictions, actual_predict_returns):\n",
    "            prediction_results.append({\n",
    "                \"Stock\": stock,\n",
    "                \"Date\": date,\n",
    "                \"Predicted Return\": predicted,\n",
    "                \"Actual Return\": actual\n",
    "            })\n",
    "        print(f\"Predictions for {stock} from {predict_start} to {predict_end} complete.\")\n",
    "        start += prediction_window\n",
    "        \n",
    "    # Print average test loss for this stock\n",
    "    avg_test_loss = np.mean(all_test_losses)\n",
    "    print(f\"Average Test Loss for {stock}: {avg_test_loss:.6f}\")\n",
    "future_predictions_df = pd.DataFrame(prediction_results)  \n",
    "\n",
    "future_predictions_df.to_csv(f\"{input_dir}/predicted_vs_actual.csv\", index=False)\n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "test_results_df.to_csv(f\"{input_dir}/test_result_output.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c244d3eff01ce48f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "feature_importance_df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e19d9b563e3796b6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_dir = \"E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\"  # Use the specified directory\n",
    "ticker_file = os.path.join(input_dir, \"tickers.txt\")  # Text file containing tickers, one per line\n",
    "factor_file = os.path.join(input_dir, \"FF_factors.csv\")  # CSV file containing factor returns\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "output_csv = os.path.join(input_dir, \"stock_prices.csv\")\n",
    "#\n",
    "# try:\n",
    "#     tickers = load_tickers_from_file(ticker_file)\n",
    "#     price_data = download_stock_prices(tickers, start_date, end_date, output_csv)\n",
    "#     daily_returns = calculate_daily_returns(price_data, return_type='arithmetic')\n",
    "#     daily_returns.to_csv(os.path.join(input_dir, \"daily_returns.csv\"))\n",
    "#     factor_returns = read_factor_returns(factor_file, daily_returns)\n",
    "#     factor_returns.to_csv(os.path.join(input_dir, \"aligned_factors.csv\"))\n",
    "#     print(\"Factor returns alignment completed.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")\n",
    "\n",
    "factors = pd.read_csv(f\"{input_dir}/aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns_raw = pd.read_csv(f\"{input_dir}/daily_returns.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors = factors.dropna()\n",
    "returns = returns_raw.dropna()\n",
    "\n",
    "# Align dates between factors and returns\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors_raw = factors.loc[dates]\n",
    "returns_raw = returns.loc[dates]\n",
    "factors_raw = factors_raw.iloc[:, :-1]\n",
    "# print(f\"factors_raw = {factors_raw}\")\n",
    "\n",
    "Nday_diff = 60  # Define the future prediction horizon for shift days\n",
    "returns_raw = returns_raw.iloc[:, :1]\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors_raw = factors_raw.dropna()\n",
    "returns_raw = returns_raw.dropna()\n",
    "# factors_raw = pd.concat([factors_raw, returns_raw], axis=1)\n",
    "# Align dates between factors_raw and returns_raw\n",
    "dates = factors_raw.index.intersection(returns_raw.index)\n",
    "factors_raw = factors_raw.loc[dates]\n",
    "returns_raw = returns_raw.loc[dates]\n",
    "\n",
    "# Shift returns to generate target variable (N-day future return)\n",
    "returns_raw_Y = returns_raw.shift(-Nday_diff).dropna()\n",
    "\n",
    "# Align all datasets with available dates in returns_raw_Y\n",
    "dates_final = factors_raw.index.intersection(returns_raw_Y.index)\n",
    "factors_raw = factors_raw.loc[dates_final]\n",
    "returns_raw = returns_raw.loc[dates_final]\n",
    "returns_raw_Y = returns_raw_Y.loc[dates_final]\n",
    "\n",
    "N = 60\n",
    "sequence_length = 60\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "rolling_window_train = 240  # Number of training samples\n",
    "rolling_window_test = 60  # Number of testing samples\n",
    "prediction_window = 60  # Number of prediction samples\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Record results\n",
    "results = []\n",
    "prediction_records = []\n",
    "shapley_records = []\n",
    "shapley_metrics_df = []\n",
    "all_test_losses = []\n",
    "test_results = []\n",
    "prediction_results = []\n",
    "combined_test_result = []\n",
    "\n",
    "for stock in returns_raw.columns:\n",
    "    print(f\"Training for stock: {stock}\")\n",
    "    stock_returns = returns_raw_Y[[stock]]  # Select the stock column\n",
    "    factors_raw_X = pd.concat([factors_raw, stock_returns], axis=1)\n",
    "    # Rolling split: Train (M days), Test (N days), Predict (W days)\n",
    "    total_samples = len(factors_raw_X)\n",
    "    # Initialize the rolling split index\n",
    "    start = 0\n",
    "\n",
    "    while start + rolling_window_train + prediction_window + prediction_window <= total_samples:\n",
    "        train_start = start\n",
    "        train_end = start + rolling_window_train\n",
    "\n",
    "        test_start = train_end\n",
    "        test_end = test_start + rolling_window_test\n",
    "\n",
    "        predict_start = test_end  # Predictions start right after training\n",
    "        predict_end = predict_start + prediction_window\n",
    "\n",
    "        # Extract the actual date indices\n",
    "        train_dates = (factors_raw_X.iloc[train_start].name, factors_raw_X.iloc[train_end - 1].name)\n",
    "        test_dates = (factors_raw_X.iloc[test_start].name, factors_raw_X.iloc[test_end - 1].name)\n",
    "        predict_dates = (factors_raw_X.iloc[predict_start].name, factors_raw_X.iloc[predict_end - 1].name)\n",
    "\n",
    "        # Print the selected date ranges\n",
    "        print(f\"Training dates: {train_dates}\")\n",
    "        print(f\"Testing dates: {test_dates}\")\n",
    "        print(f\"Prediction dates: {predict_dates}\")\n",
    "\n",
    "        # # Split data into training, testing, and prediction sets\n",
    "        train_factors = factors_raw_X.iloc[train_start:train_end]\n",
    "        train_returns = stock_returns.iloc[train_start:train_end]\n",
    "        test_factors = factors_raw_X.iloc[test_start:test_end]\n",
    "        test_returns = stock_returns.iloc[test_start:test_end]\n",
    "        predict_factors = factors_raw_X.iloc[predict_start:predict_end]\n",
    "        predict_returns = stock_returns.iloc[predict_start:predict_end]\n",
    "        #\n",
    "        # # Normalize train and test data\n",
    "        train_factors_mean, train_factors_std = train_factors.mean(), train_factors.std()\n",
    "        train_returns_mean, train_returns_std = train_returns.mean(), train_returns.std()\n",
    "\n",
    "        # Avoid division by zero\n",
    "        train_factors_std.replace(0, 1e-8, inplace=True)\n",
    "        train_returns_std.replace(0, 1e-8, inplace=True)\n",
    "\n",
    "        train_factors_norm = (train_factors - train_factors_mean) / train_factors_std\n",
    "        train_returns_norm = (train_returns - train_returns_mean) / train_returns_std\n",
    "        test_factors_norm = (test_factors - train_factors_mean) / train_factors_std\n",
    "        test_returns_norm = (test_returns - train_returns_mean) / train_returns_std\n",
    "        predict_factors_norm = (predict_factors - train_factors_mean) / train_factors_std\n",
    "        predict_returns_norm = (predict_returns - train_returns_mean) / train_returns_std\n",
    "        print(f'train_factors_norm dim: {train_factors_norm.shape}')\n",
    "        print(f'train_returns_norm dim: {train_returns_norm.shape}')\n",
    "        dataset = StockDataset(train_factors_norm, train_returns_norm, sequence_length, N=N)\n",
    "\n",
    "        sample_x, sample_y = dataset[0]\n",
    "\n",
    "        print(f\"🚀 Step 1: Sample X Raw Shape from Dataset: {sample_x.shape}\")  # Expected: (60, 6)\n",
    "        print(f\"🚀 Step 1: Sample Y Raw Shape from Dataset: {sample_y.shape}\")  # Expected: (60, 1)\n",
    "\n",
    "        # Explicitly check for extra dimensions\n",
    "        if sample_x.shape[0] == 1 and len(sample_x.shape) == 3:\n",
    "            print(\"🔥 Warning: Extra dimension detected in Sample X!\")\n",
    "\n",
    "        # # Initialize dataset and dataloader\n",
    "        dataset = StockDataset(train_factors_norm, train_returns_norm, sequence_length, N=N)\n",
    "\n",
    "        sample_x, sample_y = dataset[0]\n",
    "\n",
    "        print(f\"Fixed Sample X Shape: {sample_x.shape}\")  # Expected: (60, 6)\n",
    "        print(f\"Fixed Sample Y Shape: {sample_y.shape}\")  # Expected: (60, 1)\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        #\n",
    "        # # Model initialization\n",
    "        d_model = train_factors.shape[1]\n",
    "        print(f'd_model: {d_model}')\n",
    "\n",
    "        num_heads = min(4, d_model)  # Ensure num_heads does not exceed d_model and d_model is divisible by num_heads\n",
    "        while d_model % num_heads != 0:\n",
    "            num_heads -= 1\n",
    "        # print(f\"d_model: {d_model} num_heads: {num_heads} sequence_length: {sequence_length} and train factor return dim: {train_factors.shape[1]} {train_returns.shape[1]}\")\n",
    "        model = StockPredictAgent(d_model=d_model, num_heads=num_heads, sequence_length=sequence_length, N=N).to(device)\n",
    "        #\n",
    "        # # Optimizer and loss function\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "        criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss for stability\n",
    "        #\n",
    "        # # Training the model\n",
    "        # print(f\"Training window: {train_start} to {train_end}\")\n",
    "        # print(f\"Expected FC Layer Input Shape: {sequence_length * d_model}\")\n",
    "        # print(f\"Flattened Tensor Shape: {batch_size, sequence_length * d_model}\")\n",
    "\n",
    "        train_model_with_logging(model, dataloader, optimizer, criterion, scheduler, num_epochs, device)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        test_factors_tensor = torch.tensor(test_factors_norm.values, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "        predict_factors_tensor = torch.tensor(predict_factors_norm.values, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "        print(f'test_factors_tensor: {test_factors_tensor.shape}')\n",
    "        # print(f'test_factors_tensor: {test_factors_tensor.shape}')\n",
    "        # Run inference on test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_predictions_norm = model(test_factors_tensor).cpu().numpy().flatten()\n",
    "            future_predictions_norm = model(predict_factors_tensor).cpu().numpy().flatten()\n",
    "\n",
    "            # test_output = model(torch.tensor(test_factors_np, dtype=torch.float32).to(device))\n",
    "            # print(f\"Model output shape: {test_output.shape}\")  # Expected: (1, 20) for 20 future returns\n",
    "        # print(f'test_factors_tensor: {test_factors_tensor.shape}')\n",
    "        # Revert normalization for test and predicted returns\n",
    "        test_predictions = test_predictions_norm * train_returns_std.item() + train_returns_mean.item()\n",
    "\n",
    "        #\n",
    "        # # start SHAP\n",
    "        # Prepare your input: (1 sample, 20 days, 6 features)\n",
    "        # ######################## WRAP THIS AS FUNCTION\n",
    "        # test_factors_np = test_factors_norm.values.astype('float32')\n",
    "        # test_factors_np = test_factors_np.reshape(1, rolling_window_test,\n",
    "        #                                           d_model)  # Reshaping for 1 sample with 20 days and 7 features\n",
    "        # # print(f'test_factors_np dim: {test_factors_np.shape}')\n",
    "        # #\n",
    "        # # # Use a subset of the training data as the background dataset\n",
    "        # background_data = torch.tensor(train_factors_norm.values.astype('float32')).to(device)\n",
    "        # background_data = background_data.reshape(-1, rolling_window_test, d_model)  # Ensure same shape as model input\n",
    "        # # print(f'background_data dim: {background_data.shape}')\n",
    "        # # Ensure background_data is a tensor and avoid redundant torch.tensor() calls\n",
    "        # if not isinstance(background_data, torch.Tensor):\n",
    "        #     background_data = torch.tensor(background_data, dtype=torch.float32).to(device)\n",
    "        # else:\n",
    "        #     background_data = background_data.to(device)  # Just move to device without re-wrapping\n",
    "        #\n",
    "        # # Initialize SHAP GradientExplainer\n",
    "        # explainer = shap.GradientExplainer(model, background_data)\n",
    "        #\n",
    "        # # Compute SHAP values for the test input\n",
    "        # shap_values = explainer.shap_values(torch.tensor(test_factors_np, dtype=torch.float32).to(device),\n",
    "        #                                     ranked_outputs=1)\n",
    "        # # Aggregate SHAP values across outputs and time\n",
    "        # # Verify SHAP values shape\n",
    "        # # print(f\"SHAP values shape: {shap_values[0].shape}\")  # Expected: (1, 20, 6)\n",
    "        # # print(f\"test factor shape: {test_factors_norm.shape}\")  # Expected: (1, 20, 6)\n",
    "        #\n",
    "        # ############# train using combined SHAP measures and test factor against binary test return\n",
    "        # # Normalize SHAP values using the same mean and std as training factors\n",
    "        # shap_values_squeezed = np.squeeze(shap_values[0], axis=(0, -1))  # Shape: (60, 6)\n",
    "        # shap_values_normalized = (shap_values_squeezed - train_factors_mean.values) / train_factors_std.values\n",
    "        #\n",
    "        # # Concatenate normalized SHAP values with normalized test factors\n",
    "        # test_factors_np = test_factors_norm.values.astype('float32')  # Shape: (60, 6)\n",
    "        # augmented_features = np.concatenate([test_factors_np, shap_values_normalized], axis=1)  # Shape: (60, 12)\n",
    "        # ########## END OF WRAPPING FUNCTION\n",
    "        # Generate augmented features\n",
    "        augmented_test_features = generate_augmented_features(\n",
    "            test_factors_norm, train_factors_norm, train_factors_mean, train_factors_std,\n",
    "            model, rolling_window_test, device\n",
    "        )\n",
    "\n",
    "        # Convert to PyTorch tensor if needed\n",
    "        augmented_features_tensor = torch.tensor(augmented_test_features, dtype=torch.float32).to(device)\n",
    "\n",
    "        print(f\"Final Augmented Tensor Shape: {augmented_features_tensor.shape}\")  # Expected: (rolling_window_test, d_model * 2)\n",
    "\n",
    "        # Convert test_returns_norm into binary classification\n",
    "        cumulative_return = np.prod(test_returns_norm + 1) - 1  # Aggregate 60-day return\n",
    "        test_returns_binary = (cumulative_return > train_returns_mean).astype(int)  # 1 if positive, 0 if negative\n",
    "\n",
    "        print(f\"Cumulative Return over 60 days: {cumulative_return}\")\n",
    "        print(f\"Binary Target (Up=1, Down=0): {test_returns_binary}\")\n",
    "\n",
    "        # Train the Model Using Augmented Features\n",
    "        # sequence_length = 20\n",
    "        # batch_size = 32\n",
    "        # num_epochs = 25\n",
    "        # learning_rate = 0.001\n",
    "        d_model2 = augmented_test_features.shape[1]  # Number of features (6 original + 6 SHAP)\n",
    "        num_heads = min(4, d_model2)\n",
    "        while d_model2 % num_heads != 0:\n",
    "            num_heads -= 1\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Length of augmented_test_features: {augmented_test_features.shape}\")\n",
    "        print(f\"Length of test_returns_norm: {test_returns_norm.shape}\")\n",
    "        print(f\"Length of sequence: {sequence_length}\")\n",
    "\n",
    "        sequence_length2 = min(10, len(augmented_test_features))  # Ensure it's smaller than available data\n",
    "\n",
    "        # Create dataset and dataloader for training\n",
    "        dataset2 = StockDatasetBinary(augmented_test_features, test_returns_norm, sequence_length2)\n",
    "        dataloader2 = DataLoader(dataset2, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for batch_x, batch_y in dataloader2:\n",
    "            print(f\"Batch X Shape: {batch_x.shape}\")  # Expected: (batch_size, sequence_length, num_features)\n",
    "            print(f\"Batch Y Shape: {batch_y.shape}\")  # Expected: (batch_size, N, num_features)\n",
    "            break  # Only print for the first batch\n",
    "\n",
    "        # Initialize the binary classification model\n",
    "        model_binary = StockPredictAgentBinary(d_model=d_model2, num_heads=num_heads,\n",
    "                                               sequence_length=sequence_length2).to(device)\n",
    "\n",
    "        # Define optimizer and loss function\n",
    "        optimizer = optim.Adam(model_binary.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "        criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "\n",
    "        # Train the model\n",
    "        train_model(model_binary, dataloader2, optimizer, criterion, num_epochs, device)\n",
    "        # Convert augmented test features to tensor\n",
    "\n",
    "        augmented_predict_features = generate_augmented_features(\n",
    "            predict_factors_norm, train_factors_norm, train_factors_mean, train_factors_std,\n",
    "            model, prediction_window, device\n",
    "        )\n",
    "        augmented_features_tensor_predict = torch.tensor(augmented_predict_features, dtype=torch.float32).to(device)\n",
    "        # augmented_features_tensor_predict = torch.tensor(augmented_predict_features, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "        augmented_features_tensor_predict = augmented_features_tensor_predict[-10:]  # Select the last 10 time steps\n",
    "        augmented_features_tensor_predict = augmented_features_tensor_predict.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Ensure input has (batch_size=1, sequence_length=60, d_model=12)\n",
    "        # augmented_features_tensor_predict = augmented_features_tensor_predict.view(1, 60, 12)  # Reshape explicitly\n",
    "\n",
    "        print(f\"Fixed Shape: {augmented_features_tensor_predict.shape}\")  # Should be\n",
    "\n",
    "\n",
    "        # Get model predictions\n",
    "        model_binary.eval()\n",
    "        with torch.no_grad():\n",
    "            return_predictions_prob = model_binary(augmented_features_tensor_predict).cpu().numpy().flatten()\n",
    "            # return_test_prob = model_binary(augmented_features_tensor).cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "        # Convert probabilities to binary labels (0 = down, 1 = up)\n",
    "        # Convert probability outputs to binary labels (0 = down, 1 = up)\n",
    "        print(f'return_predictions_prob: {return_predictions_prob}')\n",
    "        return_predictions_binary = (return_predictions_prob > 0.5).astype(int)\n",
    "        #\n",
    "        # # Display predictions\n",
    "        # print(f\"Predicted Binary Stock Returns: {return_predictions_binary}\")\n",
    "\n",
    "        # Print predictions\n",
    "        # print(f\"Predicted Stock Directions: {return_predictions_binary}\")\n",
    "\n",
    "        # ###### print SHAP RELATED METRICS\n",
    "        # # shap_values_squeeze = shap_values[0].reshape(1,rolling_window_test,d_model)\n",
    "        #\n",
    "        # # 1. Mean Absolute SHAP Values (Feature Importance)\n",
    "        # mean_abs_shap_values = np.mean(np.abs(shap_values[0]), axis=(0, 1))  # Shape: (6,)\n",
    "        # # print(f'mean_abs_shap_values: {mean_abs_shap_values}')\n",
    "        # # 2. SHAP Value Variance (Variability of Feature Contributions)\n",
    "        # shap_value_variance = np.var(shap_values[0], axis=(0, 1))  # Shape: (6,)\n",
    "        # # print(f'shap_value_variance: {shap_value_variance}')\n",
    "        # # 3. SHAP Interaction Values (Feature Interdependencies)\n",
    "        # # interaction_values = shap.TreeExplainer(model).shap_interaction_values(test_factors_tensor)\n",
    "        #\n",
    "        # # 4. Create DataFrame to Display Metrics\n",
    "        # feature_importance_df = pd.DataFrame({\n",
    "        #     'Date': test_factors.index[-1],\n",
    "        #     'Feature': factors_raw_X.columns,\n",
    "        #     'Mean_Abs_SHAP_Value': mean_abs_shap_values.flatten(),\n",
    "        #     'SHAP_Value_Variance': shap_value_variance.flatten()\n",
    "        #     # 'SHAP_Value_MEAN_DIV_VAR': mean_abs_shap_values.flatten() / shap_value_variance.flatten()\n",
    "        #\n",
    "        # })\n",
    "        #\n",
    "        # feature_importance_df_pivot = feature_importance_df.pivot(\n",
    "        #         index='Date',\n",
    "        #         columns='Feature',\n",
    "        #         values=['Mean_Abs_SHAP_Value', 'SHAP_Value_Variance']\n",
    "        #     ).reset_index()\n",
    "        # # Sort by Feature Importance\n",
    "        # # feature_importance_df = feature_importance_df.sort_values(by='Mean_Abs_SHAP_Value', ascending=False)\n",
    "        #\n",
    "        # # Flatten MultiIndex columns for readability\n",
    "        # feature_importance_df_pivot.columns = ['_'.join(col).strip() if col[1] else col[0] for col in feature_importance_df_pivot.columns.values]\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        # print(\"Feature Importance Metrics (Mean SHAP & Variance):\")\n",
    "        # print(feature_importance_df_pivot.to_string(index=False))\n",
    "        # # print(f'feature_importance_df: {feature_importance_df}')\n",
    "        #\n",
    "        # # # end shap\n",
    "        # ###### print SHAP RELATED METRICS\n",
    "\n",
    "        future_predictions = future_predictions_norm * train_returns_std.item() + train_returns_mean.item()\n",
    "        actual_test_returns = test_returns.values.flatten()\n",
    "        actual_predict_returns = predict_returns.values.flatten()\n",
    "        # print(f'test_predictions dim:{test_predictions.shape} actual_test_returns dim: {actual_test_returns.shape}')\n",
    "\n",
    "        # Compute loss for test phase\n",
    "        test_loss = np.mean((test_predictions - actual_test_returns) ** 2)  # MSE Loss\n",
    "        all_test_losses.append(test_loss)\n",
    "        print(f'test_loss: {test_loss}')\n",
    "        # Store test results\n",
    "        combined_test_result = {\n",
    "            \"Stock\": stock,\n",
    "            \"Test Dates\": (test_start, test_end),\n",
    "            \"Predicted Returns (Test Set)\": test_predictions.tolist(),\n",
    "            \"Actual Returns (Test Set)\": actual_test_returns.tolist(),\n",
    "            \"Loss\": test_loss\n",
    "        }\n",
    "\n",
    "        # Add SHAP feature importance metrics to the test results\n",
    "        # combined_test_result.update(feature_importance_df_pivot.iloc[0].to_dict())\n",
    "\n",
    "        # Append the combined results to the test_results list\n",
    "        test_results.append(combined_test_result)\n",
    "\n",
    "        # Store prediction results\n",
    "        # prediction_results.append({\n",
    "        #     \"Stock\": stock,\n",
    "        #     \"Prediction Dates\": (predict_start, predict_end),\n",
    "        #     \"Predicted Returns\": future_predictions.tolist(),\n",
    "        #     \"Actual Returns\": actual_predict_returns.tolist()\n",
    "        # })\n",
    "        # Match dates with predictions and actual returns\n",
    "        # date_range = pd.date_range(start=test_start, periods=len(actual_predict_returns), freq=\"D\")\n",
    "\n",
    "        for date, predicted, actual in zip(predict_returns.index, future_predictions, actual_predict_returns):\n",
    "            prediction_results.append({\n",
    "                \"Stock\": stock,\n",
    "                \"Date\": date,\n",
    "                \"Predicted Return\": predicted,\n",
    "                \"Actual Return\": actual\n",
    "            })\n",
    "        print(f\"Predictions for {stock} from {predict_start} to {predict_end} complete.\")\n",
    "        start += prediction_window\n",
    "\n",
    "    # Print average test loss for this stock\n",
    "    avg_test_loss = np.mean(all_test_losses)\n",
    "    print(f\"Average Test Loss for {stock}: {avg_test_loss:.6f}\")\n",
    "future_predictions_df = pd.DataFrame(prediction_results)\n",
    "\n",
    "future_predictions_df.to_csv(f\"{input_dir}/predicted_vs_actual.csv\", index=False)\n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "test_results_df.to_csv(f\"{input_dir}/test_result_output.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26aa8f8056df38cb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d4c1f3f095182193"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1ab6ec11245cab12"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b41d75f70b3e3ba0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9948a9a561630953"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "82b133dae0407aa3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "future_predictions_df = pd.DataFrame(prediction_results)  \n",
    "\n",
    "future_predictions_df.to_csv(f\"{input_dir}/predicted_vs_actual.csv\", index=False)\n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "test_results_df.to_csv(f\"{input_dir}/test_result_output.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caa74e88b9a287f0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "    test_results_df = pd.DataFrame(test_results)\n",
    "    test_results_df.to_csv(f\"{input_dir}/test_result_output.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fedcdc2c84025696",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "background_data.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42b218a3d851d61e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predict_returns.index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb23d28c080b5bcd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_test_losses"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae72d128656489ca",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3b4bcccbeb07eb78",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "91ee8b9bf103b554",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b3580bcab55687da",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ba296552db00c44a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7be84bf7c089d04c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "20fa6f906cded94d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "255c22cd4be81757",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7ccea0cd92a91d6d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class RollingStockFramework:\n",
    "    def __init__(self, model, factors, returns, rolling_window_train, rolling_window_test, M, N, batch_size, num_epochs, learning_rate, device):\n",
    "        self.model = model.to(device)\n",
    "        self.factors = factors\n",
    "        self.returns = returns\n",
    "        self.rolling_window_train = rolling_window_train\n",
    "        self.rolling_window_test = rolling_window_test\n",
    "        self.M = M  # Mini-batch sequence length\n",
    "        self.N = N  # Prediction horizon\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device\n",
    "\n",
    "    def create_rolling_windows(self, factors, returns, window_size):\n",
    "        X, Y = [], []\n",
    "        num_samples = window_size - self.M - self.N\n",
    "        \n",
    "        if num_samples <= 0:\n",
    "            print(f\"⚠️ Warning: Insufficient data for rolling windows! window_size={window_size}, M={self.M}, N={self.N}\")\n",
    "            return torch.empty((0, self.M, factors.shape[1] + 1)), torch.empty((0, self.N))  # Return correctly shaped empty tensors\n",
    "\n",
    "        for start in range(num_samples):\n",
    "            factor_values = torch.tensor(factors.iloc[start:start+self.M].values, dtype=torch.float32)\n",
    "            return_values = torch.tensor(returns.iloc[start:start+self.M].values, dtype=torch.float32)\n",
    "            \n",
    "            # print(f\"factor_values shape: {factor_values.shape}, return_values shape: {return_values.shape}\")\n",
    "            X.append(torch.cat([factor_values, return_values], dim=-1))\n",
    "            Y.append(torch.tensor(returns.iloc[start+self.M:start+self.M+self.N].values, dtype=torch.float32))\n",
    "\n",
    "        X_stack = torch.stack(X) if X else torch.empty((0, self.M, factors.shape[1] + 1))\n",
    "        Y_stack = torch.stack(Y) if Y else torch.empty((0, self.N))\n",
    "        \n",
    "        print(f\"Final X_train shape: {X_stack.shape}, Y_train shape: {Y_stack.shape}\")\n",
    "        return X_stack, Y_stack\n",
    "\n",
    "    def train(self, train_factors, train_returns):\n",
    "        X_train, Y_train = self.create_rolling_windows(train_factors, train_returns, self.rolling_window_train)\n",
    "        if X_train.shape[0] == 0:\n",
    "            print(\"⚠️ Skipping training due to insufficient data.\")\n",
    "            return\n",
    "        dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            total_loss = 0.0\n",
    "            for factors_batch, target_batch in dataloader:\n",
    "                factors_batch, target_batch = factors_batch.to(self.device), target_batch.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                predictions = self.model(factors_batch)\n",
    "                target_batch = target_batch.squeeze(-1) if predictions.shape[-1] != target_batch.shape[-1] else target_batch\n",
    "                loss = criterion(predictions, target_batch)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    def test(self, test_factors, test_returns):\n",
    "        X_test, Y_test = self.create_rolling_windows(test_factors, test_returns, self.rolling_window_test)\n",
    "        if X_test.shape[0] == 0:\n",
    "            print(\"⚠️ Skipping testing due to insufficient data.\")\n",
    "            return float('nan')\n",
    "        dataset = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        self.model.eval()\n",
    "\n",
    "        total_test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for factors_batch, target_batch in dataloader:\n",
    "                factors_batch, target_batch = factors_batch.to(self.device), target_batch.to(self.device)\n",
    "                predictions = self.model(factors_batch)\n",
    "                target_batch = target_batch.squeeze(-1) if predictions.shape[-1] != target_batch.shape[-1] else target_batch\n",
    "                loss = criterion(predictions, target_batch)\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(dataloader) if len(dataloader) > 0 else float('nan')\n",
    "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "        return avg_test_loss\n",
    "\n",
    "\n",
    "    def predict(self, predict_factors, predict_returns):\n",
    "        X_predict, _ = self.create_rolling_windows(predict_factors, predict_returns, self.rolling_window_test)\n",
    "        dataset = torch.utils.data.TensorDataset(X_predict, torch.zeros(X_predict.shape[0]))\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        predictions_list = []\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for factors_batch, _ in dataloader:\n",
    "                factors_batch = factors_batch.to(self.device)\n",
    "                predictions = self.model(factors_batch)\n",
    "                predictions_list.append(predictions.cpu().numpy())\n",
    "\n",
    "        predictions = torch.cat([torch.tensor(p) for p in predictions_list], dim=0).numpy()\n",
    "        return predictions\n",
    "\n",
    "    def run(self):\n",
    "        results = []\n",
    "        for start in range(0, len(self.factors) - self.rolling_window_train - self.rolling_window_test, self.N):\n",
    "            print(f\"\\n Rolling window starting at index {start}\")\n",
    "\n",
    "            train_factors = self.factors.iloc[start:start+self.rolling_window_train]\n",
    "            train_returns = self.returns.iloc[start:start+self.rolling_window_train]\n",
    "            test_factors = self.factors.iloc[start+self.rolling_window_train:start+self.rolling_window_train+self.rolling_window_test]\n",
    "            test_returns = self.returns.iloc[start+self.rolling_window_train:start+self.rolling_window_train+self.rolling_window_test]\n",
    "            predict_factors = self.factors.iloc[-self.rolling_window_test:]\n",
    "            predict_returns = self.returns.iloc[-self.rolling_window_test:]\n",
    "\n",
    "            print(\" Training Model...\")\n",
    "            self.train(train_factors, train_returns)\n",
    "\n",
    "            print(\"\\n Evaluating Model...\")\n",
    "            test_loss = self.test(test_factors, test_returns)\n",
    "\n",
    "            print(\"\\n Predicting Future Returns...\")\n",
    "            predictions = self.predict(predict_factors, predict_returns)\n",
    "\n",
    "            results.append({\n",
    "                \"train_window\": (start, start + self.rolling_window_train),\n",
    "                \"test_window\": (start + self.rolling_window_train, start + self.rolling_window_train + self.rolling_window_test),\n",
    "                \"predict_window\": (len(self.factors) - self.rolling_window_test, len(self.factors)),\n",
    "                \"test_loss\": test_loss,\n",
    "                \"predictions\": predictions\n",
    "            })\n",
    "\n",
    "        return self.model, results\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10be697420c3c7b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "M = 60  # Define the mini-batch past sequence length\n",
    "N = 20  # Define the future prediction horizon\n",
    "returns_raw = returns_raw.iloc[:, :1]\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors_raw = factors_raw.dropna()\n",
    "returns_raw = returns_raw.dropna()\n",
    "\n",
    "# Align dates between factors_raw and returns_raw\n",
    "dates = factors_raw.index.intersection(returns_raw.index)\n",
    "factors_raw = factors_raw.loc[dates]\n",
    "returns_raw = returns_raw.loc[dates]\n",
    "\n",
    "for stock in returns_raw.columns:\n",
    "    print(f\"Training for stock: {stock}\")\n",
    "    stock_returns = returns_raw[[stock]]  # Select the stock column\n",
    "    \n",
    "    # Ensure d_model matches 6 factors + 1 stock return\n",
    "    d_model = factors_raw.shape[1] + 1  # Fix to match input shape\n",
    "    # print(f'd_model dim: {d_model}')\n",
    "    num_heads = min(4, d_model)  # Ensure num_heads does not exceed d_model\n",
    "    while d_model % num_heads != 0:\n",
    "        num_heads -= 1\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize the model before passing into the pipeline\n",
    "    model = StockPredictAgent(d_model=d_model, num_heads=num_heads, sequence_length=M, N=N).to(device)  # NEW\n",
    "    \n",
    "    # Now you can safely run the pipeline\n",
    "    pipeline = RollingStockFramework(\n",
    "        model, factors_raw, stock_returns, rolling_window_train=240, rolling_window_test=120,\n",
    "        M=M, N=N, batch_size=32, num_epochs=50, learning_rate=0.001, device=device\n",
    "    )\n",
    "    \n",
    "    trained_model, rolling_results = pipeline.run()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af84469e8c6618f6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Select the first N columns of returns_raw\n",
    "M = 60  # Define the mini-batch past sequence length\n",
    "N = 20  # Define the future prediction horizon\n",
    "returns_raw = returns_raw.iloc[:, :1]\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors_raw = factors_raw.dropna()\n",
    "returns_raw = returns_raw.dropna()\n",
    "\n",
    "# Align dates between factors_raw and returns_raw\n",
    "dates = factors_raw.index.intersection(returns_raw.index)\n",
    "factors_raw = factors_raw.loc[dates]\n",
    "returns_raw = returns_raw.loc[dates]\n",
    "# print(f'factors_raw {factors_raw.shape}')\n",
    "\n",
    "sequence_length = 20\n",
    "batch_size = 20\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "rolling_window_train = 120  # Number of training samples\n",
    "rolling_window_test = 60   # Number of testing samples\n",
    "prediction_window = 60     # Number of prediction samples\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Record results\n",
    "results = []\n",
    "prediction_records = []\n",
    "shapley_records = []\n",
    "shapley_metrics_df = []\n",
    "\n",
    "# Iterate over each stock in returns_raw\n",
    "for stock in returns_raw.columns:\n",
    "    print(f\"Training for stock: {stock}\")\n",
    "    stock_returns = returns_raw[[stock]]  # Select the stock column\n",
    "\n",
    "    # Rolling split: Train (M days), Test (N days), Predict (W days)\n",
    "    total_samples = len(factors_raw)\n",
    "    for start in range(0, total_samples - rolling_window_train - rolling_window_test - prediction_window + 1):\n",
    "        train_start_X = start\n",
    "        train_end_X = start + rolling_window_train\n",
    "        train_start_Y = train_end_X\n",
    "        train_end_Y = train_start_Y + N\n",
    "        \n",
    "        test_start_X = train_end_X\n",
    "        test_end_X = train_end_X + rolling_window_test\n",
    "        test_start_Y = test_end_X\n",
    "        test_end_Y = test_start_Y + N\n",
    "        \n",
    "        predict_start_X = test_end_X\n",
    "        predict_end_X = test_end_X + prediction_window\n",
    "        test_start_Y = predict_end_X\n",
    "        test_end_Y = predict_end_X + N\n",
    "\n",
    "        # Print start and end dates for training, testing, and prediction\n",
    "        train_dates = (factors_raw.iloc[train_start_X].name, factors_raw.iloc[train_end_X - 1].name)\n",
    "        test_dates = (factors_raw.iloc[test_start].name, factors_raw.iloc[test_end - 1].name)\n",
    "        predict_dates = (factors_raw.iloc[predict_start].name, factors_raw.iloc[predict_end - 1].name)\n",
    "\n",
    "        print(f\"Training dates: {train_dates}\")\n",
    "        print(f\"Testing dates: {test_dates}\")\n",
    "        print(f\"Prediction dates: {predict_dates}\")\n",
    "\n",
    "        # Split data into training, testing, and prediction sets\n",
    "        train_factors = factors_raw.iloc[train_start:train_end]\n",
    "        train_returns = stock_returns.iloc[train_start:train_end]\n",
    "        test_factors = factors_raw.iloc[test_start:test_end]\n",
    "        test_returns = stock_returns.iloc[test_start:test_end]\n",
    "        predict_factors = factors_raw.iloc[predict_start:predict_end]\n",
    "        predict_returns = stock_returns.iloc[predict_start:predict_end]\n",
    "\n",
    "        # Normalize train and test data\n",
    "        train_factors_mean, train_factors_std = train_factors.mean(), train_factors.std()\n",
    "        train_returns_mean, train_returns_std = train_returns.mean(), train_returns.std()\n",
    "\n",
    "        train_factors = (train_factors - train_factors_mean) / train_factors_std\n",
    "        train_returns = (train_returns - train_returns_mean) / train_returns_std\n",
    "        test_factors = (test_factors - train_factors_mean) / train_factors_std\n",
    "        test_returns = (test_returns - train_returns_mean) / train_returns_std\n",
    "\n",
    "        # Initialize dataset and dataloader\n",
    "        dataset = StockDataset(train_factors, train_returns, sequence_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Model initialization\n",
    "        d_model = train_factors.shape[1] \n",
    "        num_heads = min(4, d_model)  # Ensure num_heads does not exceed d_model and d_model is divisible by num_heads\n",
    "        while d_model % num_heads != 0:\n",
    "            num_heads -= 1\n",
    "        # print(f\"d_model: {d_model} num_heads: {num_heads} sequence_length: {sequence_length} and train factor return dim: {train_factors.shape[1]} {train_returns.shape[1]}\")\n",
    "        model = StockPredictAgent(d_model=d_model, num_heads=num_heads, sequence_length=M, N=N).to(device)\n",
    "\n",
    "        # Optimizer and loss function\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Training the model\n",
    "        print(f\"Training window: {train_start} to {train_end}\")\n",
    "        train_model_with_logging(model, dataloader, optimizer, criterion, num_epochs, device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac90302b8c755a24",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "        \n",
    "# Select the first N columns of returns_raw\n",
    "N = 2  # Number of columns to select\n",
    "returns_raw = returns_raw.iloc[:, :N]\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors_raw = factors_raw.dropna()\n",
    "returns_raw = returns_raw.dropna()\n",
    "\n",
    "# Align dates between factors_raw and returns_raw\n",
    "dates = factors_raw.index.intersection(returns_raw.index)\n",
    "factors_raw = factors_raw.loc[dates]\n",
    "returns_raw = returns_raw.loc[dates]\n",
    "\n",
    "sequence_length = 20\n",
    "batch_size = 20\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "rolling_window_train = 120  # Number of training samples\n",
    "rolling_window_test = 60   # Number of testing samples\n",
    "prediction_window = 60     # Number of prediction samples\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Record results\n",
    "results = []\n",
    "prediction_records = []\n",
    "shapley_records = []\n",
    "shapley_metrics_df = []\n",
    "\n",
    "# Iterate over each stock in returns_raw\n",
    "for stock in returns_raw.columns:\n",
    "    print(f\"Training for stock: {stock}\")\n",
    "    stock_returns = returns_raw[[stock]]  # Select the stock column\n",
    "\n",
    "    # Rolling split: Train (M days), Test (N days), Predict (W days)\n",
    "    total_samples = len(factors_raw)\n",
    "    for start in range(0, total_samples - rolling_window_train - rolling_window_test - prediction_window + 1):\n",
    "        train_start = start\n",
    "        train_end = start + rolling_window_train\n",
    "        test_start = train_end\n",
    "        test_end = train_end + rolling_window_test\n",
    "        predict_start = test_end\n",
    "        predict_end = test_end + prediction_window\n",
    "\n",
    "        # Print start and end dates for training, testing, and prediction\n",
    "        train_dates = (factors_raw.iloc[train_start].name, factors_raw.iloc[train_end - 1].name)\n",
    "        test_dates = (factors_raw.iloc[test_start].name, factors_raw.iloc[test_end - 1].name)\n",
    "        predict_dates = (factors_raw.iloc[predict_start].name, factors_raw.iloc[predict_end - 1].name)\n",
    "\n",
    "        print(f\"Training dates: {train_dates}\")\n",
    "        print(f\"Testing dates: {test_dates}\")\n",
    "        print(f\"Prediction dates: {predict_dates}\")\n",
    "\n",
    "        # Split data into training, testing, and prediction sets\n",
    "        train_factors = factors_raw.iloc[train_start:train_end]\n",
    "        train_returns = stock_returns.iloc[train_start:train_end]\n",
    "        test_factors = factors_raw.iloc[test_start:test_end]\n",
    "        test_returns = stock_returns.iloc[test_start:test_end]\n",
    "        predict_factors = factors_raw.iloc[predict_start:predict_end]\n",
    "        predict_returns = stock_returns.iloc[predict_start:predict_end]\n",
    "\n",
    "        # Normalize train and test data\n",
    "        train_factors_mean, train_factors_std = train_factors.mean(), train_factors.std()\n",
    "        train_returns_mean, train_returns_std = train_returns.mean(), train_returns.std()\n",
    "\n",
    "        train_factors = (train_factors - train_factors_mean) / train_factors_std\n",
    "        train_returns = (train_returns - train_returns_mean) / train_returns_std\n",
    "        test_factors = (test_factors - train_factors_mean) / train_factors_std\n",
    "        test_returns = (test_returns - train_returns_mean) / train_returns_std\n",
    "\n",
    "        # Initialize dataset and dataloader\n",
    "        dataset = StockDataset(train_factors, train_returns, sequence_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Model initialization\n",
    "        d_model = train_factors.shape[1] \n",
    "        num_heads = min(4, d_model)  # Ensure num_heads does not exceed d_model and d_model is divisible by num_heads\n",
    "        while d_model % num_heads != 0:\n",
    "            num_heads -= 1\n",
    "        # print(f\"d_model: {d_model} num_heads: {num_heads} sequence_length: {sequence_length} and train factor return dim: {train_factors.shape[1]} {train_returns.shape[1]}\")\n",
    "        model = StockPredictAgent(d_model=d_model, num_heads=num_heads, sequence_length=sequence_length).to(device)\n",
    "\n",
    "        # Optimizer and loss function\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Training the model\n",
    "        print(f\"Training window: {train_start} to {train_end}\")\n",
    "        train_model_with_logging(model, dataloader, optimizer, criterion, num_epochs, device)\n",
    "\n",
    "        # Testing phase\n",
    "        if len(test_factors) >= sequence_length:\n",
    "            test_dataset = StockDataset(test_factors, test_returns, sequence_length)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "            # Wrap the model for SHAP\n",
    "            model_wrapper = StockPredictWrapper(model, device)\n",
    "\n",
    "            # SHAP Explainer Initialization\n",
    "            explainer = shap.Explainer(model_wrapper, test_factors.to_numpy())\n",
    "            model.eval()\n",
    "            # shap = Shapley(test_factors.to_numpy())  # Initialize Shapley calculation\n",
    "            with torch.no_grad():\n",
    "                total_test_loss = 0.0\n",
    "                for factors, target in test_dataloader:\n",
    "                    factors, target = factors.to(device),target.to(device)\n",
    "                    predictions = model(factors)\n",
    "                    loss = criterion(predictions, target)\n",
    "                    total_test_loss += loss.item()\n",
    "                    \n",
    "                    print(f\"SHAP input factors shape: {test_factors.shape}\")\n",
    "                    print(f\"SHAP input returns shape: {test_returns.shape}\")\n",
    "\n",
    "        \n",
    "                    # # Calculate Shapley values for each factor\n",
    "                    # shap_values = explainer(test_factors.to_numpy())\n",
    "                    # \n",
    "                    # # Record Shapley metrics\n",
    "                    # shap_metrics = calculate_shapley_metrics(shap_values, test_factors)\n",
    "                    # shapley_records.append(shap_metrics[\"shapley_df\"])\n",
    "                    # \n",
    "                    # # Append metrics to records\n",
    "                    # metrics_df = pd.DataFrame({\n",
    "                    #     \"date\": test_factors.index,\n",
    "                    #     \"mean_abs_shap\": shap_metrics[\"mean_abs_shap\"],\n",
    "                    #     \"shapley_variance\": shap_metrics[\"shapley_variance\"],\n",
    "                    # })\n",
    "                    # metrics_df[\"top_factors_per_sample\"] = shap_metrics[\"top_factors_per_sample\"].apply(lambda x: \", \".join(x))\n",
    "                    # shapley_metrics_df.append(metrics_df)\n",
    "\n",
    "                avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "                print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Skipping testing phase for stock {stock} due to insufficient data for sequence length.\")\n",
    "\n",
    "        # Prediction phase\n",
    "        if len(predict_factors) >= sequence_length:\n",
    "            predict_factors = (predict_factors - train_factors_mean) / train_factors_std\n",
    "            predict_returns = (predict_returns - train_returns_mean) / train_returns_std\n",
    "\n",
    "            predict_dataset = StockDataset(predict_factors, predict_returns, sequence_length)\n",
    "            predict_dataloader = DataLoader(predict_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            predicted_returns = []\n",
    "            actual_returns = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for factors, target in predict_dataloader:\n",
    "                    factors, target = factors.to(device),  target.to(device)\n",
    "                    predictions = model(factors)\n",
    "                    predicted_returns.append(predictions.cpu().numpy())\n",
    "                    actual_returns.append(target.cpu().numpy())\n",
    "\n",
    "            # Revert normalization for predicted and actual returns\n",
    "            # predicted_returns = np.concatenate(predicted_returns) * train_returns_std.values[0] + train_returns_mean.values[0]\n",
    "            # actual_returns = np.concatenate(actual_returns) * train_returns_std.values[0] + train_returns_mean.values[0]\n",
    "\n",
    "            predicted_returns = np.concatenate(predicted_returns, axis=0).flatten() * train_returns_std.values[0] + train_returns_mean.values[0]\n",
    "            actual_returns = np.concatenate(actual_returns, axis=0).flatten() * train_returns_std.values[0] + train_returns_mean.values[0]\n",
    "\n",
    "            # Combine dates with returns\n",
    "            prediction_data = pd.DataFrame({\n",
    "                \"date\": predict_factors.index[-len(predicted_returns):],\n",
    "                \"predicted_returns\": predicted_returns,\n",
    "                \"actual_returns\": actual_returns\n",
    "            })\n",
    "            prediction_records.append(prediction_data)\n",
    "\n",
    "            # Save results for this window\n",
    "            results.append({\n",
    "                \"stock\": stock,\n",
    "                \"train_window\": (train_start, train_end),\n",
    "                \"test_window\": (test_start, test_end),\n",
    "                \"predict_window\": (predict_start, predict_end),\n",
    "                \"predicted_returns\": predicted_returns,\n",
    "                \"actual_returns\": actual_returns,\n",
    "                \"test_loss\": avg_test_loss\n",
    "            })\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f\"{input_dir}/model_results.csv\", index=False)\n",
    "\n",
    "# Save combined predicted and actual returns\n",
    "combined_predictions = pd.concat(prediction_records, ignore_index=True)\n",
    "combined_predictions.to_csv(f\"{input_dir}/predicted_vs_actual.csv\", index=False)\n",
    "\n",
    "# Save Shapley time series\n",
    "if shapley_records:\n",
    "    combined_shapley = pd.concat(shapley_records, ignore_index=False)\n",
    "    combined_shapley.to_csv(f\"{input_dir}/shapley_time_series.csv\")\n",
    "\n",
    "# Save Shapley metrics as a combined CSV\n",
    "if shapley_metrics_df:\n",
    "    combined_metrics_df = pd.concat(shapley_metrics_df, ignore_index=True)\n",
    "    combined_metrics_df.to_csv(f\"{input_dir}/shapley_metrics.csv\", index=False)\n",
    "\n",
    "print(\"All stocks training, testing, and prediction completed.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c856a147a66d332"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "returns_raw.columns[1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f31659127e2da5b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Select the first N columns of returns_raw\n",
    "N = 2  # Number of columns to select\n",
    "returns_raw = returns_raw.iloc[:, :N]\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors_raw = factors_raw.dropna()\n",
    "returns_raw = returns_raw.dropna()\n",
    "\n",
    "# Align dates between factors_raw and returns_raw\n",
    "dates = factors_raw.index.intersection(returns_raw.index)\n",
    "factors_raw = factors_raw.loc[dates]\n",
    "returns_raw = returns_raw.loc[dates]\n",
    "\n",
    "sequence_length = 20\n",
    "batch_size = 20\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "rolling_window_train = 120  # Number of training samples\n",
    "rolling_window_test = 60   # Number of testing samples\n",
    "prediction_window = 60     # Number of prediction samples\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Record results\n",
    "results = []\n",
    "prediction_records = []\n",
    "shapley_records = []\n",
    "shapley_metrics_df = []\n",
    "\n",
    "# Iterate over each stock in returns_raw\n",
    "stock=returns_raw.columns[0]\n",
    "print(f\"Training for stock: {stock}\")\n",
    "stock_returns = returns_raw[[stock]]  # Select the stock column\n",
    "\n",
    "# Rolling split: Train (M days), Test (N days), Predict (W days)\n",
    "total_samples = len(factors_raw)\n",
    "start= 0\n",
    "train_start = start\n",
    "train_end = start + rolling_window_train\n",
    "test_start = train_end\n",
    "test_end = train_end + rolling_window_test\n",
    "predict_start = test_end\n",
    "predict_end = test_end + prediction_window\n",
    "\n",
    "# Print start and end dates for training, testing, and prediction\n",
    "train_dates = (factors_raw.iloc[train_start].name, factors_raw.iloc[train_end - 1].name)\n",
    "test_dates = (factors_raw.iloc[test_start].name, factors_raw.iloc[test_end - 1].name)\n",
    "predict_dates = (factors_raw.iloc[predict_start].name, factors_raw.iloc[predict_end - 1].name)\n",
    "\n",
    "print(f\"Training dates: {train_dates}\")\n",
    "print(f\"Testing dates: {test_dates}\")\n",
    "print(f\"Prediction dates: {predict_dates}\")\n",
    "\n",
    "# Split data into training, testing, and prediction sets\n",
    "train_factors = factors_raw.iloc[train_start:train_end]\n",
    "train_returns = stock_returns.iloc[train_start:train_end]\n",
    "test_factors = factors_raw.iloc[test_start:test_end]\n",
    "test_returns = stock_returns.iloc[test_start:test_end]\n",
    "predict_factors = factors_raw.iloc[predict_start:predict_end]\n",
    "predict_returns = stock_returns.iloc[predict_start:predict_end]\n",
    "\n",
    "# Normalize train and test data\n",
    "train_factors_mean, train_factors_std = train_factors.mean(), train_factors.std()\n",
    "train_returns_mean, train_returns_std = train_returns.mean(), train_returns.std()\n",
    "\n",
    "train_factors = (train_factors - train_factors_mean) / train_factors_std\n",
    "train_returns = (train_returns - train_returns_mean) / train_returns_std\n",
    "test_factors = (test_factors - train_factors_mean) / train_factors_std\n",
    "test_returns = (test_returns - train_returns_mean) / train_returns_std\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = StockDataset(train_factors, train_returns, sequence_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "d_model = train_factors.shape[1] \n",
    "num_heads = min(4, d_model)  # Ensure num_heads does not exceed d_model and d_model is divisible by num_heads\n",
    "while d_model % num_heads != 0:\n",
    "    num_heads -= 1\n",
    "# print(f\"d_model: {d_model} num_heads: {num_heads} sequence_length: {sequence_length} and train factor return dim: {train_factors.shape[1]} {train_returns.shape[1]}\")\n",
    "model = StockPredictAgent(d_model=d_model, num_heads=num_heads, sequence_length=sequence_length).to(device)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "534a109a4dbdc682",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c2c5c3392438542",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "train_model_with_logging(model, dataloader, optimizer, criterion, num_epochs, device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ad7aefcfba3ff56",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(test_factors)\n",
    "sequence_length"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "543614357c85083f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "\n",
    "test_dataset = StockDataset(test_factors, test_returns, sequence_length)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Wrap the model for SHAP (pass test_returns)\n",
    "model_wrapper = StockPredictWrapper(model, device, test_returns.to_numpy())\n",
    "\n",
    "# Use `DeepExplainer`, which is designed for deep learning models\n",
    "explainer = shap.DeepExplainer(model_wrapper, torch.tensor(test_factors.to_numpy(), dtype=torch.float32).to(device))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_test_loss = 0.0\n",
    "    for factors, returns, target in test_dataloader:\n",
    "        factors, returns, target = factors.to(device), returns.to(device), target.to(device)\n",
    "        predictions = model(factors, returns)\n",
    "        loss = criterion(predictions, target)\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "    # Compute SHAP values using DeepExplainer\n",
    "    shap_values = explainer.shap_values(torch.tensor(test_factors.to_numpy(), dtype=torch.float32).to(device))\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Visualize Feature Importance\n",
    "shap.summary_plot(shap_values, test_factors)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17f951dc564657d5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_factors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc38d9247365b791",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define a PyTorch model wrapper for SHAP\n",
    "class StockPredictWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(StockPredictWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        factors, returns = inputs  # Extract both inputs\n",
    "\n",
    "        factors = torch.tensor(factors, dtype=torch.float32).to(device)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "\n",
    "        if len(factors.shape) == 2:\n",
    "            factors = factors.unsqueeze(1)\n",
    "        if len(returns.shape) == 2:\n",
    "            returns = returns.unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            return self.model(factors, returns).cpu()\n",
    "\n",
    "# Wrap the model\n",
    "model_wrapper = StockPredictWrapper(model).to(device)\n",
    "\n",
    "# Select a subset of training data for background (60 rows)\n",
    "background_factors = torch.tensor(test_factors.sample(60).values, dtype=torch.float32).to(device)\n",
    "background_returns = torch.tensor(test_returns.sample(60).values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Initialize `DeepExplainer` with both inputs\n",
    "explainer = shap.DeepExplainer(model_wrapper, [background_factors, background_returns])\n",
    "\n",
    "# Convert test data to tensor\n",
    "test_factors_tensor = torch.tensor(test_factors.values, dtype=torch.float32).to(device)\n",
    "test_returns_tensor = torch.tensor(test_returns.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer.shap_values([test_factors_tensor, test_returns_tensor])\n",
    "\n",
    "# Visualize Feature Importance\n",
    "shap.summary_plot(shap_values, test_factors)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcc9833be44ba851",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e8f3ae1bb1c223d2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "544012e9e37438ab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cc4bb554bc0a9e81"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5b8e233d503684ba"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5b91ef7bf08edf34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "43dff8021fea3722"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_dir = \"E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\"  # Use the specified directory\n",
    "ticker_file = os.path.join(input_dir, \"tickers.txt\")  # Text file containing tickers, one per line\n",
    "factor_file = os.path.join(input_dir, \"FF_factors.csv\")  # CSV file containing factor returns\n",
    "start_date = \"2005-01-01\"\n",
    "end_date = \"2014-12-31\"\n",
    "output_csv = os.path.join(input_dir, \"stock_prices.csv\")\n",
    "\n",
    "# try:\n",
    "#     tickers = load_tickers_from_file(ticker_file)\n",
    "#     price_data = download_stock_prices(tickers, start_date, end_date, output_csv)\n",
    "#     daily_returns = calculate_daily_returns(price_data, return_type='arithmetic')\n",
    "#     daily_returns.to_csv(os.path.join(input_dir, \"daily_returns.csv\"))\n",
    "#     factor_returns = read_factor_returns(factor_file, daily_returns)\n",
    "#     factor_returns.to_csv(os.path.join(input_dir, \"aligned_factors.csv\"))\n",
    "#     print(\"Factor returns alignment completed.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")\n",
    "\n",
    "factors = pd.read_csv(f\"{input_dir}/aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns_raw = pd.read_csv(f\"{input_dir}/daily_returns.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors = factors.dropna()\n",
    "returns = returns_raw.dropna()\n",
    "\n",
    "# Align dates between factors and returns\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors_raw = factors.loc[dates]\n",
    "returns_raw = returns.loc[dates]\n",
    "factors_raw = factors_raw.iloc[:, :-1]\n",
    "# print(f\"factors_raw = {factors_raw}\")\n",
    "\n",
    "Nday_diff = 10  # Define the future prediction horizon for shift days\n",
    "# limit to 2 stock ONLY\n",
    "# returns_raw = returns_raw.iloc[:, :2]\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors_raw = factors_raw.dropna()\n",
    "returns_raw = returns_raw.dropna()\n",
    "# factors_raw = pd.concat([factors_raw, returns_raw], axis=1)\n",
    "# Align dates between factors_raw and returns_raw\n",
    "dates = factors_raw.index.intersection(returns_raw.index)\n",
    "factors_raw = factors_raw.loc[dates]\n",
    "returns_raw = returns_raw.loc[dates]\n",
    "\n",
    "# Shift returns to generate target variable (N-day future return)\n",
    "returns_raw_Y = returns_raw.shift(-Nday_diff).dropna()\n",
    "\n",
    "# Align all datasets with available dates in returns_raw_Y\n",
    "dates_final = factors_raw.index.intersection(returns_raw_Y.index)\n",
    "factors_raw = factors_raw.loc[dates_final]\n",
    "returns_raw = returns_raw.loc[dates_final]\n",
    "returns_raw_Y = returns_raw_Y.loc[dates_final]\n",
    "\n",
    "N = 10\n",
    "sequence_length = 10\n",
    "batch_size = 64\n",
    "num_epochs = 500\n",
    "learning_rate = 0.001\n",
    "rolling_window_train = 360  # Reduce for faster adaptation\n",
    "rolling_window_test = 10    # Reduce test window\n",
    "prediction_window = 10      # Align prediction period with test window\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Record results\n",
    "results = []\n",
    "prediction_records = []\n",
    "shapley_records = []\n",
    "shapley_metrics_df = []\n",
    "all_test_losses = []\n",
    "all_binary_test_losses = []\n",
    "results = []\n",
    "classification_results = []\n",
    "\n",
    "test_results = []\n",
    "prediction_results = []\n",
    "combined_test_result = []\n",
    "portfolio_records = []\n",
    "start = 0  # Initialize rolling split index\n",
    "\n",
    "# state = 'train_data_generate'\n",
    "state = 'train_RL'\n",
    "# state = 'final_trade'\n",
    "\n",
    "\n",
    "if state == 'train_data_generate':\n",
    "    # Rolling split: Train (M days), Test (N days), Predict (W days)\n",
    "    total_samples = len(factors_raw)\n",
    "    while start + rolling_window_train + rolling_window_test + prediction_window <= total_samples:\n",
    "\n",
    "        train_start = start\n",
    "        train_end = start + rolling_window_train\n",
    "\n",
    "        test_start = train_end\n",
    "        test_end = test_start + rolling_window_test\n",
    "\n",
    "        predict_start = test_end  # Predictions start right after testing\n",
    "        predict_end = predict_start + prediction_window\n",
    "\n",
    "        # Extract the actual date indices\n",
    "        train_dates = (factors_raw.iloc[train_start].name, factors_raw.iloc[train_end - 1].name)\n",
    "        test_dates = (factors_raw.iloc[test_start].name, factors_raw.iloc[test_end - 1].name)\n",
    "        predict_dates = (factors_raw.iloc[predict_start].name, factors_raw.iloc[predict_end - 1].name)\n",
    "\n",
    "        print(f\"\\n🔹 Processing Date Window: Train {train_dates}, Test {test_dates}, Predict {predict_dates}\")\n",
    "\n",
    "        for stock in returns_raw.columns:\n",
    "            print(f\"Training for stock: {stock}\")\n",
    "            stock_returns = returns_raw_Y[[stock]]  # Select the stock column\n",
    "            stock_returns_unshift = returns_raw[[stock]]\n",
    "            factors_raw_X = pd.concat([factors_raw, stock_returns_unshift], axis=1)\n",
    "            # Rolling split: Train (M days), Test (N days), Predict (W days)\n",
    "            # total_samples = len(factors_raw_X)\n",
    "            # Initialize the rolling split index\n",
    "\n",
    "            # Print the selected date ranges\n",
    "            print(f\"Training dates: {train_dates}\")\n",
    "            print(f\"Testing dates: {test_dates}\")\n",
    "            print(f\"Prediction dates: {predict_dates}\")\n",
    "\n",
    "            # # Split data into training, testing, and prediction sets\n",
    "            train_factors = factors_raw_X.iloc[train_start:train_end]\n",
    "            train_returns = stock_returns.iloc[train_start:train_end]\n",
    "            test_factors = factors_raw_X.iloc[test_start:test_end]\n",
    "            test_returns = stock_returns.iloc[test_start:test_end]\n",
    "            predict_factors = factors_raw_X.iloc[predict_start:predict_end]\n",
    "            predict_returns = stock_returns.iloc[predict_start:predict_end]\n",
    "            #\n",
    "            # # Normalize train and test data\n",
    "            train_factors_mean, train_factors_std = train_factors.mean(), train_factors.std()\n",
    "            train_returns_mean, train_returns_std = train_returns.mean(), train_returns.std()\n",
    "\n",
    "            # Avoid division by zero\n",
    "            train_factors_std.replace(0, 1e-8, inplace=True)\n",
    "            train_returns_std.replace(0, 1e-8, inplace=True)\n",
    "\n",
    "            train_factors_norm = (train_factors - train_factors_mean) / train_factors_std\n",
    "            train_returns_norm = (train_returns - train_returns_mean) / train_returns_std\n",
    "            test_factors_norm = (test_factors - train_factors_mean) / train_factors_std\n",
    "            test_returns_norm = (test_returns - train_returns_mean) / train_returns_std\n",
    "            predict_factors_norm = (predict_factors - train_factors_mean) / train_factors_std\n",
    "            predict_returns_norm = (predict_returns - train_returns_mean) / train_returns_std\n",
    "            # print(f'train_factors_norm dim: {train_factors_norm.shape}')\n",
    "            # print(f'train_returns_norm dim: {train_returns_norm.shape}')\n",
    "            dataset = StockDataset(train_factors_norm, train_returns_norm, sequence_length, N=N)\n",
    "\n",
    "            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            # for batch_x, batch_y in dataloader:\n",
    "            #     print(f\"Batch X Shape: {batch_x.shape}\")  # Expected: (batch_size, sequence_length, num_features)\n",
    "            #     print(f\"Batch Y Shape: {batch_y.shape}\")  # Expected: (batch_size, N, num_features)\n",
    "            #     break  # Only print for the first batch\n",
    "            # # Model initialization\n",
    "            d_model = train_factors.shape[1]\n",
    "            # print(f'd_model: {d_model}')\n",
    "\n",
    "            num_heads = min(4,\n",
    "                            d_model)  # Ensure num_heads does not exceed d_model and d_model is divisible by num_heads\n",
    "            while d_model % num_heads != 0:\n",
    "                num_heads -= 1\n",
    "            # print(f\"d_model: {d_model} num_heads: {num_heads} sequence_length: {sequence_length} and train factor return dim: {train_factors.shape[1]} {train_returns.shape[1]}\")\n",
    "            model = StockPredictAgent(d_model=d_model, num_heads=num_heads, sequence_length=sequence_length, N=N).to(\n",
    "                device)\n",
    "            #\n",
    "            # # Optimizer and loss function\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "            criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss for stability\n",
    "            #\n",
    "            # # Training the model\n",
    "            # print(f\"Training window: {train_start} to {train_end}\")\n",
    "            # print(f\"Expected FC Layer Input Shape: {sequence_length * d_model}\")\n",
    "            # print(f\"Flattened Tensor Shape: {batch_size, sequence_length * d_model}\")\n",
    "\n",
    "            train_model_with_logging(model, dataloader, optimizer, criterion, scheduler, num_epochs, device)\n",
    "\n",
    "            print(f'Finished training model 1')\n",
    "            # print(f'train_factors_norm dim: {train_factors_norm.shape}')\n",
    "            # Convert to PyTorch tensors\n",
    "            # print(f'test_factors_tensor: {test_factors_tensor.shape}')\n",
    "            # Run inference on test data\n",
    "            augmented_train_features = generate_augmented_features(\n",
    "                train_factors_norm, train_factors_mean, train_factors_std, model, rolling_window_test, device\n",
    "            )\n",
    "            augmented_features_tensor = torch.tensor(augmented_train_features, dtype=torch.float32).to(device)\n",
    "\n",
    "            print(\n",
    "                f\"Final Augmented Tensor Shape: {augmented_train_features.shape}\")  # Expected: (rolling_window_test, d_model * 2)\n",
    "\n",
    "            # Convert test_returns_norm into binary classification\n",
    "            mean_return_binary = np.mean(train_returns_norm)  # Aggregate 60-day return\n",
    "            train_returns_threshold = (0 - train_returns_mean) / train_returns_std  # Convert zero into normalized scale\n",
    "            train_returns_binary = (mean_return_binary > train_returns_threshold).astype(\n",
    "                int)  # 1 if positive, 0 if negative\n",
    "            # print(f'train_returns_norm = {train_returns_norm}')\n",
    "            print(f\"Cumulative Return over 60 days: {mean_return_binary}\")\n",
    "            print(f\"Binary Target (Up=1, Down=0): {train_returns_binary}\")\n",
    "\n",
    "            # Train the Model Using Augmented Features\n",
    "            # sequence_length = 20\n",
    "            # batch_size = 32\n",
    "            # num_epochs = 25\n",
    "            # learning_rate = 0.001\n",
    "            d_model2 = augmented_train_features.shape[1]  # Number of features (6 original + 6 SHAP)\n",
    "            num_heads = min(4, d_model2)\n",
    "            while d_model2 % num_heads != 0:\n",
    "                num_heads -= 1\n",
    "\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            print(f\"Length of augmented_test_features: {augmented_train_features.shape}\")\n",
    "            print(f\"Length of sequence: {sequence_length}\")\n",
    "\n",
    "            # sequence_length2 = min(10, len(augmented_test_features))  # Ensure it's smaller than available data\n",
    "            sequence_length2 = prediction_window\n",
    "            # Create dataset and dataloader for training\n",
    "            dataset2 = StockDatasetBinary(augmented_train_features, train_returns_norm, sequence_length2)\n",
    "            dataloader2 = DataLoader(dataset2, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            for batch_x, batch_y in dataloader2:\n",
    "                print(f\"Batch X Shape: {batch_x.shape}\")  # Expected: (batch_size, sequence_length, num_features)\n",
    "                print(f\"Batch Y Shape: {batch_y.shape}\")  # Expected: (batch_size, N, num_features)\n",
    "                break  # Only print for the first batch\n",
    "\n",
    "            # Initialize the binary classification model\n",
    "            model_binary = StockPredictAgentBinary(d_model=d_model2, num_heads=num_heads,\n",
    "                                                   sequence_length=sequence_length2).to(device)\n",
    "\n",
    "            # Define optimizer and loss function\n",
    "            optimizer = optim.Adam(model_binary.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "            criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "\n",
    "            # Train the model\n",
    "            train_model(model_binary, dataloader2, optimizer, criterion, num_epochs, device)\n",
    "            print(f'Finished training model 2')\n",
    "\n",
    "            test_factors_tensor = torch.tensor(test_factors_norm.values, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "            predict_factors_tensor = torch.tensor(predict_factors_norm.values, dtype=torch.float32).to(\n",
    "                device).unsqueeze(0)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_predictions_norm = model(test_factors_tensor).cpu().numpy().flatten()\n",
    "                future_predictions_norm = model(predict_factors_tensor).cpu().numpy().flatten()\n",
    "\n",
    "                # test_output = model(torch.tensor(test_factors_np, dtype=torch.float32).to(device))\n",
    "                # print(f\"Model output shape: {test_output.shape}\")  # Expected: (1, 20) for 20 future returns\n",
    "            # print(f'test_factors_tensor: {test_factors_tensor.shape}')\n",
    "            # Revert normalization for test and predicted returns\n",
    "            test_predictions = test_predictions_norm * train_returns_std.item() + train_returns_mean.item()\n",
    "            future_predictions = future_predictions_norm * train_returns_std.item() + train_returns_mean.item()\n",
    "\n",
    "            augmented_test_features = generate_augmented_features_testphase(\n",
    "                test_factors_norm, train_factors_norm, train_factors_mean, train_factors_std,\n",
    "                model, prediction_window, device\n",
    "            )\n",
    "            # ✅ Convert NumPy array to PyTorch tensor\n",
    "            augmented_features_tensor_test = torch.tensor(augmented_test_features, dtype=torch.float32).to(device)\n",
    "\n",
    "            # ✅ Ensure the tensor is 3D by adding a batch dimension at index 0\n",
    "            augmented_features_tensor_test = augmented_features_tensor_test.unsqueeze(0)  # Shape becomes (1, 20, 12)\n",
    "\n",
    "            augmented_predict_features = generate_augmented_features_testphase(\n",
    "                predict_factors_norm, train_factors_norm, train_factors_mean, train_factors_std,\n",
    "                model, prediction_window, device\n",
    "            )\n",
    "            # ✅ Convert NumPy array to PyTorch tensor\n",
    "            augmented_features_tensor_test = torch.tensor(augmented_test_features, dtype=torch.float32).to(device)\n",
    "            augmented_features_tensor_predict = torch.tensor(augmented_predict_features, dtype=torch.float32).to(device)\n",
    "\n",
    "            # ✅ Ensure the tensor is 3D by adding a batch dimension at index 0\n",
    "            augmented_features_tensor_test = augmented_features_tensor_test.unsqueeze(0)  # Shape becomes (1, 20, 12)\n",
    "            augmented_features_tensor_predict = augmented_features_tensor_predict.unsqueeze(\n",
    "                0)  # Shape becomes (1, 20, 12)\n",
    "\n",
    "            # ✅ Debugging: Print final shape\n",
    "            print(\n",
    "                f'✅ Transformed augmented_features_tensor_test Shape: {augmented_features_tensor_test.shape}')  # Expected: (1, 20, 12)\n",
    "\n",
    "            # Get model predictions\n",
    "            model_binary.eval()\n",
    "            with torch.no_grad():\n",
    "                return_test_prob = model_binary(augmented_features_tensor_test).cpu().numpy().flatten()\n",
    "                return_predict_prob = model_binary(augmented_features_tensor_predict).cpu().numpy().flatten()\n",
    "\n",
    "            print(f'fnished 2nd model test')\n",
    "            # Convert probabilities to binary labels (0 = down, 1 = up)\n",
    "            # Convert probability outputs to binary labels (0 = down, 1 = up)\n",
    "            return_test_binary = (return_test_prob > 0.5).astype(int).item()\n",
    "            return_predict_binary = (return_predict_prob > 0.5).astype(int).item()\n",
    "\n",
    "            # TO-DO: add test phase stats (loss from both models)\n",
    "            # TO-DO: add prediction data from both models, find stocks that go ups and downs with their predicted return series\n",
    "\n",
    "            # future_predictions = future_predictions_norm * train_returns_std.item() + train_returns_mean.item()\n",
    "            actual_test_returns = test_returns.values.flatten()\n",
    "            actual_predict_returns = predict_returns.values.flatten()\n",
    "            actual_test_returns_binary = (np.mean(actual_test_returns) > 0).astype(int)\n",
    "            actual_predict_returns_binary = (np.mean(actual_predict_returns) > 0).astype(int)\n",
    "\n",
    "            # print(f'test_predictions dim:{test_predictions.shape} actual_test_returns dim: {actual_test_returns.shape}')\n",
    "\n",
    "            # Compute loss for test and prediction phases\n",
    "            test_loss = np.mean((test_predictions - actual_test_returns) ** 2)\n",
    "            predict_loss = np.mean((future_predictions - actual_predict_returns) ** 2)\n",
    "\n",
    "            binary_test_loss = np.mean(np.abs(return_test_binary - actual_test_returns_binary))\n",
    "            binary_predict_loss = np.mean(np.abs(return_predict_binary - actual_predict_returns_binary))\n",
    "\n",
    "            all_test_losses.append(test_loss)\n",
    "            print(f'test_loss: {test_loss}')\n",
    "            all_binary_test_losses.append(binary_test_loss)\n",
    "            print(f'binary test_loss: {binary_test_loss}')\n",
    "\n",
    "            # Store test results\n",
    "            combined_test_result = {\n",
    "                \"Stock\": stock,\n",
    "                \"Test Dates\": (test_start, test_end),\n",
    "                \"Predicted Returns (Test Set)\": test_predictions.tolist(),\n",
    "                \"Actual Returns (Test Set)\": actual_test_returns.tolist(),\n",
    "                \"Loss\": test_loss\n",
    "            }\n",
    "\n",
    "            # Store time series results (date-wise)\n",
    "            for i in range(len(test_predictions)):\n",
    "                results.append([\n",
    "                    stock,\n",
    "                    \"test\",\n",
    "                    test_returns.index[i],  # Store the date\n",
    "                    test_predictions[i],\n",
    "                    actual_test_returns[i]\n",
    "                ])\n",
    "\n",
    "            for i in range(len(future_predictions)):\n",
    "                results.append([\n",
    "                    stock,\n",
    "                    \"predict\",\n",
    "                    predict_returns.index[i],  # Store the date\n",
    "                    future_predictions[i],\n",
    "                    actual_predict_returns[i]\n",
    "                ])\n",
    "\n",
    "            # calculate 60 days vol for stock i\n",
    "            stock_historical_returns_test = stock_returns_unshift.iloc[(train_end - 60):train_end]\n",
    "            stock_historical_returns_predict = stock_returns_unshift.iloc[(test_end - 60):test_end]\n",
    "\n",
    "            stock_volatility_test = stock_historical_returns_test.std().values[0]\n",
    "            stock_volatility_predict = stock_historical_returns_predict.std().values[0]\n",
    "\n",
    "            # Store scalar classification results (with start & end dates)\n",
    "            classification_results.append([\n",
    "                stock, \"test_binary\", test_dates[0], test_dates[1], return_test_binary, return_test_prob,\n",
    "                actual_test_returns_binary, stock_volatility_test, float(np.prod(test_returns + 1) - 1)\n",
    "            ])\n",
    "            classification_results.append([\n",
    "                stock, \"predict_binary\", predict_dates[0], predict_dates[1], return_predict_binary, return_predict_prob,\n",
    "                actual_predict_returns_binary, stock_volatility_predict, float(np.prod(predict_returns + 1) - 1)\n",
    "            ])\n",
    "\n",
    "            print(f\"Loss for {stock}: Test Loss: {test_loss:.6f}, Predict Loss: {predict_loss:.6f}\")\n",
    "            print(\n",
    "                f\"Binary Model Loss for {stock}: Test Loss: {binary_test_loss:.6f}, Predict Loss: {binary_predict_loss:.6f}\")\n",
    "\n",
    "            # Convert results to DataFrame\n",
    "            # Add SHAP feature importance metrics to the test results\n",
    "            # combined_test_result.update(feature_importance_df_pivot.iloc[0].to_dict())\n",
    "\n",
    "            # Append the combined results to the test_results list\n",
    "            # test_results.append(combined_test_result)\n",
    "            #\n",
    "            # for date, predicted, actual in zip(predict_returns.index, future_predictions, actual_predict_returns):\n",
    "            #     prediction_results.append({\n",
    "            #         \"Stock\": stock,\n",
    "            #         \"Date\": date,\n",
    "            #         \"Predicted Return\": predicted,\n",
    "            #         \"Actual Return\": actual\n",
    "            #     })\n",
    "            # print(f\"Predictions for {stock} from {predict_start} to {predict_end} complete.\")\n",
    "            # Print average test loss for this stock\n",
    "            avg_test_loss = np.mean(all_test_losses)\n",
    "            avg_binary_test_loss = np.mean(all_binary_test_losses)\n",
    "            print(\n",
    "                f\"Average Test Loss for {stock}: {avg_test_loss:.6f} and Average Binary Test Loss is {avg_binary_test_loss:.6f}\")\n",
    "\n",
    "        # Convert classification_results into a DataFrame\n",
    "        classification_df = pd.DataFrame(classification_results, columns=[\n",
    "            \"Stock\", \"Phase\", \"Start Date\", \"End Date\", \"Predicted Binary\", \"Predicted Probability\",\n",
    "            \"Actual Binary\", \"Volatility\", \"Next Period Return\"\n",
    "        ])\n",
    "\n",
    "        # 🔹 Filter for a specific test period (test_dates[1])\n",
    "        selected_test_date = test_dates[1]  # Replace with the target test end date\n",
    "        filtered_df = classification_df[(classification_df[\"Phase\"] == \"predict_binary\") &\n",
    "                                        (classification_df[\"End Date\"] == selected_test_date)]\n",
    "\n",
    "        # 🔹 Extract required inputs\n",
    "        stock_tickers = filtered_df[\"Stock\"].tolist()\n",
    "        predicted_probs = filtered_df[\"Predicted Probability\"].values.astype(float)\n",
    "        rolling_volatility = filtered_df[\"Volatility\"].values.astype(float)\n",
    "\n",
    "        # 🔹 Map stocks to sectors (Assume `sector_dict` contains stock-sector mapping)\n",
    "        sector_dict = {\n",
    "            'BA': 'Industrials',\n",
    "            'AMGN': 'Healthcare',\n",
    "            'DIS': 'Communication Services',\n",
    "            'NKE': 'Consumer Discretionary',\n",
    "            'HON': 'Industrials',\n",
    "            'MMM': 'Industrials',\n",
    "            'CAT': 'Industrials',\n",
    "            'KO': 'Consumer Staples',\n",
    "            'PG': 'Consumer Staples',\n",
    "            'AXP': 'Financials',\n",
    "            'GS': 'Financials',\n",
    "            'JPM': 'Financials',\n",
    "            'MCD': 'Consumer Discretionary',\n",
    "            'HD': 'Consumer Discretionary',\n",
    "            'AAPL': 'Technology',\n",
    "            'CRM': 'Technology',\n",
    "            'CSCO': 'Technology',\n",
    "            'IBM': 'Technology',\n",
    "            'MSFT': 'Technology',\n",
    "            'TRV': 'Financials',\n",
    "            'UNH': 'Healthcare',\n",
    "            'CVX': 'Energy',\n",
    "            'JNJ': 'Healthcare',\n",
    "            'MRK': 'Healthcare',\n",
    "            'AMZN': 'Consumer Discretionary',\n",
    "            'WMT': 'Consumer Staples',\n",
    "            'INTC': 'Technology',\n",
    "            'VZ': 'Communication Services'\n",
    "        }  # Example mapping, replace with real sector data\n",
    "        sectors = [sector_dict.get(stock, \"Unknown\") for stock in stock_tickers]  # Default to \"Unknown\" if missing\n",
    "\n",
    "        # TO-DO: i can use test period data to fine tune confidence_threshold and sector_limit parameter and then trade on prediction period:\n",
    "        # 🔹 Call compute_portfolio_weights function\n",
    "        portfolio_weights = compute_portfolio_weights(\n",
    "            stock_tickers=stock_tickers,\n",
    "            predicted_probs=predicted_probs,\n",
    "            rolling_volatility=rolling_volatility,\n",
    "            sectors=sectors,\n",
    "            confidence_threshold=(0.4, 0.6),  # Exclude low-confidence predictions\n",
    "            sector_limit=0.3,  # Max 30% per sector\n",
    "            annualize_volatility=False  # Convert daily volatility to annualized if needed\n",
    "        )\n",
    "\n",
    "        # 🔹 Display final portfolio allocation\n",
    "        print(\"\\n🔹 Final Portfolio Weights for Test Date:\", selected_test_date)\n",
    "        print(portfolio_weights)\n",
    "\n",
    "        # 🔹 Merge all inputs with portfolio weights\n",
    "        for i in range(len(stock_tickers)):\n",
    "            portfolio_records.append([\n",
    "                selected_test_date,  # Store time period\n",
    "                stock_tickers[i],\n",
    "                sectors[i],\n",
    "                predicted_probs[i],\n",
    "                rolling_volatility[i],\n",
    "                0.4,  # Confidence threshold lower bound\n",
    "                0.6,  # Confidence threshold upper bound\n",
    "                0.3,  # Sector constraint limit\n",
    "                False,  # Annualized volatility flag\n",
    "                portfolio_weights.loc[portfolio_weights[\"Stock\"] == stock_tickers[i], \"Normalized Weight\"].values[0]\n",
    "                # Extract weight\n",
    "            ])\n",
    "\n",
    "        start += prediction_window\n",
    "\n",
    "    portfolio_results_df = pd.DataFrame(portfolio_records, columns=[\n",
    "        \"Time Period\", \"Stock\", \"Sector\", \"Predicted Probability\", \"Rolling Volatility\",\n",
    "        \"Confidence Threshold (Low)\", \"Confidence Threshold (High)\", \"Sector Limit\",\n",
    "        \"Annualized Volatility\", \"Final Portfolio Weight\"\n",
    "    ])\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    portfolio_results_df.to_csv(\"portfolio_results.csv\", index=False)\n",
    "    # Convert to DataFrame\n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results, columns=[\"Stock\", \"Phase\", \"Date\", \"Predicted Return\", \"Actual Return\"])\n",
    "    df_classification = pd.DataFrame(classification_results,\n",
    "                                     columns=[\"Stock\", \"Phase\", \"Start Date\", \"End Date\", \"Predicted Binary\",\n",
    "                                              \"Predict Probability\", \"Actual Binary\", \"Stock Volatility\",\n",
    "                                              \"Next Period Return\"])\n",
    "\n",
    "    # Save results to CSV\n",
    "    df_results.to_csv(\"stock_predictions_results.csv\", index=False)\n",
    "    df_classification.to_csv(\"stock_predictions_classification.csv\", index=False)\n",
    "\n",
    "    print(\"Results saved successfully!\")\n",
    "\n",
    "elif state == 'train_RL':\n",
    "\n",
    "    # Load sample stock data (Replace with actual data loading)\n",
    "    stock_data = pd.read_csv(\"sample_stock_data.csv\")  # Assume CSV file with test data\n",
    "\n",
    "     # Train the RL agent\n",
    "    trained_model = train_rl_agent(stock_data, max_periods=213, num_episodes=5000, N=100)\n",
    "    \n",
    "    # Get averaged optimal parameters\n",
    "    M = 100  # Number of samples to generate\n",
    "    optimal_params = get_average_optimal_parameters(trained_model, stock_data, num_periods=10, M=M)\n",
    "    \n",
    "    print(\"Averaged Optimized Confidence Threshold:\", optimal_params[0])\n",
    "    print(\"Averaged Optimized Sector Limit:\", optimal_params[1])\n",
    "    \n",
    "    # Save averaged optimal parameters to CSV\n",
    "    optimal_params_df = pd.DataFrame({\n",
    "        'Confidence Threshold Low': [optimal_params[0][0]],\n",
    "        'Confidence Threshold High': [optimal_params[0][1]],\n",
    "        'Sector Limit': [optimal_params[1]]\n",
    "    })\n",
    "    optimal_params_df.to_csv(\"optimal_parameters.csv\", index=False)\n",
    "    \n",
    "    print(\"Optimal parameters saved to optimal_parameters.csv\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb9badb644b77bf7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_dir = \"E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\"  # Use the specified directory\n",
    "ticker_file = os.path.join(input_dir, \"tickers.txt\")  # Text file containing tickers, one per line\n",
    "factor_file = os.path.join(input_dir, \"FF_factors.csv\")  # CSV file containing factor returns\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "output_csv = os.path.join(input_dir, \"stock_prices.csv\")\n",
    "\n",
    "try:\n",
    "    tickers = load_tickers_from_file(ticker_file)\n",
    "    price_data = download_stock_prices(tickers, start_date, end_date, output_csv)\n",
    "    daily_returns = calculate_daily_returns(price_data, return_type='arithmetic')\n",
    "    daily_returns.to_csv(os.path.join(input_dir, \"daily_returns.csv\"))\n",
    "    factor_returns = read_factor_returns(factor_file, daily_returns)\n",
    "    factor_returns.to_csv(os.path.join(input_dir, \"aligned_factors.csv\"))\n",
    "    print(\"Factor returns alignment completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "factors = pd.read_csv(f\"{input_dir}/aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns_raw = pd.read_csv(f\"{input_dir}/daily_returns.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors = factors.dropna()\n",
    "returns = returns_raw.dropna()\n",
    "\n",
    "# Align dates between factors and returns\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors_raw = factors.loc[dates]\n",
    "returns_raw = returns.loc[dates]\n",
    "factors_raw = factors_raw.iloc[:, :-1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18b7a818ea52546f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "factors_raw"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b343a4d4e147471",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5d813989a28f24a0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "43161fbf12b9ba3d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2009-07-16    0.007590\n",
      "2009-07-17    0.000810\n",
      "2009-07-20    0.008193\n",
      "2009-07-21   -0.001259\n",
      "2009-07-22    0.003083\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "weights_file = \"port_wt_test.csv\"  # Optimal security weights\n",
    "returns_file = \"port_ret_test.csv\"  # Daily stock returns\n",
    "\n",
    "# Read the files\n",
    "weights_df = pd.read_csv(weights_file)\n",
    "returns_df = pd.read_csv(returns_file)\n",
    "\n",
    "# Convert Date columns to datetime format for proper alignment\n",
    "weights_df[\"Date\"] = pd.to_datetime(weights_df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "returns_df[\"Date\"] = pd.to_datetime(returns_df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "# Set Date as index for easier merging\n",
    "weights_df.set_index(\"Date\", inplace=True)\n",
    "returns_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Sort data to ensure chronological order\n",
    "weights_df.sort_index(inplace=True)\n",
    "returns_df.sort_index(inplace=True)\n",
    "\n",
    "# Initialize drifted weights dataframe with the same structure as returns_df\n",
    "aligned_weights_df = weights_df.reindex(returns_df.index, method='ffill')\n",
    "drifted_weights_df = aligned_weights_df.copy()\n",
    "\n",
    "# Iterate through each day to apply drifted weight calculation\n",
    "for i in range(1, len(drifted_weights_df)):\n",
    "    # If the day is a rebalancing day, keep the given weights\n",
    "    if drifted_weights_df.index[i] in weights_df.index:\n",
    "        continue\n",
    "\n",
    "    # Otherwise, update weights based on previous day's weights and returns\n",
    "    drifted_weights_df.iloc[i] = drifted_weights_df.iloc[i - 1] * (1 + returns_df.iloc[i - 1])\n",
    "\n",
    "    # Normalize weights to ensure they sum to 100%\n",
    "    drifted_weights_df.iloc[i] /= drifted_weights_df.iloc[i].sum()\n",
    "\n",
    "# Compute portfolio returns using drifted weights\n",
    "portfolio_returns_drifted = (returns_df * drifted_weights_df).sum(axis=1)\n",
    "\n",
    "# Save portfolio returns to CSV\n",
    "portfolio_returns_drifted.to_csv(\"portfolio_returns_drifted.csv\")\n",
    "# Display first few rows\n",
    "print(portfolio_returns_drifted.head())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-21T00:37:23.541954Z",
     "start_time": "2025-03-21T00:37:17.294961Z"
    }
   },
   "id": "3a663bb6cc0bbcd4",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "776f80058cf7a0c0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "drifted_weights_df.to_csv(\"portfolio_returns_driftedweights.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba9efdcd6db7dfc8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\\tickers_DIA.txt\n",
      "['AAPL', 'DIA', 'IYR']\n",
      "Downloading data for AAPL...\n",
      "Downloading data for DIA...\n",
      "Downloading data for IYR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\\stock_prices_DIA.csv\n",
      "Factor returns alignment completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper\n",
    "input_dir = \"E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\"  # Use the specified directory\n",
    "\n",
    "start_date = \"2009-07-16\"\n",
    "end_date = \"2024-12-31\"\n",
    "output_csv = os.path.join(input_dir, \"stock_prices_DIA.csv\")\n",
    "\n",
    "ticker_file = os.path.join(input_dir, \"tickers_DIA.txt\")  # Text file containing tickers, one per line\n",
    "print(ticker_file)\n",
    "tickers = load_tickers_from_file(ticker_file)\n",
    "print(tickers)\n",
    "price_data = download_stock_prices(tickers, start_date, end_date, output_csv)\n",
    "\n",
    "print(\"Factor returns alignment completed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-21T00:44:41.099592Z",
     "start_time": "2025-03-21T00:44:40.672454Z"
    }
   },
   "id": "a52c144339858f09",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def compute_portfolio_weights(stock_tickers, predicted_probs, rolling_volatility, sectors,\n",
    "                              confidence_threshold=(0.4, 0.6), short_ratio=0.25,\n",
    "                              annualize_volatility=False):\n",
    "    \"\"\"\n",
    "    Compute optimal portfolio weights for long-short portfolio ensuring total weight sums to 100%\n",
    "    while maintaining the short ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - stock_tickers (list): List of stock tickers.\n",
    "    - predicted_probs (np.array): Predicted probability of stock moving up.\n",
    "    - rolling_volatility (np.array): Rolling volatility (daily or annualized).\n",
    "    - sectors (list): List of sector classifications for each stock.\n",
    "    - confidence_threshold (tuple): Threshold (low, high) to filter out uncertain predictions.\n",
    "    - short_ratio (float): Proportion of short exposure (between 0 and 50%).\n",
    "    - annualize_volatility (bool): Whether to annualize daily volatility using sqrt(252).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Portfolio allocation with normalized long-short weights.\n",
    "    \"\"\"\n",
    "    # Convert probability into raw weights for long-short portfolio\n",
    "    raw_weights = 2 * (predicted_probs - 0.5)\n",
    "\n",
    "    # Apply confidence threshold: Only include confident stocks\n",
    "    confidence_mask = (predicted_probs >= confidence_threshold[1]) | (predicted_probs <= confidence_threshold[0])\n",
    "    # print(f'confidence_mask = {confidence_mask}')\n",
    "    filtered_weights = raw_weights * confidence_mask  # Zero out low-confidence predictions\n",
    "    # print(f'filtered_weights = {filtered_weights}')\n",
    "\n",
    "    # Check if no stock qualifies\n",
    "    if np.all(filtered_weights == 0):\n",
    "        raise ValueError(\"No stocks passed the confidence threshold; cannot generate portfolio.\")\n",
    "\n",
    "    # Annualize volatility if required\n",
    "    if annualize_volatility:\n",
    "        rolling_volatility = rolling_volatility * np.sqrt(252)\n",
    "\n",
    "    # Apply Volatility Adjustment: Wi,Tadjusted = Wi,T / σi\n",
    "    adjusted_weights = filtered_weights / rolling_volatility  # Scale by inverse volatility\n",
    "\n",
    "    # Separate long and short portfolios based on short ratio\n",
    "    long_weights = np.where(adjusted_weights > 0, adjusted_weights, 0)\n",
    "    short_weights = np.where(adjusted_weights < 0, adjusted_weights, 0)\n",
    "\n",
    "    # Normalize long and short weights so they sum to the target allocations\n",
    "    long_sum = np.sum(long_weights)\n",
    "    short_sum = np.abs(np.sum(short_weights))\n",
    "\n",
    "    if long_sum > 0:\n",
    "        long_weights *= (1 + short_ratio) / long_sum  # Scale long portfolio to (1 + short_ratio)\n",
    "    if short_sum > 0:\n",
    "        short_weights *= short_ratio / short_sum  # Scale short portfolio to -short_ratio\n",
    "\n",
    "    # Compute final portfolio weights (long + short should sum to 100%)\n",
    "    final_weights = long_weights + short_weights\n",
    "    # final_weights /= np.sum(np.abs(final_weights))  # Normalize to ensure exact sum of 100%\n",
    "\n",
    "    # Create portfolio DataFrame\n",
    "    portfolio_df = pd.DataFrame({\n",
    "        \"Stock\": stock_tickers,\n",
    "        \"Sector\": sectors,\n",
    "        \"Probability\": predicted_probs,\n",
    "        \"Raw Weight\": raw_weights,\n",
    "        \"Volatility\": rolling_volatility,\n",
    "        \"Adjusted Weight\": adjusted_weights,\n",
    "        \"Final Weight\": final_weights\n",
    "    })\n",
    "    # print(f'portfolio_wt = {np.sum(portfolio_df[\"Final Weight\"])}')\n",
    "    # print(f'portfolio_short_ratio = {np.abs(np.sum(portfolio_df[portfolio_df[\"Final Weight\"] < 0][\"Final Weight\"])) }')\n",
    "    # print(f'total_portfolio_weight = {np.sum(portfolio_df[\"Final Weight\"]) }')\n",
    "\n",
    "    # Validate constraints\n",
    "    total_portfolio_weight = np.sum(portfolio_df[\"Final Weight\"])\n",
    "    assert np.isclose(total_portfolio_weight, 1.0,\n",
    "                      atol=1e-6), f\"Error: Total Portfolio Weight is {total_portfolio_weight}, should be 1.0.\"\n",
    "\n",
    "    actual_short_ratio = np.abs(np.sum(portfolio_df[portfolio_df[\"Final Weight\"] < 0][\"Final Weight\"])) / total_portfolio_weight\n",
    "    assert np.isclose(actual_short_ratio, short_ratio,\n",
    "                      atol=1e-6), f\"Error: Actual Short Ratio is {actual_short_ratio}, expected {short_ratio}.\"\n",
    "    # print(f'final weight : {portfolio_df[[\"Final Weight\"]]}')\n",
    "    return portfolio_df[[\"Stock\", \"Sector\", \"Probability\", \"Final Weight\"]]\n",
    "\n",
    "\n",
    "# Generate an example where no stock passes the confidence threshold filter\n",
    "# Generate synthetic stock tickers\n",
    "stock_tickers = [f\"Stock_{i}\" for i in range(10)]\n",
    "\n",
    "# Generate random predicted probabilities (between 0 and 1)\n",
    "predicted_probs = np.random.uniform(0.45, 0.55, 10)  # Values centered within 0.45-0.55 range\n",
    "\n",
    "# Generate random rolling volatility (between 0.1 and 0.5)\n",
    "rolling_volatility = np.random.uniform(0.1, 0.5, 10)\n",
    "\n",
    "# Assign random sectors from a predefined list\n",
    "sectors = np.random.choice([\"Tech\", \"Finance\", \"Healthcare\", \"Energy\"], size=10)\n",
    "\n",
    "# Use a very wide confidence threshold to exclude all stocks\n",
    "wide_confidence_threshold = (0.5, 0.5)  # No stock will qualify as long or short\n",
    "\n",
    "# Run the function\n",
    "try:\n",
    "    portfolio_df = compute_portfolio_weights(\n",
    "        stock_tickers=stock_tickers,\n",
    "        predicted_probs=predicted_probs,\n",
    "        rolling_volatility=rolling_volatility,\n",
    "        sectors=sectors,\n",
    "        confidence_threshold=wide_confidence_threshold,  # No stock qualifies\n",
    "        short_ratio=0,\n",
    "        annualize_volatility=False\n",
    "    )\n",
    "    print(\"Portfolio successfully generated.\")\n",
    "    print(portfolio_df)\n",
    "except ValueError as e:\n",
    "    print(f\"Portfolio generation failed: {e}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "600933410d26626",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5008760d6fdabfb8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c08dbd731cf00b67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper\n",
    "\n",
    "# Example usage\n",
    "input_dir = \"E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\"  # Use the specified directory\n",
    "ticker_file = os.path.join(input_dir, \"tickers.txt\")  # Text file containing tickers, one per line\n",
    "factor_file = os.path.join(input_dir, \"FF_factors.csv\")  # CSV file containing factor returns\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "output_csv = os.path.join(input_dir, \"stock_prices.csv\")\n",
    "\n",
    "try:\n",
    "    tickers = load_tickers_from_file(ticker_file)\n",
    "    price_data = download_stock_prices(tickers, start_date, end_date, output_csv)\n",
    "    daily_returns = calculate_daily_returns(price_data, return_type='arithmetic')\n",
    "    daily_returns.to_csv(os.path.join(input_dir, \"daily_returns.csv\"))\n",
    "    factor_returns = read_factor_returns(factor_file, daily_returns)\n",
    "    factor_returns.to_csv(os.path.join(input_dir, \"aligned_factors.csv\"))\n",
    "    print(\"Factor returns alignment completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "factors = pd.read_csv(f\"{input_dir}/aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns_raw = pd.read_csv(f\"{input_dir}/daily_returns.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors = factors.dropna()\n",
    "returns = returns_raw.dropna()\n",
    "\n",
    "# Align dates between factors and returns\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors_raw = factors.loc[dates]\n",
    "returns_raw = returns.loc[dates]\n",
    "factors_raw = factors_raw.iloc[:, :-1]\n",
    "# print(f\"factors_raw = {factors_raw}\")\n",
    "\n",
    "Nday_diff = 10  # Define the future prediction horizon for shift days\n",
    "# limit to 2 stock ONLY\n",
    "# returns_raw = returns_raw.iloc[:, :2]\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors_raw = factors_raw.dropna()\n",
    "returns_raw = returns_raw.dropna()\n",
    "# factors_raw = pd.concat([factors_raw, returns_raw], axis=1)\n",
    "# Align dates between factors_raw and returns_raw\n",
    "dates = factors_raw.index.intersection(returns_raw.index)\n",
    "factors_raw = factors_raw.loc[dates]\n",
    "returns_raw = returns_raw.loc[dates]\n",
    "\n",
    "# Shift returns to generate target variable (N-day future return)\n",
    "returns_raw_Y = returns_raw.shift(-Nday_diff).dropna()\n",
    "\n",
    "# Align all datasets with available dates in returns_raw_Y\n",
    "dates_final = factors_raw.index.intersection(returns_raw_Y.index)\n",
    "factors_raw = factors_raw.loc[dates_final]\n",
    "returns_raw = returns_raw.loc[dates_final]\n",
    "returns_raw_Y = returns_raw_Y.loc[dates_final]\n",
    "\n",
    "N = 10\n",
    "sequence_length = 10\n",
    "batch_size = 64\n",
    "num_epochs = 500\n",
    "learning_rate = 0.001\n",
    "rolling_window_train = 360  # Reduce for faster adaptation\n",
    "rolling_window_test = 10    # Reduce test window\n",
    "prediction_window = 10      # Align prediction period with test window\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Record results\n",
    "results = []\n",
    "prediction_records = []\n",
    "shapley_records = []\n",
    "shapley_metrics_df = []\n",
    "all_test_losses = []\n",
    "all_binary_test_losses = []\n",
    "results = []\n",
    "classification_results = []\n",
    "\n",
    "test_results = []\n",
    "prediction_results = []\n",
    "combined_test_result = []\n",
    "portfolio_records = []\n",
    "start = 0  # Initialize rolling split index\n",
    "\n",
    "# state = 'train_data_generate'\n",
    "state = 'train_data_generate'\n",
    "# state = 'train_RL'\n",
    "\n",
    "\n",
    "if state == 'train_data_generate':\n",
    "    # Rolling split: Train (M days), Test (N days), Predict (W days)\n",
    "    total_samples = len(factors_raw)\n",
    "    while start + rolling_window_train + rolling_window_test + prediction_window <= total_samples:\n",
    "\n",
    "        train_start = start\n",
    "        train_end = start + rolling_window_train\n",
    "\n",
    "        test_start = train_end\n",
    "        test_end = test_start + rolling_window_test\n",
    "\n",
    "        predict_start = test_end  # Predictions start right after testing\n",
    "        predict_end = predict_start + prediction_window\n",
    "\n",
    "        # Extract the actual date indices\n",
    "        train_dates = (factors_raw.iloc[train_start].name, factors_raw.iloc[train_end - 1].name)\n",
    "        test_dates = (factors_raw.iloc[test_start].name, factors_raw.iloc[test_end - 1].name)\n",
    "        predict_dates = (factors_raw.iloc[predict_start].name, factors_raw.iloc[predict_end - 1].name)\n",
    "\n",
    "        print(f\"\\n🔹 Processing Date Window: Train {train_dates}, Test {test_dates}, Predict {predict_dates}\")\n",
    "\n",
    "        for stock in returns_raw.columns:\n",
    "            print(f\"Training for stock: {stock}\")\n",
    "            stock_returns = returns_raw_Y[[stock]]  # Select the stock column\n",
    "            stock_returns_unshift = returns_raw[[stock]]\n",
    "            factors_raw_X = pd.concat([factors_raw, stock_returns_unshift], axis=1)\n",
    "            # Rolling split: Train (M days), Test (N days), Predict (W days)\n",
    "            # total_samples = len(factors_raw_X)\n",
    "            # Initialize the rolling split index\n",
    "\n",
    "            # Print the selected date ranges\n",
    "            print(f\"Training dates: {train_dates}\")\n",
    "            print(f\"Testing dates: {test_dates}\")\n",
    "            print(f\"Prediction dates: {predict_dates}\")\n",
    "\n",
    "            # # Split data into training, testing, and prediction sets\n",
    "            train_factors = factors_raw_X.iloc[train_start:train_end]\n",
    "            train_returns = stock_returns.iloc[train_start:train_end]\n",
    "            test_factors = factors_raw_X.iloc[test_start:test_end]\n",
    "            test_returns = stock_returns.iloc[test_start:test_end]\n",
    "            predict_factors = factors_raw_X.iloc[predict_start:predict_end]\n",
    "            predict_returns = stock_returns.iloc[predict_start:predict_end]\n",
    "            #\n",
    "            # # Normalize train and test data\n",
    "            train_factors_mean, train_factors_std = train_factors.mean(), train_factors.std()\n",
    "            train_returns_mean, train_returns_std = train_returns.mean(), train_returns.std()\n",
    "\n",
    "            # Avoid division by zero\n",
    "            train_factors_std.replace(0, 1e-8, inplace=True)\n",
    "            train_returns_std.replace(0, 1e-8, inplace=True)\n",
    "\n",
    "            train_factors_norm = (train_factors - train_factors_mean) / train_factors_std\n",
    "            train_returns_norm = (train_returns - train_returns_mean) / train_returns_std\n",
    "            test_factors_norm = (test_factors - train_factors_mean) / train_factors_std\n",
    "            test_returns_norm = (test_returns - train_returns_mean) / train_returns_std\n",
    "            predict_factors_norm = (predict_factors - train_factors_mean) / train_factors_std\n",
    "            predict_returns_norm = (predict_returns - train_returns_mean) / train_returns_std\n",
    "            # print(f'train_factors_norm dim: {train_factors_norm.shape}')\n",
    "            # print(f'train_returns_norm dim: {train_returns_norm.shape}')\n",
    "            dataset = StockDataset(train_factors_norm, train_returns_norm, sequence_length, N=N)\n",
    "\n",
    "            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            # for batch_x, batch_y in dataloader:\n",
    "            #     print(f\"Batch X Shape: {batch_x.shape}\")  # Expected: (batch_size, sequence_length, num_features)\n",
    "            #     print(f\"Batch Y Shape: {batch_y.shape}\")  # Expected: (batch_size, N, num_features)\n",
    "            #     break  # Only print for the first batch\n",
    "            # # Model initialization\n",
    "            d_model = train_factors.shape[1]\n",
    "            # print(f'd_model: {d_model}')\n",
    "\n",
    "            num_heads = min(4,\n",
    "                            d_model)  # Ensure num_heads does not exceed d_model and d_model is divisible by num_heads\n",
    "            while d_model % num_heads != 0:\n",
    "                num_heads -= 1\n",
    "            # print(f\"d_model: {d_model} num_heads: {num_heads} sequence_length: {sequence_length} and train factor return dim: {train_factors.shape[1]} {train_returns.shape[1]}\")\n",
    "            model = StockPredictAgent(d_model=d_model, num_heads=num_heads, sequence_length=sequence_length, N=N).to(\n",
    "                device)\n",
    "            #\n",
    "            # # Optimizer and loss function\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "            criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss for stability\n",
    "            #\n",
    "            # # Training the model\n",
    "            # print(f\"Training window: {train_start} to {train_end}\")\n",
    "            # print(f\"Expected FC Layer Input Shape: {sequence_length * d_model}\")\n",
    "            # print(f\"Flattened Tensor Shape: {batch_size, sequence_length * d_model}\")\n",
    "\n",
    "            train_model_with_logging(model, dataloader, optimizer, criterion, scheduler, num_epochs, device)\n",
    "\n",
    "            print(f'Finished training model 1')\n",
    "            # print(f'train_factors_norm dim: {train_factors_norm.shape}')\n",
    "            # Convert to PyTorch tensors\n",
    "            # print(f'test_factors_tensor: {test_factors_tensor.shape}')\n",
    "            # Run inference on test data\n",
    "            augmented_train_features = generate_augmented_features(\n",
    "                train_factors_norm, train_factors_mean, train_factors_std, model, rolling_window_test, device\n",
    "            )\n",
    "            augmented_features_tensor = torch.tensor(augmented_train_features, dtype=torch.float32).to(device)\n",
    "\n",
    "            print(\n",
    "                f\"Final Augmented Tensor Shape: {augmented_train_features.shape}\")  # Expected: (rolling_window_test, d_model * 2)\n",
    "\n",
    "            # Convert test_returns_norm into binary classification\n",
    "            mean_return_binary = np.mean(train_returns_norm)  # Aggregate 60-day return\n",
    "            train_returns_threshold = (0 - train_returns_mean) / train_returns_std  # Convert zero into normalized scale\n",
    "            train_returns_binary = (mean_return_binary > train_returns_threshold).astype(\n",
    "                int)  # 1 if positive, 0 if negative\n",
    "            # print(f'train_returns_norm = {train_returns_norm}')\n",
    "            print(f\"Cumulative Return over 60 days: {mean_return_binary}\")\n",
    "            print(f\"Binary Target (Up=1, Down=0): {train_returns_binary}\")\n",
    "\n",
    "            # Train the Model Using Augmented Features\n",
    "            # sequence_length = 20\n",
    "            # batch_size = 32\n",
    "            # num_epochs = 25\n",
    "            # learning_rate = 0.001\n",
    "            d_model2 = augmented_train_features.shape[1]  # Number of features (6 original + 6 SHAP)\n",
    "            num_heads = min(4, d_model2)\n",
    "            while d_model2 % num_heads != 0:\n",
    "                num_heads -= 1\n",
    "\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            print(f\"Length of augmented_test_features: {augmented_train_features.shape}\")\n",
    "            print(f\"Length of sequence: {sequence_length}\")\n",
    "\n",
    "            # sequence_length2 = min(10, len(augmented_test_features))  # Ensure it's smaller than available data\n",
    "            sequence_length2 = prediction_window\n",
    "            # Create dataset and dataloader for training\n",
    "            dataset2 = StockDatasetBinary(augmented_train_features, train_returns_norm, sequence_length2)\n",
    "            dataloader2 = DataLoader(dataset2, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            for batch_x, batch_y in dataloader2:\n",
    "                print(f\"Batch X Shape: {batch_x.shape}\")  # Expected: (batch_size, sequence_length, num_features)\n",
    "                print(f\"Batch Y Shape: {batch_y.shape}\")  # Expected: (batch_size, N, num_features)\n",
    "                break  # Only print for the first batch\n",
    "\n",
    "            # Initialize the binary classification model\n",
    "            model_binary = StockPredictAgentBinary(d_model=d_model2, num_heads=num_heads,\n",
    "                                                   sequence_length=sequence_length2).to(device)\n",
    "\n",
    "            # Define optimizer and loss function\n",
    "            optimizer = optim.Adam(model_binary.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "            criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "\n",
    "            # Train the model\n",
    "            train_model(model_binary, dataloader2, optimizer, criterion, num_epochs, device)\n",
    "            print(f'Finished training model 2')\n",
    "\n",
    "            test_factors_tensor = torch.tensor(test_factors_norm.values, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "            predict_factors_tensor = torch.tensor(predict_factors_norm.values, dtype=torch.float32).to(\n",
    "                device).unsqueeze(0)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_predictions_norm = model(test_factors_tensor).cpu().numpy().flatten()\n",
    "                future_predictions_norm = model(predict_factors_tensor).cpu().numpy().flatten()\n",
    "\n",
    "                # test_output = model(torch.tensor(test_factors_np, dtype=torch.float32).to(device))\n",
    "                # print(f\"Model output shape: {test_output.shape}\")  # Expected: (1, 20) for 20 future returns\n",
    "            # print(f'test_factors_tensor: {test_factors_tensor.shape}')\n",
    "            # Revert normalization for test and predicted returns\n",
    "            test_predictions = test_predictions_norm * train_returns_std.item() + train_returns_mean.item()\n",
    "            future_predictions = future_predictions_norm * train_returns_std.item() + train_returns_mean.item()\n",
    "\n",
    "            augmented_test_features = generate_augmented_features_testphase(\n",
    "                test_factors_norm, train_factors_norm, train_factors_mean, train_factors_std,\n",
    "                model, prediction_window, device\n",
    "            )\n",
    "            # ✅ Convert NumPy array to PyTorch tensor\n",
    "            augmented_features_tensor_test = torch.tensor(augmented_test_features, dtype=torch.float32).to(device)\n",
    "\n",
    "            # ✅ Ensure the tensor is 3D by adding a batch dimension at index 0\n",
    "            augmented_features_tensor_test = augmented_features_tensor_test.unsqueeze(0)  # Shape becomes (1, 20, 12)\n",
    "\n",
    "            augmented_predict_features = generate_augmented_features_testphase(\n",
    "                predict_factors_norm, train_factors_norm, train_factors_mean, train_factors_std,\n",
    "                model, prediction_window, device\n",
    "            )\n",
    "            # ✅ Convert NumPy array to PyTorch tensor\n",
    "            augmented_features_tensor_test = torch.tensor(augmented_test_features, dtype=torch.float32).to(device)\n",
    "            augmented_features_tensor_predict = torch.tensor(augmented_predict_features, dtype=torch.float32).to(device)\n",
    "\n",
    "            # ✅ Ensure the tensor is 3D by adding a batch dimension at index 0\n",
    "            augmented_features_tensor_test = augmented_features_tensor_test.unsqueeze(0)  # Shape becomes (1, 20, 12)\n",
    "            augmented_features_tensor_predict = augmented_features_tensor_predict.unsqueeze(\n",
    "                0)  # Shape becomes (1, 20, 12)\n",
    "\n",
    "            # ✅ Debugging: Print final shape\n",
    "            print(\n",
    "                f'✅ Transformed augmented_features_tensor_test Shape: {augmented_features_tensor_test.shape}')  # Expected: (1, 20, 12)\n",
    "\n",
    "            # Get model predictions\n",
    "            model_binary.eval()\n",
    "            with torch.no_grad():\n",
    "                return_test_prob = model_binary(augmented_features_tensor_test).cpu().numpy().flatten()\n",
    "                return_predict_prob = model_binary(augmented_features_tensor_predict).cpu().numpy().flatten()\n",
    "\n",
    "            print(f'fnished 2nd model test')\n",
    "            # Convert probabilities to binary labels (0 = down, 1 = up)\n",
    "            # Convert probability outputs to binary labels (0 = down, 1 = up)\n",
    "            return_test_binary = (return_test_prob > 0.5).astype(int).item()\n",
    "            return_predict_binary = (return_predict_prob > 0.5).astype(int).item()\n",
    "\n",
    "            # TO-DO: add test phase stats (loss from both models)\n",
    "            # TO-DO: add prediction data from both models, find stocks that go ups and downs with their predicted return series\n",
    "\n",
    "            # future_predictions = future_predictions_norm * train_returns_std.item() + train_returns_mean.item()\n",
    "            actual_test_returns = test_returns.values.flatten()\n",
    "            actual_predict_returns = predict_returns.values.flatten()\n",
    "            actual_test_returns_binary = (np.mean(actual_test_returns) > 0).astype(int)\n",
    "            actual_predict_returns_binary = (np.mean(actual_predict_returns) > 0).astype(int)\n",
    "\n",
    "            # print(f'test_predictions dim:{test_predictions.shape} actual_test_returns dim: {actual_test_returns.shape}')\n",
    "\n",
    "            # Compute loss for test and prediction phases\n",
    "            test_loss = np.mean((test_predictions - actual_test_returns) ** 2)\n",
    "            predict_loss = np.mean((future_predictions - actual_predict_returns) ** 2)\n",
    "\n",
    "            binary_test_loss = np.mean(np.abs(return_test_binary - actual_test_returns_binary))\n",
    "            binary_predict_loss = np.mean(np.abs(return_predict_binary - actual_predict_returns_binary))\n",
    "\n",
    "            all_test_losses.append(test_loss)\n",
    "            print(f'test_loss: {test_loss}')\n",
    "            all_binary_test_losses.append(binary_test_loss)\n",
    "            print(f'binary test_loss: {binary_test_loss}')\n",
    "\n",
    "            # Store test results\n",
    "            combined_test_result = {\n",
    "                \"Stock\": stock,\n",
    "                \"Test Dates\": (test_start, test_end),\n",
    "                \"Predicted Returns (Test Set)\": test_predictions.tolist(),\n",
    "                \"Actual Returns (Test Set)\": actual_test_returns.tolist(),\n",
    "                \"Loss\": test_loss\n",
    "            }\n",
    "\n",
    "            # Store time series results (date-wise)\n",
    "            for i in range(len(test_predictions)):\n",
    "                results.append([\n",
    "                    stock,\n",
    "                    \"test\",\n",
    "                    test_returns.index[i],  # Store the date\n",
    "                    test_predictions[i],\n",
    "                    actual_test_returns[i]\n",
    "                ])\n",
    "\n",
    "            for i in range(len(future_predictions)):\n",
    "                results.append([\n",
    "                    stock,\n",
    "                    \"predict\",\n",
    "                    predict_returns.index[i],  # Store the date\n",
    "                    future_predictions[i],\n",
    "                    actual_predict_returns[i]\n",
    "                ])\n",
    "\n",
    "            # calculate 60 days vol for stock i\n",
    "            stock_historical_returns_test = stock_returns_unshift.iloc[(train_end - 60):train_end]\n",
    "            stock_historical_returns_predict = stock_returns_unshift.iloc[(test_end - 60):test_end]\n",
    "\n",
    "            stock_volatility_test = stock_historical_returns_test.std().values[0]\n",
    "            stock_volatility_predict = stock_historical_returns_predict.std().values[0]\n",
    "\n",
    "            # Store scalar classification results (with start & end dates)\n",
    "            classification_results.append([\n",
    "                stock, \"test_binary\", test_dates[0], test_dates[1], return_test_binary, return_test_prob,\n",
    "                actual_test_returns_binary, stock_volatility_test, float(np.prod(test_returns + 1) - 1)\n",
    "\n",
    "            ])\n",
    "            classification_results.append([\n",
    "                stock, \"predict_binary\", predict_dates[0], predict_dates[1], return_predict_binary, return_predict_prob,\n",
    "                actual_predict_returns_binary, stock_volatility_predict, float(np.prod(predict_returns + 1) - 1)\n",
    "            ])\n",
    "            print(f'return_predict_prob = {return_predict_prob}')\n",
    "            print(f\"Loss for {stock}: Test Loss: {test_loss:.6f}, Predict Loss: {predict_loss:.6f}\")\n",
    "            print(\n",
    "                f\"Binary Model Loss for {stock}: Test Loss: {binary_test_loss:.6f}, Predict Loss: {binary_predict_loss:.6f}\")\n",
    "\n",
    "            # Convert results to DataFrame\n",
    "            # Add SHAP feature importance metrics to the test results\n",
    "            # combined_test_result.update(feature_importance_df_pivot.iloc[0].to_dict())\n",
    "\n",
    "            # Append the combined results to the test_results list\n",
    "            # test_results.append(combined_test_result)\n",
    "            #\n",
    "            # for date, predicted, actual in zip(predict_returns.index, future_predictions, actual_predict_returns):\n",
    "            #     prediction_results.append({\n",
    "            #         \"Stock\": stock,\n",
    "            #         \"Date\": date,\n",
    "            #         \"Predicted Return\": predicted,\n",
    "            #         \"Actual Return\": actual\n",
    "            #     })\n",
    "            # print(f\"Predictions for {stock} from {predict_start} to {predict_end} complete.\")\n",
    "            # Print average test loss for this stock\n",
    "            avg_test_loss = np.mean(all_test_losses)\n",
    "            avg_binary_test_loss = np.mean(all_binary_test_losses)\n",
    "            print(\n",
    "                f\"Average Test Loss for {stock}: {avg_test_loss:.6f} and Average Binary Test Loss is {avg_binary_test_loss:.6f}\")\n",
    "\n",
    "        # Convert classification_results into a DataFrame\n",
    "        classification_df = pd.DataFrame(classification_results, columns=[\n",
    "            \"Stock\", \"Phase\", \"Start Date\", \"End Date\", \"Predicted Binary\", \"Predicted Probability\",\n",
    "            \"Actual Binary\", \"Volatility\", \"Next Period Return\"\n",
    "        ])\n",
    "\n",
    "        # 🔹 Filter for a specific test period (test_dates[1])\n",
    "        selected_test_date = predict_dates[1]  # Replace with the target test end date\n",
    "        filtered_df = classification_df[(classification_df[\"Phase\"] == \"predict_binary\") &\n",
    "                                        (classification_df[\"End Date\"] == selected_test_date)]\n",
    "\n",
    "        # 🔹 Extract required inputs\n",
    "        stock_tickers = filtered_df[\"Stock\"].tolist()\n",
    "        predicted_probs = filtered_df[\"Predicted Probability\"].values.astype(float)\n",
    "        rolling_volatility = filtered_df[\"Volatility\"].values.astype(float)\n",
    "        print(f'predicted_probs={predicted_probs}')\n",
    "        # 🔹 Map stocks to sectors (Assume `sector_dict` contains stock-sector mapping)\n",
    "        sector_dict = {\n",
    "            'BA': 'Industrials',\n",
    "            'AMGN': 'Healthcare',\n",
    "            'DIS': 'Communication Services',\n",
    "            'NKE': 'Consumer Discretionary',\n",
    "            'HON': 'Industrials',\n",
    "            'MMM': 'Industrials',\n",
    "            'CAT': 'Industrials',\n",
    "            'KO': 'Consumer Staples',\n",
    "            'PG': 'Consumer Staples',\n",
    "            'AXP': 'Financials',\n",
    "            'GS': 'Financials',\n",
    "            'JPM': 'Financials',\n",
    "            'MCD': 'Consumer Discretionary',\n",
    "            'HD': 'Consumer Discretionary',\n",
    "            'AAPL': 'Technology',\n",
    "            'CRM': 'Technology',\n",
    "            'CSCO': 'Technology',\n",
    "            'IBM': 'Technology',\n",
    "            'MSFT': 'Technology',\n",
    "            'TRV': 'Financials',\n",
    "            'UNH': 'Healthcare',\n",
    "            'CVX': 'Energy',\n",
    "            'JNJ': 'Healthcare',\n",
    "            'MRK': 'Healthcare',\n",
    "            'AMZN': 'Consumer Discretionary',\n",
    "            'WMT': 'Consumer Staples',\n",
    "            'INTC': 'Technology',\n",
    "            'VZ': 'Communication Services'\n",
    "        }  # Example mapping, replace with real sector data\n",
    "        sectors = [sector_dict.get(stock, \"Unknown\") for stock in stock_tickers]  # Default to \"Unknown\" if missing\n",
    "\n",
    "        # TO-DO: i can use test period data to fine tune confidence_threshold and sector_limit parameter and then trade on prediction period:\n",
    "        # 🔹 Call compute_portfolio_weights function\n",
    "        portfolio_weights = compute_portfolio_weights(\n",
    "            stock_tickers=stock_tickers,\n",
    "            predicted_probs=predicted_probs,\n",
    "            rolling_volatility=rolling_volatility,\n",
    "            sectors=sectors,\n",
    "            confidence_threshold=(0.353392545,\t0.662810753),  # Exclude low-confidence predictions\n",
    "            short_ratio=0.262342442,  # Max 30% per sector\n",
    "            annualize_volatility=False  # Convert daily volatility to annualized if needed\n",
    "        )\n",
    "        bmk_weights_abs_wt = compute_portfolio_weights_BMK(\n",
    "            stock_tickers=stock_tickers,\n",
    "            predicted_probs=predicted_probs,\n",
    "            rolling_volatility=rolling_volatility,\n",
    "            sectors=sectors,\n",
    "            confidence_threshold=(0.0, 0.0),  # Exclude low-confidence predictions\n",
    "            short_ratio=0,  # Max 30% per sector\n",
    "            annualize_volatility=False,\n",
    "            raw_weight_version =1\n",
    "        )\n",
    "        bmk_weights_raw_pred = compute_portfolio_weights_BMK(\n",
    "            stock_tickers=stock_tickers,\n",
    "            predicted_probs=predicted_probs,\n",
    "            rolling_volatility=rolling_volatility,\n",
    "            sectors=sectors,\n",
    "            confidence_threshold=(0.0, 0.0),  # Exclude low-confidence predictions\n",
    "            short_ratio=0,  # Max 30% per sector\n",
    "            annualize_volatility=False,\n",
    "            raw_weight_version=2\n",
    "        )\n",
    "        # 🔹 Display final portfolio allocation\n",
    "        print(\"\\n🔹 Final Portfolio Weights for Test Date:\", selected_test_date)\n",
    "        print(portfolio_weights)\n",
    "\n",
    "        # 🔹 Merge all inputs with portfolio weights\n",
    "        for i in range(len(stock_tickers)):\n",
    "            portfolio_records.append([\n",
    "                selected_test_date,  # Store time period\n",
    "                stock_tickers[i],\n",
    "                sectors[i],\n",
    "                predicted_probs[i],\n",
    "                rolling_volatility[i],\n",
    "                0.353392545,  # Confidence threshold lower bound\n",
    "                0.662810753,  # Confidence threshold upper bound\n",
    "                0.262342442,  # Sector constraint limit\n",
    "                False,  # Annualized volatility flag\n",
    "                portfolio_weights.loc[portfolio_weights[\"Stock\"] == stock_tickers[i], \"Final Weight\"].values[0],\n",
    "                bmk_weights_abs_wt.loc[bmk_weights_abs_wt[\"Stock\"] == stock_tickers[i], \"Final Weight\"].values[0],\n",
    "                bmk_weights_raw_pred.loc[bmk_weights_raw_pred[\"Stock\"] == stock_tickers[i], \"Final Weight\"].values[0]\n",
    "\n",
    "                # Extract weight\n",
    "            ])\n",
    "\n",
    "        start += prediction_window\n",
    "\n",
    "    portfolio_results_df = pd.DataFrame(portfolio_records, columns=[\n",
    "        \"Time Period\", \"Stock\", \"Sector\", \"Predicted Probability\", \"Rolling Volatility\",\n",
    "        \"Confidence Threshold (Low)\", \"Confidence Threshold (High)\", \"Short Ratio\",\n",
    "        \"Annualized Volatility\", \"Final Portfolio Weight\", \"BM Weight Absolute Weight Scale\", \"BM Weight Raw Preditor Scale\"\n",
    "    ])\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    portfolio_results_df.to_csv(\"portfolio_results.csv\", index=False)\n",
    "    # Convert to DataFrame\n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results, columns=[\"Stock\", \"Phase\", \"Date\", \"Predicted Return\", \"Actual Return\"])\n",
    "    df_classification = pd.DataFrame(classification_results,\n",
    "                                     columns=[\"Stock\", \"Phase\", \"Start Date\", \"End Date\", \"Predicted Binary\",\n",
    "                                              \"Predict Probability\", \"Actual Binary\", \"Stock Volatility\",\n",
    "                                              \"Next Period Return\"])\n",
    "\n",
    "    # Save results to CSV\n",
    "    df_results.to_csv(\"stock_predictions_results.csv\", index=False)\n",
    "    df_classification.to_csv(\"stock_predictions_classification.csv\", index=False)\n",
    "\n",
    "    print(\"Results saved successfully!\")\n",
    "\n",
    "elif state == 'train_RL':\n",
    "\n",
    "    # Load sample stock data (Replace with actual data loading)\n",
    "    stock_data = pd.read_csv(\"sample_stock_data.csv\")  # Assume CSV file with test data\n",
    "\n",
    "    # Train the RL agent\n",
    "    trained_model = train_rl_agent(stock_data, max_periods=213, num_episodes=5000)\n",
    "    # trained_model = train_rl_agent(stock_data, max_periods=213, num_episodes=5000)\n",
    "\n",
    "    model_save_path = \"trained_portfolio_rl_agent.zip\"\n",
    "    trained_model.save(model_save_path)\n",
    "    print(f\"Trained model saved to {model_save_path}\")\n",
    "\n",
    "    # Load the trained model (Optional)\n",
    "    loaded_model = PPO.load(model_save_path)\n",
    "    print(\"Loaded trained model successfully!\")\n",
    "\n",
    "    # Get averaged optimal parameters\n",
    "    M = 100  # Number of samples to generate\n",
    "    avg_short_ratio, avg_confidence_threshold = get_average_optimal_parameters(trained_model, stock_data,\n",
    "                                                                               num_periods=10, M=M)\n",
    "\n",
    "    print(\"Averaged Optimized Short Ratio:\", avg_short_ratio)\n",
    "    print(\"Averaged Optimized Confidence Threshold Low:\", avg_confidence_threshold[0])\n",
    "    print(\"Averaged Optimized Confidence Threshold High:\", avg_confidence_threshold[1])\n",
    "\n",
    "    # Save averaged optimal parameters to CSV\n",
    "    optimal_params_df = pd.DataFrame({\n",
    "        'Short Ratio': [avg_short_ratio],\n",
    "        'Confidence Threshold Low': [avg_confidence_threshold[0]],\n",
    "        'Confidence Threshold High': [avg_confidence_threshold[1]]\n",
    "    })\n",
    "    optimal_params_df.to_csv(\"optimal_parameters.csv\", index=False)\n",
    "\n",
    "    print(\"Optimal parameters saved to optimal_parameters.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a8ba5e71c913131",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ace_tools\n",
      "  Downloading ace_tools-0.0-py3-none-any.whl.metadata (300 bytes)\n",
      "Downloading ace_tools-0.0-py3-none-any.whl (1.1 kB)\n",
      "Installing collected packages: ace_tools\n",
      "Successfully installed ace_tools-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ace_tools\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-06T03:22:58.183521Z",
     "start_time": "2025-03-06T03:22:53.736712Z"
    }
   },
   "id": "9fb71d01fb009039",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "((np.float64(0.5799999999999997), np.float64(0.6199999999999998)),\n np.float64(0.0))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "portfolio_data = pd.read_csv(\"sample_stock_data.csv\")  \n",
    "# Extract relevant data from the uploaded dataset\n",
    "stock_tickers = portfolio_data[\"Stock\"].values\n",
    "predicted_probs = portfolio_data[\"Predict Probability\"].values\n",
    "rolling_volatility = portfolio_data[\"Volatility\"].values\n",
    "sectors = portfolio_data[\"Sector\"].values\n",
    "actual_returns = portfolio_data[\"Next Period Return\"].values  # To evaluate portfolio performance\n",
    "\n",
    "# Define search space for confidence_threshold and short_ratio\n",
    "confidence_thresholds = [(low, high) for low, high in product(np.arange(0.1, 0.8, 0.01), repeat=2) if low < high]\n",
    "short_ratios = np.arange(0.0, 0.5, 0.01)\n",
    "\n",
    "# Define function to evaluate Sharpe Ratio for a given parameter set\n",
    "def evaluate_parameters(confidence_threshold, short_ratio):\n",
    "    try:\n",
    "        portfolio_value = 1000 \n",
    "        portfolio_df = compute_portfolio_weights(stock_tickers, predicted_probs, rolling_volatility, sectors,\n",
    "                                                 confidence_threshold=confidence_threshold, short_ratio=short_ratio,\n",
    "                                                 annualize_volatility=False)\n",
    "        # Compute portfolio return\n",
    "        portfolio_return = (portfolio_df['Final Weight'] * portfolio_data['Next Period Return']).sum()\n",
    " \n",
    "        # Update portfolio value\n",
    "        portfolio_value *= (1 + portfolio_return)\n",
    "\n",
    "        # Compute portfolio return and volatility\n",
    "        # portfolio_return = np.sum(portfolio_df[\"Final Weight\"].values * actual_returns)\n",
    "        portfolio_volatility = np.std(portfolio_df['Final Weight'] * portfolio_data['Next Period Return'])\n",
    "        sharpe_ratio = portfolio_return / (portfolio_volatility + 1e-6)  # Avoid division by zero\n",
    "        # sharpe_ratio = portfolio_value\n",
    "        # return sharpe_ratio, portfolio_return, portfolio_volatility\n",
    "        return sharpe_ratio, portfolio_return\n",
    "    except Exception:\n",
    "        return -np.inf, 0  # Return worst possible Sharpe ratio if constraints fail\n",
    "\n",
    "# Perform grid search\n",
    "best_sharpe_ratio = -np.inf\n",
    "best_params = None\n",
    "results = []\n",
    "\n",
    "for conf_thresh, short_r in product(confidence_thresholds, short_ratios):\n",
    "    sharpe, ret = evaluate_parameters(conf_thresh, short_r)\n",
    "    results.append((conf_thresh, short_r, sharpe, ret))\n",
    "    if sharpe > best_sharpe_ratio:\n",
    "        best_sharpe_ratio = sharpe\n",
    "        best_params = (conf_thresh, short_r)\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results, columns=[\"Confidence Threshold\", \"Short Ratio\", \"Sharpe Ratio\", \"Return\"])\n",
    "\n",
    "\n",
    "# Return the best parameters found\n",
    "best_params\n",
    "# results_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-06T03:54:23.164794Z",
     "start_time": "2025-03-06T03:50:54.151583Z"
    }
   },
   "id": "8f63108b034a79db",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portfolio_df=       End Date Stock                  Sector   Probability  Raw Weight  \\\n",
      "0     1/10/2008  AAPL              Technology  6.470000e-08   -1.000000   \n",
      "1     1/10/2008  AMGN              Healthcare  5.655593e-01    0.131119   \n",
      "2     1/10/2008  AMZN  Consumer Discretionary  1.365920e-03   -0.997268   \n",
      "3     1/10/2008   AXP              Financials  1.000000e+00    1.000000   \n",
      "4     1/10/2008    BA             Industrials  5.396600e-04   -0.998921   \n",
      "...         ...   ...                     ...           ...         ...   \n",
      "5959   9/8/2010    PG        Consumer Staples  4.061087e-02   -0.918778   \n",
      "5960   9/8/2010   TRV              Financials  9.813436e-01    0.962687   \n",
      "5961   9/8/2010   UNH              Healthcare  2.026568e-02   -0.959469   \n",
      "5962   9/8/2010    VZ  Communication Services  9.999965e-01    0.999993   \n",
      "5963   9/8/2010   WMT        Consumer Staples  2.013629e-02   -0.959727   \n",
      "\n",
      "      Volatility  Adjusted Weight  Final Weight  \n",
      "0       0.027976       -35.744920     -0.018722  \n",
      "1       0.015867         0.000000      0.000000  \n",
      "2       0.031056       -32.111932     -0.016819  \n",
      "3       0.023051        43.382066      0.107796  \n",
      "4       0.014474       -69.014832     -0.036148  \n",
      "...          ...              ...           ...  \n",
      "5959    0.010466       -87.786954     -0.064653  \n",
      "5960    0.012565        76.616565      0.170012  \n",
      "5961    0.014776       -64.934261     -0.047822  \n",
      "5962    0.011223        89.102119      0.197718  \n",
      "5963    0.010608       -90.472042     -0.066630  \n",
      "\n",
      "[5964 rows x 8 columns]\n",
      "portfolio_return = 1.1779204819765277\n",
      "portfolio_value = 2177.9204819765273\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_portfolio_weights_per_period(df, confidence_threshold=(0.4, 0.6), short_ratio=0.25, annualize_volatility=False):\n",
    "    \"\"\"\n",
    "    Compute optimal portfolio weights for long-short portfolio ensuring total weight sums to 100%\n",
    "    while maintaining the short ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - stock_tickers (list): List of stock tickers.\n",
    "    - predicted_probs (np.array): Predicted probability of stock moving up.\n",
    "    - rolling_volatility (np.array): Rolling volatility (daily or annualized).\n",
    "    - sectors (list): List of sector classifications for each stock.\n",
    "    - confidence_threshold (tuple): Threshold (low, high) to filter out uncertain predictions.\n",
    "    - short_ratio (float): Proportion of short exposure (between 0 and 50%).\n",
    "    - annualize_volatility (bool): Whether to annualize daily volatility using sqrt(252).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Portfolio allocation with normalized long-short weights.\n",
    "    \"\"\"\n",
    "    portfolio_results = []\n",
    "\n",
    "    for period, period_data in df.groupby(\"End Date\"):\n",
    "        stock_tickers = period_data[\"Stock\"].values\n",
    "        predicted_probs = period_data[\"Predict Probability\"].values\n",
    "        rolling_volatility = period_data[\"Volatility\"].values\n",
    "        sectors = period_data[\"Sector\"].values\n",
    "\n",
    "        # Convert probability into raw weights for long-short portfolio\n",
    "        raw_weights = 2 * (predicted_probs - 0.5)\n",
    "\n",
    "        # Apply confidence threshold: Only include confident stocks\n",
    "        confidence_mask = (predicted_probs >= confidence_threshold[1]) | (predicted_probs <= confidence_threshold[0])\n",
    "        filtered_weights = raw_weights * confidence_mask  # Zero out low-confidence predictions\n",
    "\n",
    "        # Skip period if no stocks qualify\n",
    "        if np.all(filtered_weights == 0):\n",
    "            continue\n",
    "\n",
    "        # Annualize volatility if required\n",
    "        if annualize_volatility:\n",
    "            rolling_volatility *= np.sqrt(252)\n",
    "\n",
    "        # Apply Volatility Adjustment: Wi,Tadjusted = Wi,T / σi\n",
    "        adjusted_weights = filtered_weights / rolling_volatility  # Scale by inverse volatility\n",
    "\n",
    "        # Separate long and short portfolios based on short ratio\n",
    "        long_weights = np.where(adjusted_weights > 0, adjusted_weights, 0)\n",
    "        short_weights = np.where(adjusted_weights < 0, adjusted_weights, 0)\n",
    "\n",
    "        # Normalize long and short weights so they sum to the target allocations\n",
    "        long_sum = np.sum(long_weights)\n",
    "        short_sum = np.abs(np.sum(short_weights))\n",
    "\n",
    "        if long_sum > 0:\n",
    "            long_weights *= (1 + short_ratio) / long_sum  # Scale long portfolio to (1 + short_ratio)\n",
    "        if short_sum > 0:\n",
    "            short_weights *= short_ratio / short_sum  # Scale short portfolio to -short_ratio\n",
    "\n",
    "        # Compute final portfolio weights (long + short should sum to 100%)\n",
    "        final_weights = long_weights + short_weights\n",
    "\n",
    "        # Store results\n",
    "        period_portfolio = pd.DataFrame({\n",
    "            \"End Date\": period,\n",
    "            \"Stock\": stock_tickers,\n",
    "            \"Sector\": sectors,\n",
    "            \"Probability\": predicted_probs,\n",
    "            \"Raw Weight\": raw_weights,\n",
    "            \"Volatility\": rolling_volatility,\n",
    "            \"Adjusted Weight\": adjusted_weights,\n",
    "            \"Final Weight\": final_weights\n",
    "        })\n",
    "\n",
    "        portfolio_results.append(period_portfolio)\n",
    "\n",
    "    # Combine all periods into a single DataFrame\n",
    "    portfolio_df = pd.concat(portfolio_results, ignore_index=True)\n",
    "    return portfolio_df\n",
    "portfolio_data = pd.read_csv(\"sample_stock_data.csv\")  \n",
    "portfolio_value = 1000 \n",
    "confidence_threshold = (0.1, 0.66)\n",
    "short_ratio = 0.49\n",
    "portfolio_df = compute_portfolio_weights_per_period(portfolio_data, confidence_threshold=confidence_threshold, short_ratio=short_ratio,\n",
    "                                     annualize_volatility=False)\n",
    "print(f'portfolio_df={portfolio_df}')\n",
    "# Compute portfolio return\n",
    "portfolio_return = (portfolio_df['Final Weight'] * portfolio_data['Next Period Return']).sum()\n",
    "\n",
    "print(f'portfolio_return = {portfolio_return}')\n",
    "# Update portfolio value\n",
    "portfolio_value *= (1 + portfolio_return)\n",
    "print(f'portfolio_value = {portfolio_value}')\n",
    "# Compute portfolio return and volatility\n",
    "# portfolio_return = np.sum(portfolio_df[\"Final Weight\"].values * actual_returns)\n",
    "# portfolio_volatility = np.std(portfolio_df[\"Final Weight\"].values * rolling_volatility)\n",
    "# sharpe_ratio = portfolio_return / (portfolio_volatility + 1e-6)  # Avoid division by zero\n",
    "sharpe_ratio = portfolio_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-06T04:07:38.924219Z",
     "start_time": "2025-03-06T04:07:38.805193Z"
    }
   },
   "id": "f2154f95c1f06327",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for IYW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for IYF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for IYZ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for IYM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for IYK...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for IYC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for IYE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for IYG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for IYH...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for IYJ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for IYR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for IDU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: AAPL data is required to check the reference length.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper\n",
    "\n",
    "# Example usage\n",
    "input_dir = \"E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\"  # Use the specified directory\n",
    "ticker_file = os.path.join(input_dir, \"tickers.txt\")  # Text file containing tickers, one per line\n",
    "factor_file = os.path.join(input_dir, \"FF_factors.csv\")  # CSV file containing factor returns\n",
    "start_date = \"2005-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "output_csv = os.path.join(input_dir, \"stock_prices.csv\")\n",
    "\n",
    "try:\n",
    "    tickers = load_tickers_from_file(ticker_file)\n",
    "    price_data = download_stock_prices(tickers, start_date, end_date, output_csv)\n",
    "    daily_returns = calculate_daily_returns(price_data, return_type='arithmetic')\n",
    "    daily_returns.to_csv(os.path.join(input_dir, \"daily_returns.csv\"))\n",
    "    factor_returns = read_factor_returns(factor_file, daily_returns)\n",
    "    factor_returns.to_csv(os.path.join(input_dir, \"aligned_factors.csv\"))\n",
    "    print(\"Factor returns alignment completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T22:13:14.173876Z",
     "start_time": "2025-03-07T22:13:11.010722Z"
    }
   },
   "id": "9381f1e33f39368",
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
