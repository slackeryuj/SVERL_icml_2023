{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "895e7096145459d4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # stage 1 training and prediction\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "# import shap\n",
    "# import ta\n",
    "# import joblib\n",
    "# from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# import time\n",
    "# import random\n",
    "# import os\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import numpy as np\n",
    "# \n",
    "# GLOBAL_SEED = 42\n",
    "# np.random.seed(GLOBAL_SEED)\n",
    "# random.seed(GLOBAL_SEED)\n",
    "# os.environ['PYTHONHASHSEED'] = str(GLOBAL_SEED)\n",
    "# os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "# # ------------------------------------------------------------------------\n",
    "# # 0. Load and align data\n",
    "# # ------------------------------------------------------------------------\n",
    "# factors = pd.read_csv(\"aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "# returns = pd.read_csv(\"daily_returns_DIA_ETF.csv\", index_col=0, parse_dates=True)\n",
    "# \n",
    "# # Align dates to ensure matching indices\n",
    "# dates = factors.index.intersection(returns.index)\n",
    "# factors = factors.loc[dates]\n",
    "# returns = returns.loc[dates]\n",
    "# \n",
    "# \n",
    "# # ------------------------------------------------------------------------\n",
    "# # 1. Compute technical indicators and lagged features per ETF\n",
    "# # ------------------------------------------------------------------------\n",
    "# all_tech_features = []\n",
    "# \n",
    "# for etf in returns.columns:\n",
    "#     close = (1 + returns[etf]).cumprod()\n",
    "#     tech_df = pd.DataFrame(index=returns.index)\n",
    "# \n",
    "#     # Selected indicators (others commented out to reduce noise)\n",
    "#     tech_df[f'{etf}_SMA_5']   = ta.trend.sma_indicator(close, window=5)\n",
    "#     tech_df[f'{etf}_EMA_12']  = ta.trend.ema_indicator(close, window=12)\n",
    "#     tech_df[f'{etf}_RSI_7']   = ta.momentum.rsi(close, window=7)\n",
    "#     tech_df[f'{etf}_MACD']    = ta.trend.macd_diff(close)\n",
    "#     tech_df[f'{etf}_ATR']     = ta.volatility.average_true_range(\n",
    "#         high=close * 1.01, low=close * 0.99, close=close, window=10\n",
    "#     )\n",
    "#     tech_df[f'{etf}_Vol_5']   = returns[etf].rolling(window=5).std()\n",
    "#     tech_df[f'{etf}_Mom_3']   = returns[etf].rolling(window=3).mean()\n",
    "# \n",
    "#     # Lagged returns (shifted so only past information is used)\n",
    "#     for lag in [1, 2, 3]:\n",
    "#         tech_df[f'{etf}_LagRet_{lag}'] = returns[etf].shift(lag)\n",
    "# \n",
    "#     all_tech_features.append(tech_df)\n",
    "# \n",
    "# # Concatenate technical indicators for all ETFs\n",
    "# technical_features = pd.concat(all_tech_features, axis=1)\n",
    "# \n",
    "# # ------------------------------------------------------------------------\n",
    "# # 2. Create lagged factor features\n",
    "# # ------------------------------------------------------------------------\n",
    "# for factor in ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']:\n",
    "#     for lag in [1, 2, 3]:\n",
    "#         factors[f'{factor}_lag_{lag}'] = factors[factor].shift(lag)\n",
    "# \n",
    "# # Drop rows with NA values arising from lagging\n",
    "# factors = factors.dropna()\n",
    "# \n",
    "# # ------------------------------------------------------------------------\n",
    "# # 3. Combine factors, technical features, and VIX change\n",
    "# # ------------------------------------------------------------------------\n",
    "# features = pd.concat([factors, technical_features], axis=1).dropna()\n",
    "# vix = pd.read_csv(\"VIX_History.csv\", index_col=0, parse_dates=True)\n",
    "# \n",
    "# # Align VIX to our feature dates and compute lagged change\n",
    "# vix_aligned = vix['CLOSE'].reindex(features.index).ffill()\n",
    "# features['VIX'] = vix_aligned.pct_change(fill_method=None).shift(1)\n",
    "# features['VIX'] = features['VIX'].fillna(0)\n",
    "# \n",
    "# # Define the target: next-day return per ETF\n",
    "# target_returns = returns.shift(-1).loc[features.index].dropna()\n",
    "# features = features.loc[target_returns.index]\n",
    "# \n",
    "# # ------------------------------------------------------------------------\n",
    "# # 4. Define rolling window parameters\n",
    "# # ------------------------------------------------------------------------\n",
    "# train_years = 12      # years used for training\n",
    "# valid_years = 1       # years used for validation\n",
    "# test_years  = 1       # years used for testing/prediction\n",
    "# retrain_frequency = 1 # years between retrainings\n",
    "# start_year = 2009\n",
    "# end_year   = 2024\n",
    "# \n",
    "# # List generic features used for SHAP importance ranking\n",
    "# all_generic_features = [\n",
    "#     'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA',\n",
    "#     'Mkt-RF_lag_1', 'Mkt-RF_lag_2', 'Mkt-RF_lag_3',\n",
    "#     'SMB_lag_1', 'SMB_lag_2', 'SMB_lag_3',\n",
    "#     'HML_lag_1', 'HML_lag_2', 'HML_lag_3',\n",
    "#     'RMW_lag_1', 'RMW_lag_2', 'RMW_lag_3',\n",
    "#     'CMA_lag_1', 'CMA_lag_2', 'CMA_lag_3',\n",
    "#     'SMA_5', 'EMA_12', 'RSI_7', 'MACD',\n",
    "#     'Vol_5', 'Mom_3',\n",
    "#     'LagRet_1', 'LagRet_2', 'LagRet_3', 'VIX'\n",
    "# ]\n",
    "# \n",
    "# # ------------------------------------------------------------------------\n",
    "# # 5. Compute generic feature importance via SHAP\n",
    "# #    (aggregated across ETFs, using only the initial training window)\n",
    "# # ------------------------------------------------------------------------\n",
    "# shap_importances = pd.DataFrame(0.0, index=all_generic_features, columns=['SHAP_Value'])\n",
    "# \n",
    "# # Use a fixed period (e.g. up to year 2009) for computing importances\n",
    "# base_train_start = pd.Timestamp(start_year - train_years, 1, 1)\n",
    "# base_train_end   = pd.Timestamp(start_year - valid_years - 1, 12, 31)\n",
    "# \n",
    "# for etf in returns.columns:\n",
    "#     print(f\"Computing SHAP importances for ETF: {etf}\")\n",
    "#     # Filter columns relevant to this ETF (generic + factor features)\n",
    "#     etf_cols = [\n",
    "#         col for col in features.columns\n",
    "#         if (etf in col and any(k in col for k in ['SMA_5', 'EMA_12', 'RSI_7',\n",
    "#                                                   'MACD', 'Vol_5', 'Mom_3',\n",
    "#                                                   'LagRet_1','LagRet_2','LagRet_3', 'VIX']))\n",
    "#         or col in ['Mkt-RF','SMB','HML','RMW','CMA',\n",
    "#                    'Mkt-RF_lag_1','Mkt-RF_lag_2','Mkt-RF_lag_3',\n",
    "#                    'SMB_lag_1','SMB_lag_2','SMB_lag_3',\n",
    "#                    'HML_lag_1','HML_lag_2','HML_lag_3',\n",
    "#                    'RMW_lag_1','RMW_lag_2','RMW_lag_3',\n",
    "#                    'CMA_lag_1','CMA_lag_2','CMA_lag_3']\n",
    "#     ]\n",
    "#     X_base  = features.loc[base_train_start:base_train_end, etf_cols]\n",
    "#     y_base  = target_returns[etf].loc[base_train_start:base_train_end]\n",
    "# \n",
    "#     # Fit a quick model to compute SHAP\n",
    "#     model_base = xgb.XGBRegressor(\n",
    "#         objective='reg:squarederror',\n",
    "#         tree_method='hist',\n",
    "#         random_state=GLOBAL_SEED,\n",
    "#         seed=GLOBAL_SEED,\n",
    "#         device='cuda'\n",
    "#     ).fit(X_base, y_base)\n",
    "# \n",
    "#     explainer_base = shap.Explainer(model_base)\n",
    "#     shap_vals = explainer_base(X_base)\n",
    "# \n",
    "#     # Aggregate SHAP values per generic feature\n",
    "#     for gen_feat in all_generic_features:\n",
    "#         cols = [c for c in X_base.columns if gen_feat in c]\n",
    "#         if cols:\n",
    "#             idx = [X_base.columns.get_loc(c) for c in cols]\n",
    "#             shap_importances.loc[gen_feat] += np.mean(np.abs(shap_vals.values[:, idx]))\n",
    "# \n",
    "# # Average importance across ETFs and select top N\n",
    "# shap_importances /= len(returns.columns)\n",
    "# top_generic_features = (\n",
    "#     shap_importances.sort_values('SHAP_Value', ascending=False)\n",
    "#                     .head(10)\n",
    "#                     .index\n",
    "#                     .tolist()\n",
    "# )\n",
    "# \n",
    "# # returns = returns.iloc[:,:2]\n",
    "# # ------------------------------------------------------------------------\n",
    "# # 6. Retrain models using the selected generic features in rolling windows\n",
    "# # ------------------------------------------------------------------------\n",
    "# all_predictions = []\n",
    "# \n",
    "# # for etf in returns.columns:\n",
    "# #     print(f\"\\n==== Training models for ETF: {etf} ====\")\n",
    "# #     # Select columns containing any of the top_generic_features or factor names\n",
    "# #     selected_features = [\n",
    "# #         f for f in features.columns\n",
    "# #         if any(gen in f for gen in top_generic_features) or f in factors.columns\n",
    "# #     ]\n",
    "# \n",
    "# for etf in returns.columns:\n",
    "#     print(f\"\\n==== Training models for ETF: {etf} ====\")\n",
    "# \n",
    "#     # Select features explicitly relevant to current ETF\n",
    "#     # selected_features = [\n",
    "#     #     f for f in features.columns\n",
    "#     #     if (\n",
    "#     #         # Include ETF-specific technical indicators explicitly\n",
    "#     #         (any(gen in f for gen in top_generic_features) and (etf in f))\n",
    "#     #         # Include ONLY explicitly selected generic FF factors or their lags\n",
    "#     #         or (any(gen == f for gen in top_generic_features))\n",
    "#     #     )\n",
    "#     # ]\n",
    "# \n",
    "#     selected_features = []\n",
    "# \n",
    "#     for feature in top_generic_features:\n",
    "#         # Clearly check if the feature is ETF-specific (technical indicators)\n",
    "#         etf_specific_feature_name = f'{etf}_{feature}'\n",
    "# \n",
    "#         # Add ETF-specific feature explicitly if present in columns\n",
    "#         if etf_specific_feature_name in features.columns:\n",
    "#             selected_features.append(etf_specific_feature_name)\n",
    "# \n",
    "#         # If not ETF-specific, explicitly add generic factor features directly\n",
    "#         elif feature in features.columns:\n",
    "#             selected_features.append(feature)\n",
    "# \n",
    "#     # Sanity check to ensure you have valid selected features\n",
    "#     if not selected_features:\n",
    "#         raise ValueError(f\"No features selected for {etf}, please verify feature names.\")\n",
    "# \n",
    "#     print(f\"Selected features for {etf}: {selected_features}\")\n",
    "# \n",
    "#     year = start_year\n",
    "#     while year <= end_year - test_years + 1:\n",
    "#         print(f\"\\nTraining window starting {year}\")\n",
    "#         start_time = time.time()\n",
    "# \n",
    "#         # Define periods\n",
    "#         train_start = pd.Timestamp(year - train_years, 1, 1)\n",
    "#         train_end   = pd.Timestamp(year - valid_years - 1, 12, 31)\n",
    "#         valid_start = pd.Timestamp(year - valid_years, 1, 1)\n",
    "#         valid_end   = pd.Timestamp(year - 1, 12, 31)\n",
    "#         test_start  = pd.Timestamp(year, 1, 1)\n",
    "#         test_end    = pd.Timestamp(year + test_years - 1, 12, 31)\n",
    "# \n",
    "#         # Extract data\n",
    "#         X_train = features.loc[train_start:train_end, selected_features]\n",
    "#         y_train = target_returns[etf].loc[train_start:train_end]\n",
    "#         X_valid = features.loc[valid_start:valid_end, selected_features]\n",
    "#         y_valid = target_returns[etf].loc[valid_start:valid_end]\n",
    "#         X_test  = features.loc[test_start:test_end, selected_features]\n",
    "#         y_test  = target_returns[etf].loc[test_start:test_end]\n",
    "# \n",
    "#         scaler = StandardScaler()\n",
    "#         X_train_scaled = scaler.fit_transform(X_train)\n",
    "#         X_valid_scaled = scaler.transform(X_valid)\n",
    "#         X_test_scaled = scaler.transform(X_test)\n",
    "# \n",
    "#         # # Base model with early stopping\n",
    "#         # base_model = xgb.XGBRegressor(\n",
    "#         #     objective='reg:squarederror',\n",
    "#         #     tree_method='hist',\n",
    "#         #     device='cuda',\n",
    "#         #     random_state=42,\n",
    "#         #     n_jobs=4,\n",
    "#         #     # eval_metric='rmse', # The metric to monitor for early stopping\n",
    "#         #     # early_stopping_rounds=50\n",
    "#         # )\n",
    "#         # \n",
    "#         # param_grid = {\n",
    "#         #     'n_estimators': [200, 400],\n",
    "#         #     'max_depth': [3, 4, 5],\n",
    "#         #     'learning_rate': [0.03, 0.05],\n",
    "#         #     'subsample': [0.7, 0.8],\n",
    "#         #     'colsample_bytree': [0.7, 0.8]\n",
    "#         # }\n",
    "#         # \n",
    "#         # tscv = TimeSeriesSplit(n_splits=3)\n",
    "#         # \n",
    "#         # grid_search = GridSearchCV(\n",
    "#         #     base_model,\n",
    "#         #     param_grid,\n",
    "#         #     cv=tscv,\n",
    "#         #     scoring='neg_mean_squared_error',\n",
    "#         #     verbose=0,\n",
    "#         #     n_jobs=4\n",
    "#         # )\n",
    "#         # \n",
    "#         # # Fit with early stopping on the explicit validation set\n",
    "#         # # fit_params = {\n",
    "#         # #     \"eval_set\": [(X_valid, y_valid)],\n",
    "#         # #     \"verbose\": False\n",
    "#         # # }\n",
    "#         # # \n",
    "#         # # # grid_search.fit(X_train, y_train, **fit_params)\n",
    "#         # # grid_search.fit(X_train_scaled, y_train)\n",
    "#         # \n",
    "#         # fit_params = {\n",
    "#         #     'eval_set': [(X_valid_scaled, y_valid)],\n",
    "#         #     'eval_metric': 'rmse',\n",
    "#         #     'early_stopping_rounds': 50,\n",
    "#         #     'verbose': False\n",
    "#         # }\n",
    "#         # \n",
    "#         # grid_search.fit(X_train_scaled, y_train, **fit_params)\n",
    "#         # \n",
    "#         # best_model = grid_search.best_estimator_\n",
    "#         # \n",
    "#         # # Predict on the test period\n",
    "#         # preds = best_model.predict(X_test_scaled)\n",
    "# \n",
    "# \n",
    "#         # Define your parameter grid explicitly\n",
    "#         param_grid = {\n",
    "#             'n_estimators': [200, 400],\n",
    "#             'max_depth': [3, 4, 5],\n",
    "#             'learning_rate': [0.03, 0.05],\n",
    "#             'subsample': [0.7, 0.8],\n",
    "#             'colsample_bytree': [0.7, 0.8]\n",
    "#         }\n",
    "# \n",
    "#         tscv = TimeSeriesSplit(n_splits=3)\n",
    "# \n",
    "#         best_score = float('inf')\n",
    "#         best_params = None\n",
    "#         best_model = None\n",
    "# \n",
    "#         # Explicit loop for parameter search and cross-validation\n",
    "#         for params in ParameterGrid(param_grid):\n",
    "#             cv_rmse = []\n",
    "# \n",
    "#             for train_idx, val_idx in tscv.split(X_train_scaled):\n",
    "#                 X_fold_train, X_fold_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "#                 y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "# \n",
    "#                 # DMatrix explicitly required by XGBoost native API\n",
    "#                 dtrain = xgb.DMatrix(X_fold_train, label=y_fold_train)\n",
    "#                 dval = xgb.DMatrix(X_fold_val, label=y_fold_val)\n",
    "# \n",
    "#                 # Setup watchlist explicitly for early stopping\n",
    "#                 watchlist = [(dtrain, 'train'), (dval, 'validation')]\n",
    "# \n",
    "#                 xgb_params = {\n",
    "#                     'objective': 'reg:squarederror',\n",
    "#                     'tree_method': 'hist',\n",
    "#                     'device': 'cuda',\n",
    "#                     'eval_metric': 'rmse',\n",
    "#                     'seed': 42,\n",
    "#                     'max_depth': params['max_depth'],\n",
    "#                     'learning_rate': params['learning_rate'],\n",
    "#                     'subsample': params['subsample'],\n",
    "#                     'colsample_bytree': params['colsample_bytree']\n",
    "#                 }\n",
    "# \n",
    "#                 # Explicitly train with early stopping\n",
    "#                 model = xgb.train(\n",
    "#                     xgb_params,\n",
    "#                     dtrain,\n",
    "#                     num_boost_round=params['n_estimators'],\n",
    "#                     evals=watchlist,\n",
    "#                     early_stopping_rounds=50,\n",
    "#                     verbose_eval=False\n",
    "#                 )\n",
    "# \n",
    "#                 preds = model.predict(dval)\n",
    "#                 rmse = np.sqrt(mean_squared_error(y_fold_val, preds))\n",
    "#                 cv_rmse.append(rmse)\n",
    "# \n",
    "#             avg_rmse = np.mean(cv_rmse)\n",
    "#             # print(f\"Params: {params}, CV Avg RMSE: {avg_rmse:.6f}\")\n",
    "# \n",
    "#             if avg_rmse < best_score:\n",
    "#                 best_score = avg_rmse\n",
    "#                 best_params = params\n",
    "#                 best_model = model\n",
    "# \n",
    "#         # Train final model explicitly with best parameters on full training data\n",
    "#         dtrain_full = xgb.DMatrix(X_train_scaled, label=y_train)\n",
    "#         dvalid_full = xgb.DMatrix(X_valid_scaled, label=y_valid)\n",
    "# \n",
    "#         watchlist_full = [(dtrain_full, 'train'), (dvalid_full, 'validation')]\n",
    "# \n",
    "#         final_xgb_params = {\n",
    "#             'objective': 'reg:squarederror',\n",
    "#             'tree_method': 'hist',\n",
    "#             'device': 'cuda',\n",
    "#             'eval_metric': 'rmse',\n",
    "#             'seed': 42,\n",
    "#             'max_depth': best_params['max_depth'],\n",
    "#             'learning_rate': best_params['learning_rate'],\n",
    "#             'subsample': best_params['subsample'],\n",
    "#             'colsample_bytree': best_params['colsample_bytree']\n",
    "#         }\n",
    "# \n",
    "#         best_model = xgb.train(\n",
    "#             final_xgb_params,\n",
    "#             dtrain_full,\n",
    "#             num_boost_round=best_params['n_estimators'],\n",
    "#             evals=watchlist_full,\n",
    "#             early_stopping_rounds=50,\n",
    "#             verbose_eval=False\n",
    "#         )\n",
    "# \n",
    "#         # Predict explicitly on test data\n",
    "#         dtest = xgb.DMatrix(X_test_scaled)\n",
    "#         preds = best_model.predict(dtest)\n",
    "# \n",
    "#         # Metrics clearly\n",
    "#         test_rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "#         # print(f\"Test RMSE: {test_rmse:.6f}\")\n",
    "# \n",
    "#         # Compute evaluation metrics\n",
    "#         mse  = mean_squared_error(y_test, preds)\n",
    "#         rmse = np.sqrt(mse)\n",
    "#         mae  = mean_absolute_error(y_test, preds)\n",
    "#         r2   = r2_score(y_test, preds)\n",
    "#         dir_acc = np.mean((np.sign(y_test) == np.sign(preds)).astype(int))\n",
    "# \n",
    "#         print(f\"MSE: {mse:.6f}  RMSE: {rmse:.6f}  MAE: {mae:.6f}  \"\n",
    "#               f\"R²: {r2:.6f}  DirAcc: {dir_acc:.2%}\")\n",
    "# \n",
    "#         # Save the model for reproducibility\n",
    "#         joblib.dump(best_model, f\"best_model_{etf}_{year}.joblib\")\n",
    "# \n",
    "#         # Save predictions\n",
    "#         preds_df = pd.DataFrame({\n",
    "#             'Date': X_test.index,\n",
    "#             'ETF': etf,\n",
    "#             'Year': year,\n",
    "#             'Actual_Return': y_test,\n",
    "#             'Predicted_Return': preds\n",
    "#         }).reset_index(drop=True)\n",
    "# \n",
    "#         # Compute SHAP values on the test set\n",
    "#         explainer_test = shap.Explainer(best_model, feature_names=X_test.columns)\n",
    "#         shap_vals_test = explainer_test(X_test_scaled)\n",
    "# \n",
    "#         clean_shap_cols = [\n",
    "#             f'SHAP_{col.replace(f\"{etf}_\", \"\")}' if col.startswith(f'{etf}_') else f'SHAP_{col}'\n",
    "#             for col in X_test.columns\n",
    "#         ]\n",
    "# \n",
    "#         shap_df = pd.DataFrame(\n",
    "#             shap_vals_test.values,\n",
    "#             columns=clean_shap_cols,\n",
    "#             # columns=[f'SHAP_{col}' for col in X_test.columns],\n",
    "#             index=X_test.index\n",
    "#         ).reset_index().rename(columns={'index': 'Date'})\n",
    "# \n",
    "#         # Merge SHAP values with predictions\n",
    "#         preds_df = preds_df.merge(shap_df, on='Date', how='left')\n",
    "# \n",
    "#         all_predictions.append(preds_df)\n",
    "# \n",
    "#         # Advance the window\n",
    "#         year += retrain_frequency\n",
    "#         print(f\"Window processed in {time.time() - start_time:.2f} seconds\")\n",
    "# \n",
    "# # Concatenate and save all predictions and SHAP values\n",
    "# final_predictions_df = pd.concat(all_predictions, ignore_index=True)\n",
    "# final_predictions_df.to_csv(\"stage1_predictions_with_shap_DIA_ETF.csv\", index=False)\n",
    "# \n",
    "# print(\"Stage 1 completed and data saved for Stage 2.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.008626Z",
     "start_time": "2025-08-12T04:25:51.991623Z"
    }
   },
   "id": "8f0500c59fdc3167",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.024630Z",
     "start_time": "2025-08-12T04:25:52.010628Z"
    }
   },
   "id": "a7d768cbcbe73b35",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.072641Z",
     "start_time": "2025-08-12T04:25:52.057638Z"
    }
   },
   "id": "c449d0418b87114f",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.104648Z",
     "start_time": "2025-08-12T04:25:52.099647Z"
    }
   },
   "id": "46b46b2e610dc12b",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.136655Z",
     "start_time": "2025-08-12T04:25:52.133654Z"
    }
   },
   "id": "ea958cf59209ccad",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import json\n",
    "# \n",
    "# # Load stage 1 predictions with SHAP values explicitly\n",
    "# stage1_df = pd.read_csv(\"stage1_predictions_with_shap_DIA_ETF.csv\", parse_dates=['Date'])\n",
    "# etfs = stage1_df['ETF'].unique()\n",
    "# \n",
    "# # Initialize DataFrame explicitly for aggregated daily data\n",
    "# dates = sorted(stage1_df['Date'].unique())\n",
    "# aggregated_data = pd.DataFrame({'Date': dates})\n",
    "# \n",
    "# # Pivot tables for efficient cross-sectional computations\n",
    "# predicted_returns = stage1_df.pivot(index='Date', columns='ETF', values='Predicted_Return')\n",
    "# actual_returns = stage1_df.pivot(index='Date', columns='ETF', values='Actual_Return')\n",
    "# \n",
    "# # Compute ETF-specific volatility (rolling 5-day window)\n",
    "# volatility = actual_returns.rolling(window=5).std()\n",
    "# \n",
    "# # Merge explicitly into aggregated_data\n",
    "# for etf in etfs:\n",
    "#     aggregated_data[f'Predicted_Return_{etf}'] = aggregated_data['Date'].map(predicted_returns[etf])\n",
    "#     aggregated_data[f'Actual_Return_{etf}'] = aggregated_data['Date'].map(actual_returns[etf])\n",
    "#     aggregated_data[f'Volatility_{etf}'] = aggregated_data['Date'].map(volatility[etf])\n",
    "# \n",
    "# # Dynamically load the top generic features from Stage 1 explicitly to maintain consistency\n",
    "# # generic_shap_features = ['LagRet_1',\n",
    "# #  'HML_lag_2',\n",
    "# #  'LagRet_2',\n",
    "# #  'Vol_5',\n",
    "# #  'Mom_3',\n",
    "# #  'LagRet_3',\n",
    "# #  'SMB_lag_2',\n",
    "# #  'MACD',\n",
    "# #  'Mkt-RF',\n",
    "# #  'HML']\n",
    "# generic_shap_features = top_generic_features\n",
    "# \n",
    "# # Aggregate SHAP values (mean and std across ETFs) explicitly by generic feature\n",
    "# shap_aggregated_features = {}\n",
    "# \n",
    "# for feature in generic_shap_features:\n",
    "#     matching_cols = [col for col in stage1_df.columns \n",
    "#                      if col.startswith('SHAP_') and col.endswith(feature)]\n",
    "# \n",
    "#     if matching_cols:\n",
    "#         shap_means = stage1_df.groupby('Date')[matching_cols].mean().mean(axis=1)\n",
    "#         shap_stds = stage1_df.groupby('Date')[matching_cols].std().mean(axis=1)\n",
    "# \n",
    "#         shap_aggregated_features[f'Avg_SHAP_{feature}'] = shap_means\n",
    "#         shap_aggregated_features[f'Std_SHAP_{feature}'] = shap_stds\n",
    "#     else:\n",
    "#         print(f\"Warning: No matches found for SHAP feature: {feature}\")\n",
    "# \n",
    "# # Convert aggregated SHAP features explicitly to DataFrame\n",
    "# shap_aggregated_df = pd.DataFrame(shap_aggregated_features).reset_index()\n",
    "# \n",
    "# # Merge aggregated SHAP features explicitly\n",
    "# aggregated_data = pd.merge(aggregated_data, shap_aggregated_df, on='Date', how='left')\n",
    "# \n",
    "# # Explicitly compute additional cross-sectional signals for richer Stage 2 observations\n",
    "# # Cross-sectional mean and std of predicted returns\n",
    "# aggregated_data['CrossSec_Mean_PredRet'] = predicted_returns.mean(axis=1).values\n",
    "# aggregated_data['CrossSec_Std_PredRet'] = predicted_returns.std(axis=1).values\n",
    "# \n",
    "# # Cross-sectional mean volatility\n",
    "# aggregated_data['CrossSec_Mean_Volatility'] = volatility.mean(axis=1).values\n",
    "# \n",
    "# # Rank ETFs by predicted return explicitly (percentile ranks)\n",
    "# ranked_preds = predicted_returns.rank(axis=1, pct=True)\n",
    "# for etf in etfs:\n",
    "#     aggregated_data[f'Rank_PredRet_{etf}'] = aggregated_data['Date'].map(ranked_preds[etf])\n",
    "# \n",
    "# # Handle missing values explicitly and robustly:\n",
    "# # Forward-fill only SHAP and cross-sectional features explicitly\n",
    "# shap_and_crosssec_cols = [col for col in aggregated_data.columns if 'SHAP' in col or 'CrossSec' in col]\n",
    "# aggregated_data[shap_and_crosssec_cols] = aggregated_data[shap_and_crosssec_cols].ffill()\n",
    "# \n",
    "# # Drop rows explicitly where ETF volatility calculations have initial NaNs\n",
    "# vol_cols = [f'Volatility_{etf}' for etf in etfs]\n",
    "# aggregated_data.dropna(subset=vol_cols, inplace=True)\n",
    "# \n",
    "# # Final sanity checks explicitly for data quality assurance\n",
    "# if aggregated_data.empty:\n",
    "#     raise ValueError(\"Aggregated dataset is empty after preprocessing. Verify your input data.\")\n",
    "# else:\n",
    "#     # Quick summary statistics explicitly for diagnostics\n",
    "#     print(\"Aggregated DataFrame shape:\", aggregated_data.shape)\n",
    "#     print(\"Aggregated DataFrame summary stats:\")\n",
    "#     print(aggregated_data.describe().transpose())\n",
    "# \n",
    "#     # Save optimized data explicitly for Stage 2\n",
    "#     aggregated_data.to_csv(\"stage2_rl_observations_optimized_DIA_ETF.csv\", index=False)\n",
    "#     print(\"Optimized Stage 2 RL dataset successfully saved.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.184666Z",
     "start_time": "2025-08-12T04:25:52.172664Z"
    }
   },
   "id": "626373434f64d0c4",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.264685Z",
     "start_time": "2025-08-12T04:25:52.250683Z"
    }
   },
   "id": "d8f3f0813e659ead",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.296693Z",
     "start_time": "2025-08-12T04:25:52.290691Z"
    }
   },
   "id": "cfe656732378c6e7",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.343703Z",
     "start_time": "2025-08-12T04:25:52.332701Z"
    }
   },
   "id": "7aba3a43020d11ca",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.375709Z",
     "start_time": "2025-08-12T04:25:52.370709Z"
    }
   },
   "id": "93ecdbb778338c51",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.407717Z",
     "start_time": "2025-08-12T04:25:52.403715Z"
    }
   },
   "id": "5556d1f98d4833ec",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "# import os\n",
    "# \n",
    "# # Configuration\n",
    "# output_dir = 'stage2_iterations'  # Adjust if your path is different\n",
    "# pattern = os.path.join(output_dir, 'iteration_*', 'window_*', 'weights.csv')\n",
    "# \n",
    "# # Find all weight files matching the pattern\n",
    "# files = glob.glob(pattern)\n",
    "# \n",
    "# # Initialize an empty list to collect DataFrames\n",
    "# all_weights = []\n",
    "# \n",
    "# for file_path in files:\n",
    "#     # Extract iteration and window numbers\n",
    "#     parts = file_path.split(os.sep)\n",
    "#     iteration = int(parts[-3].split('_')[1])\n",
    "#     window = int(parts[-2].split('_')[1])\n",
    "# \n",
    "#     # Load weights file\n",
    "#     df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "# \n",
    "#     # Add columns for iteration and window\n",
    "#     df.insert(0, 'Window', window)\n",
    "#     df.insert(0, 'Iteration', iteration)\n",
    "# \n",
    "#     # Append to the list\n",
    "#     all_weights.append(df)\n",
    "# \n",
    "# # Concatenate all DataFrames into one\n",
    "# combined_df = pd.concat(all_weights, ignore_index=True)\n",
    "# \n",
    "# # Sort by iteration, window, and date\n",
    "# combined_df.sort_values(['Iteration', 'Window', 'Date'], inplace=True)\n",
    "# \n",
    "# # Save combined data\n",
    "# combined_df.to_csv(os.path.join(output_dir, 'combined_weights.csv'), index=False)\n",
    "# \n",
    "# print(f\"Combined weights saved to: {os.path.join(output_dir, 'combined_weights.csv')}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.503739Z",
     "start_time": "2025-08-12T04:25:52.493736Z"
    }
   },
   "id": "9b01351041b154b6",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from scipy.stats import spearmanr\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# def _monthly_agg(series, method=\"compound\"):\n",
    "#     \"\"\"Aggregate a daily return series to monthly.\"\"\"\n",
    "#     s = series.dropna()\n",
    "#     if s.empty: \n",
    "#         return np.nan\n",
    "#     if method == \"sum\":\n",
    "#         return s.sum()                 # good if returns are already small daily\n",
    "#     elif method == \"compound\":\n",
    "#         return float(np.prod(1.0 + s.values) - 1.0)  # exact\n",
    "#     else:\n",
    "#         raise ValueError(\"method must be 'sum' or 'compound'\")\n",
    "# \n",
    "# def monthly_ic(feature_df: pd.DataFrame,\n",
    "#                etf_list: list,\n",
    "#                pred_prefix: str = \"Predicted_Return_\",\n",
    "#                actual_prefix: str = \"Actual_Return_\",\n",
    "#                agg_method: str = \"compound\",\n",
    "#                shift_horizon: int = 1,\n",
    "#                make_plots: bool = True) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Computes cross-sectional monthly Spearman IC between Stage-1 predictions and FUTURE realized returns.\n",
    "# \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     feature_df : DataFrame\n",
    "#         Must contain 'Date', columns Predicted_Return_{ETF}, Actual_Return_{ETF}.\n",
    "#         Daily frequency (or higher), one row per day.\n",
    "#     etf_list : list[str]\n",
    "#         Universe tickers matching the suffix in column names.\n",
    "#     pred_prefix, actual_prefix : str\n",
    "#         Column prefixes to find predicted and actual returns.\n",
    "#     agg_method : 'compound' or 'sum'\n",
    "#         How to get monthly realized returns from daily.\n",
    "#     shift_horizon : int\n",
    "#         How many months ahead to align realized returns for IC.\n",
    "#         1 = next month (recommended). 0 = same month (leakage risk).\n",
    "#     make_plots : bool\n",
    "#         If True, shows time series + histogram.\n",
    "# \n",
    "#     Returns\n",
    "#     -------\n",
    "#     ic_df : DataFrame with columns [Month, IC, N]\n",
    "#     \"\"\"\n",
    "#     df = feature_df.copy()\n",
    "#     df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "#     if \"Date\" not in df.columns:\n",
    "#         raise ValueError(\"feature_df must contain a 'Date' column\")\n",
    "#     df[\"Month\"] = df[\"Date\"].dt.to_period(\"M\")\n",
    "# \n",
    "#     pred_cols = [f\"{pred_prefix}{e}\" for e in etf_list]\n",
    "#     act_cols  = [f\"{actual_prefix}{e}\" for e in etf_list]\n",
    "#     missing = [c for c in pred_cols + act_cols if c not in df.columns]\n",
    "#     if missing:\n",
    "#         raise ValueError(f\"Missing columns: {missing}\")\n",
    "# \n",
    "#     # (A) Aggregate PREDICTIONS per month cross-sectionally: use the **mean prediction** within month\n",
    "#     #     You could also take the last day of the month if that’s how you trade.\n",
    "#     pred_m = (df.groupby(\"Month\")[pred_cols]\n",
    "#                 .mean()\n",
    "#                 .dropna(how=\"all\"))\n",
    "# \n",
    "#     # (B) Aggregate ACTUAL returns per month per ETF via chosen method\n",
    "#     #     (sum is okay; compound is exact)\n",
    "#     pieces = []\n",
    "#     for m, g in df.groupby(\"Month\"):\n",
    "#         monthly = {}\n",
    "#         for e in etf_list:\n",
    "#             monthly[f\"{actual_prefix}{e}\"] = _monthly_agg(g[f\"{actual_prefix}{e}\"], agg_method)\n",
    "#         s = pd.Series(monthly, name=m)\n",
    "#         pieces.append(s)\n",
    "#     act_m = pd.DataFrame(pieces)\n",
    "#     act_m.index.name = \"Month\"\n",
    "# \n",
    "#     # Align, and **shift realized returns forward** (so month t predictions are evaluated vs month t+1 returns)\n",
    "#     common = pred_m.index.intersection(act_m.index)\n",
    "#     pred_m = pred_m.loc[common]\n",
    "#     act_m = act_m.loc[common]\n",
    "# \n",
    "#     if shift_horizon != 0:\n",
    "#         act_m = act_m.shift(-shift_horizon)\n",
    "# \n",
    "#     # Drop last row if it became NaN after shift\n",
    "#     aligned = pred_m.join(act_m, how=\"inner\")\n",
    "#     aligned = aligned.dropna(how=\"any\")\n",
    "# \n",
    "#     rows = []\n",
    "#     for m, row in aligned.iterrows():\n",
    "#         preds = np.array([row[f\"{pred_prefix}{e}\"] for e in etf_list], dtype=float)\n",
    "#         rets  = np.array([row[f\"{actual_prefix}{e}\"] for e in etf_list], dtype=float)\n",
    "#         mask = np.isfinite(preds) & np.isfinite(rets)\n",
    "#         if mask.sum() >= max(3, int(0.6 * len(etf_list))):  # need enough names\n",
    "#             ic, _ = spearmanr(preds[mask], rets[mask])\n",
    "#             rows.append({\"Month\": m.to_timestamp(), \"IC\": float(ic), \"N\": int(mask.sum())})\n",
    "#         else:\n",
    "#             rows.append({\"Month\": m.to_timestamp(), \"IC\": np.nan, \"N\": int(mask.sum())})\n",
    "# \n",
    "#     ic_df = pd.DataFrame(rows).sort_values(\"Month\")\n",
    "#     if ic_df.empty:\n",
    "#         print(\"No monthly ICs computed (check inputs).\")\n",
    "#         return ic_df\n",
    "# \n",
    "#     mean_ic = ic_df[\"IC\"].mean(skipna=True)\n",
    "#     med_ic  = ic_df[\"IC\"].median(skipna=True)\n",
    "#     pos_frac = (ic_df[\"IC\"] > 0).mean()\n",
    "# \n",
    "#     print(f\"Monthly IC (horizon=+{shift_horizon}): mean={mean_ic:.3f}, median={med_ic:.3f}, \"\n",
    "#           f\"%>0={pos_frac:.1%}, Nmonths={ic_df['IC'].notna().sum()}\")\n",
    "# \n",
    "#     if make_plots:\n",
    "#         plt.figure(figsize=(9, 3))\n",
    "#         plt.plot(ic_df[\"Month\"], ic_df[\"IC\"], lw=1)\n",
    "#         plt.axhline(0, color=\"k\", ls=\"--\", lw=1)\n",
    "#         plt.title(\"Monthly Spearman IC (Pred vs next-month realized)\")\n",
    "#         plt.ylabel(\"IC\")\n",
    "#         plt.xlabel(\"Month\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "# \n",
    "#         plt.figure(figsize=(5, 3))\n",
    "#         ic_df[\"IC\"].hist(bins=21)\n",
    "#         plt.axvline(ic_df[\"IC\"].mean(), color=\"r\", lw=1, label=\"mean\")\n",
    "#         plt.title(\"IC distribution\")\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "# \n",
    "#     return ic_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.535746Z",
     "start_time": "2025-08-12T04:25:52.525744Z"
    }
   },
   "id": "2b0f510c1ff23295",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # etf_list inferred earlier; or define explicitly\n",
    "# data = pd.read_csv(\"stage2_rl_observations_optimized_DIA_ETF.csv\", parse_dates=[\"Date\"])\n",
    "# price_data = pd.read_csv(\"stock_prices_DIA_ETF.csv\")\n",
    "# price_data[\"Date\"] = pd.to_datetime(price_data[\"Date\"], utc=True).dt.tz_localize(None)\n",
    "# price_cols = {col: f\"Price_{col}\" for col in price_data.columns if col != \"Date\"}\n",
    "# price_data.rename(columns=price_cols, inplace=True)\n",
    "# \n",
    "# merged = pd.merge(data, price_data, on=\"Date\", how=\"inner\").reset_index(drop=True)\n",
    "# if len(merged) != len(data):\n",
    "#     print(\"Warning: data length mismatch after merge.\")\n",
    "# \n",
    "# # Universe: infer from Actual_Return_* columns (safer than hand-typing)\n",
    "# etf_list = infer_etf_list(merged)\n",
    "# \n",
    "# # Add stable (return-based) features and optional filters\n",
    "# feature_data = add_stable_features(merged, etf_list)\n",
    "# feature_data = filter_features(feature_data, include_predicted_returns=True, include_shap_metrics=True)\n",
    "# \n",
    "# # Impute/scale FEATURES ONLY\n",
    "# feature_cols = [c for c in feature_data.columns if c != \"Date\" and not c.startswith(\"Actual_Return_\")]\n",
    "# ret_cols = [c for c in feature_data.columns if c.startswith(\"Actual_Return_\")]\n",
    "# \n",
    "# # Forward/backward fill only features\n",
    "# feature_data[feature_cols] = feature_data[feature_cols].ffill().bfill()\n",
    "# # Drop any rows with missing realized returns\n",
    "# feature_data.dropna(subset=ret_cols, inplace=True)\n",
    "# ic_df = monthly_ic(\n",
    "#     feature_df=feature_data,\n",
    "#     etf_list=etf_list,\n",
    "#     pred_prefix=\"Predicted_Return_\",\n",
    "#     actual_prefix=\"Actual_Return_\",\n",
    "#     agg_method=\"compound\",   # or \"sum\"\n",
    "#     shift_horizon=1,         # next-month\n",
    "#     make_plots=True\n",
    "# )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.615765Z",
     "start_time": "2025-08-12T04:25:52.600761Z"
    }
   },
   "id": "63b041aace98f216",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def longshort_quintiles(feature_df, etf_list,\n",
    "#                         pred_prefix=\"Predicted_Return_\", actual_prefix=\"Actual_Return_\",\n",
    "#                         agg_method=\"compound\", shift_horizon=1):\n",
    "#     df = feature_df.copy().sort_values(\"Date\").reset_index(drop=True)\n",
    "#     df[\"Month\"] = df[\"Date\"].dt.to_period(\"M\")\n",
    "# \n",
    "#     # monthly predictions: last day of month is common in practice\n",
    "#     pred_m = df.groupby(\"Month\")[[f\"{pred_prefix}{e}\" for e in etf_list]].last()\n",
    "# \n",
    "#     # monthly realized returns\n",
    "#     pieces = []\n",
    "#     for m, g in df.groupby(\"Month\"):\n",
    "#         monthly = {}\n",
    "#         for e in etf_list:\n",
    "#             monthly[e] = _monthly_agg(g[f\"{actual_prefix}{e}\"], agg_method)\n",
    "#         pieces.append(pd.Series(monthly, name=m))\n",
    "#     act_m = pd.DataFrame(pieces)\n",
    "# \n",
    "#     # shift realized returns to next month\n",
    "#     act_m = act_m.shift(-shift_horizon)\n",
    "# \n",
    "#     aligned = pred_m.join(act_m, how=\"inner\", lsuffix=\"_pred\", rsuffix=\"_ret\").dropna(how=\"any\")\n",
    "#     if aligned.empty:\n",
    "#         print(\"No data after alignment.\")\n",
    "#         return None\n",
    "# \n",
    "#     ls_rets, q1_rets, q5_rets = [], [], []\n",
    "#     for m, row in aligned.iterrows():\n",
    "#         preds = row[[f\"{pred_prefix}{e}\" for e in etf_list]].values\n",
    "#         rets  = row[etf_list].values\n",
    "#         if not (np.isfinite(preds).all() and np.isfinite(rets).all()):\n",
    "#             continue\n",
    "#         ranks = preds.argsort()  # ascending\n",
    "#         q = len(etf_list) // 5\n",
    "#         lo_idx = ranks[:q]\n",
    "#         hi_idx = ranks[-q:]\n",
    "#         q1_rets.append(np.mean(rets[lo_idx]))\n",
    "#         q5_rets.append(np.mean(rets[hi_idx]))\n",
    "#         ls_rets.append(np.mean(rets[hi_idx]) - np.mean(rets[lo_idx]))\n",
    "# \n",
    "#     res = pd.DataFrame({\n",
    "#         \"Month\": [p.to_timestamp() for p in aligned.index],\n",
    "#         \"Q5_Long\": q5_rets,\n",
    "#         \"Q1_Short\": q1_rets,\n",
    "#         \"Q5_minus_Q1\": ls_rets\n",
    "#     })\n",
    "#     res[\"Cum_LS\"] = (1 + res[\"Q5_minus_Q1\"]).cumprod()\n",
    "# \n",
    "#     print(f\"LS mean={res['Q5_minus_Q1'].mean():.4f}, \"\n",
    "#           f\"vol={res['Q5_minus_Q1'].std(ddof=1):.4f}, \"\n",
    "#           f\"Sharpe={res['Q5_minus_Q1'].mean()/res['Q5_minus_Q1'].std(ddof=1)*np.sqrt(12):.2f}\")\n",
    "# \n",
    "#     res.plot(x=\"Month\", y=\"Cum_LS\", title=\"Cumulative L/S (Q5 - Q1)\")\n",
    "#     plt.show()\n",
    "#     return res\n",
    "# \n",
    "# ls_df = longshort_quintiles(feature_data, etf_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.647771Z",
     "start_time": "2025-08-12T04:25:52.632769Z"
    }
   },
   "id": "ccd988c09334b364",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.679778Z",
     "start_time": "2025-08-12T04:25:52.664775Z"
    }
   },
   "id": "10563904fd258e2f",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Monthly Mean-CVaR portfolio environment.\n",
    "\n",
    "    Reward (paid once per month at rebalance boundary):\n",
    "        reward_t = month_ret_t  -  risk_coefficient * CVaR( monthly_ret_history )\n",
    "                     - lambda_turnover * turnover_t\n",
    "                     - lambda_hhi * HHI(weights_t)\n",
    "                     - transaction_cost_rate * turnover_t\n",
    "\n",
    "    Key behaviors:\n",
    "    - Rebalance every `rebalance_period` days using action -> weights mapping\n",
    "      that supports baseline-relative or delta-to-current semantics.\n",
    "    - Between rebalances, weights drift self-financing.\n",
    "    - Observations are a flat vector of the last `lookback_period` days of features.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        etf_list,\n",
    "        reward_type=\"mean_cvar\",\n",
    "        risk_coefficient=0.5,\n",
    "        rebalance_period=21,\n",
    "        lookback_period=21,\n",
    "        weight_bounds=(0.0, 1.0),            # (min_weight, max_weight)\n",
    "        desired_long=1.0,                    # total long budget (e.g., 1.2 for 120/20)\n",
    "        desired_short=0.0,                   # total short budget (e.g., 0.2 for 120/20)\n",
    "        use_baseline=False,\n",
    "        baseline_fn=None,                    # fn(date) -> weight vector\n",
    "        transaction_cost_rate=0.0,\n",
    "        lambda_turnover=0.001,\n",
    "        lambda_hhi=0.1,\n",
    "        # Enhancements / numerics:\n",
    "        alpha=0.05,                          # CVaR tail level\n",
    "        cvar_warmup_months=12,               # months before CVaR kicks in\n",
    "        exclude_price_levels=True,           # drop columns starting with \"Price_\"\n",
    "        action_temperature=1.0,              # scale for tanh squashing\n",
    "        daily_return_clip=None               # e.g., 0.5 to clip daily returns; None to disable\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------- Data / config ----------\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = list(etf_list)\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = float(risk_coefficient)\n",
    "        self.rebalance_period = int(rebalance_period)\n",
    "        self.lookback_period = int(lookback_period)\n",
    "        self.weight_bounds = tuple(weight_bounds)\n",
    "        self.desired_long = float(desired_long)\n",
    "        self.desired_short = float(desired_short)\n",
    "        self.use_baseline = bool(use_baseline)\n",
    "        self.baseline_fn = baseline_fn\n",
    "        self.transaction_cost_rate = float(transaction_cost_rate)\n",
    "        self.lambda_turnover = float(lambda_turnover)\n",
    "        self.lambda_hhi = float(lambda_hhi)\n",
    "        self.alpha = float(alpha)\n",
    "        self.cvar_warmup_months = int(cvar_warmup_months)\n",
    "        self.exclude_price_levels = bool(exclude_price_levels)\n",
    "        self.action_temperature = float(action_temperature)\n",
    "        self.daily_return_clip = daily_return_clip if daily_return_clip is None else float(daily_return_clip)\n",
    "\n",
    "        # ---------- Validate required columns ----------\n",
    "        for etf in self.etf_list:\n",
    "            col = f\"Actual_Return_{etf}\"\n",
    "            if col not in self.data.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}' in data.\")\n",
    "        if \"Date\" not in self.data.columns:\n",
    "            raise ValueError(\"Data must contain a 'Date' column.\")\n",
    "\n",
    "        # ---------- Observation features ----------\n",
    "        # Exclude date & realized returns; optionally exclude raw prices\n",
    "        def keep_feature(c: str) -> bool:\n",
    "            if c == \"Date\" or c.startswith(\"Actual_Return_\"):\n",
    "                return False\n",
    "            if self.exclude_price_levels and c.startswith(\"Price_\"):\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        self.feature_cols = [c for c in self.data.columns if keep_feature(c)]\n",
    "        self.num_features_per_day = len(self.feature_cols)\n",
    "\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(len(self.etf_list),), dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.num_features_per_day * self.lookback_period,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # ---------- State ----------\n",
    "        self.current_step = self.lookback_period\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.ones(len(self.etf_list)) / len(self.etf_list)\n",
    "\n",
    "        # Monthly bookkeeping\n",
    "        self.daily_port_ret_buffer = []   # daily portfolio returns in current month\n",
    "        self.monthly_ret_history = []     # realized monthly returns\n",
    "        self._last_turnover = 0.0\n",
    "\n",
    "    # ------------------------ Helpers ------------------------\n",
    "\n",
    "    def _get_obs(self):\n",
    "        start = self.current_step - self.lookback_period\n",
    "        end = self.current_step\n",
    "        obs_window = self.data.iloc[start:end]\n",
    "        obs_values = obs_window[self.feature_cols].to_numpy(dtype=np.float32).reshape(-1)\n",
    "        if not np.all(np.isfinite(obs_values)):\n",
    "            obs_values = np.nan_to_num(obs_values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return obs_values\n",
    "\n",
    "    def _self_financing_drift(self, returns_today: np.ndarray) -> None:\n",
    "        \"\"\"Update weights passively between rebalances (long/short compatible).\"\"\"\n",
    "        gross = self.current_weights * (1.0 + returns_today)\n",
    "        denom = 1.0 + float(np.dot(self.current_weights, returns_today))\n",
    "        self.current_weights = gross / max(denom, 1e-12)\n",
    "\n",
    "    def _baseline_weights(self, date):\n",
    "        \"\"\"Fetch and sanitize baseline weights (non-negative & normalized).\"\"\"\n",
    "        if self.baseline_fn is None:\n",
    "            return np.ones(len(self.etf_list)) / len(self.etf_list)\n",
    "        w = np.asarray(self.baseline_fn(date), dtype=float)\n",
    "        if not np.all(np.isfinite(w)) or w.shape[0] != len(self.etf_list):\n",
    "            w = np.ones(len(self.etf_list)) / len(self.etf_list)\n",
    "        # For safety, clip negatives and renormalize baseline\n",
    "        w = np.clip(w, 0.0, None)\n",
    "        s = w.sum()\n",
    "        return w / s if s > 0 else np.ones(len(self.etf_list)) / len(self.etf_list)\n",
    "\n",
    "    def _map_action_to_weights(self, action: np.ndarray, date) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Map agent action to portfolio weights with long/short budgets and bounds.\n",
    "        - Supports baseline-relative (if self.use_baseline) or delta-to-current.\n",
    "        - Enforces desired long/short budgets after clipping to weight_bounds.\n",
    "        \"\"\"\n",
    "        a = np.asarray(action, dtype=float)\n",
    "        if not np.all(np.isfinite(a)):\n",
    "            a = np.zeros_like(self.current_weights)\n",
    "\n",
    "        # Smoothly squash actions to (-1, 1)\n",
    "        temp = max(self.action_temperature, 1e-6)\n",
    "        a = np.tanh(a / temp)\n",
    "\n",
    "        if self.use_baseline:\n",
    "            base = self._baseline_weights(date)\n",
    "            raw = base * (1.0 + a)            # relative tilt around baseline\n",
    "        else:\n",
    "            raw = self.current_weights + a     # delta to current weights\n",
    "\n",
    "        # Split long/short parts\n",
    "        long_w = np.clip(raw, 0.0, None)\n",
    "        short_w = np.clip(-raw, 0.0, None)\n",
    "\n",
    "        has_long = long_w.sum() > 0\n",
    "        has_short = short_w.sum() > 0\n",
    "        n = len(raw)\n",
    "\n",
    "        if has_long and has_short:\n",
    "            norm_long = self.desired_long * long_w / long_w.sum()\n",
    "            norm_short = self.desired_short * short_w / short_w.sum()\n",
    "        elif has_long and not has_short:\n",
    "            norm_long = long_w / long_w.sum()         # 100% long\n",
    "            norm_short = np.zeros(n)\n",
    "        elif not has_long and has_short:\n",
    "            # Fallback: equal-weight long-only (avoid pathological all-short)\n",
    "            norm_long = np.ones(n) / n\n",
    "            norm_short = np.zeros(n)\n",
    "        else:\n",
    "            # All zeros -> equal-weight long-only\n",
    "            norm_long = np.ones(n) / n\n",
    "            norm_short = np.zeros(n)\n",
    "\n",
    "        combined = norm_long - norm_short\n",
    "\n",
    "        # Clip to hard bounds, then renormalize long/short budgets\n",
    "        lo, hi = self.weight_bounds\n",
    "        clipped = np.clip(combined, lo, hi)\n",
    "\n",
    "        long_c = np.clip(clipped, 0.0, None)\n",
    "        short_c = np.clip(-clipped, 0.0, None)\n",
    "\n",
    "        if long_c.sum() > 0 and short_c.sum() > 0:\n",
    "            final_long = self.desired_long * long_c / long_c.sum()\n",
    "            final_short = self.desired_short * short_c / short_c.sum()\n",
    "        elif long_c.sum() > 0:\n",
    "            final_long = long_c / long_c.sum()\n",
    "            final_short = np.zeros(n)\n",
    "        else:\n",
    "            final_long = np.ones(n) / n\n",
    "            final_short = np.zeros(n)\n",
    "\n",
    "        return final_long - final_short\n",
    "\n",
    "    # ------------------------ Gym API ------------------------\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.ones(len(self.etf_list)) / len(self.etf_list)\n",
    "        self.daily_port_ret_buffer = []\n",
    "        self.monthly_ret_history = []\n",
    "        self._last_turnover = 0.0\n",
    "        obs = self._get_obs()\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        next_step = self.current_step + 1\n",
    "        reward = 0.0\n",
    "        terminated = False\n",
    "\n",
    "        # 1) Rebalance day: map action -> new weights, compute turnover\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            prev_w = self.current_weights.copy()\n",
    "            current_date = self.data.loc[self.current_step, \"Date\"]\n",
    "            new_w = self._map_action_to_weights(action, current_date)\n",
    "            # L1 turnover\n",
    "            self._last_turnover = float(np.sum(np.abs(new_w - prev_w)))\n",
    "            self.current_weights = new_w\n",
    "\n",
    "        # 2) Realize today's return (t -> t+1)\n",
    "        if next_step >= len(self.data):\n",
    "            # No next day to realize\n",
    "            terminated = True\n",
    "        else:\n",
    "            asset_returns = np.array(\n",
    "                [self.data.loc[next_step, f\"Actual_Return_{etf}\"] for etf in self.etf_list],\n",
    "                dtype=float\n",
    "            )\n",
    "            # Optional daily return clipping (rarely needed if features are clean)\n",
    "            port_ret = float(np.dot(self.current_weights, asset_returns))\n",
    "            if self.daily_return_clip is not None:\n",
    "                port_ret = float(np.clip(port_ret, -self.daily_return_clip, self.daily_return_clip))\n",
    "            if not np.isfinite(port_ret):\n",
    "                port_ret = 0.0\n",
    "\n",
    "            self.cumulative_wealth *= (1.0 + port_ret)\n",
    "            self.daily_port_ret_buffer.append(port_ret)\n",
    "\n",
    "            # Passive drift on non-rebalance days\n",
    "            if self.current_step % self.rebalance_period != 0:\n",
    "                self._self_financing_drift(asset_returns)\n",
    "\n",
    "            # 3) Pay monthly reward at boundary (or at the end)\n",
    "            end_of_month = (next_step % self.rebalance_period == 0)\n",
    "            last_step = (next_step == len(self.data) - 1)\n",
    "            if end_of_month or last_step:\n",
    "                month_ret = np.prod([1.0 + r for r in self.daily_port_ret_buffer]) - 1.0\n",
    "                self.daily_port_ret_buffer = []\n",
    "\n",
    "                # CVaR on time-series of monthly returns (excluding current month)\n",
    "                cvar_loss = 0.0\n",
    "                if len(self.monthly_ret_history) >= self.cvar_warmup_months:\n",
    "                    arr = np.array(self.monthly_ret_history, dtype=float)\n",
    "                    var = np.percentile(arr, 100.0 * self.alpha)\n",
    "                    tail = arr[arr <= var]\n",
    "                    if tail.size > 0:\n",
    "                        cvar_loss = -float(np.mean(tail))  # positive loss\n",
    "\n",
    "                # Base reward\n",
    "                if self.reward_type == \"mean_cvar\":\n",
    "                    base_reward = month_ret - self.risk_coefficient * cvar_loss\n",
    "                elif self.reward_type == \"cumulative_return\":\n",
    "                    base_reward = month_ret\n",
    "                elif self.reward_type == \"log_wealth\":\n",
    "                    base_reward = float(np.log(max(1.0 + month_ret, 1e-12)))\n",
    "                elif self.reward_type == \"mean_var\":\n",
    "                    # Not recommended here (monthly VAR of daily cross-sec isn't meaningful)\n",
    "                    base_reward = month_ret\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid reward type: {self.reward_type}\")\n",
    "\n",
    "                # Penalties and costs (once per month)\n",
    "                hhi = float(np.sum(np.square(np.abs(self.current_weights))))\n",
    "                penalty = self.lambda_hhi * hhi + self.lambda_turnover * self._last_turnover\n",
    "                costs = self.transaction_cost_rate * self._last_turnover\n",
    "                reward = base_reward - penalty - costs\n",
    "\n",
    "                # Update monthly history AFTER computing reward\n",
    "                self.monthly_ret_history.append(month_ret)\n",
    "\n",
    "            terminated = next_step >= (len(self.data) - 1)\n",
    "\n",
    "        self.current_step += 1\n",
    "        obs = self._get_obs()\n",
    "        if not np.all(np.isfinite(obs)):\n",
    "            obs = np.nan_to_num(obs, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        info = {\n",
    "            \"date\": self.data.loc[min(self.current_step, len(self.data)-1), \"Date\"],\n",
    "            \"weights\": self.current_weights.copy(),\n",
    "            \"cumulative_wealth\": float(self.cumulative_wealth),\n",
    "            \"last_turnover\": float(self._last_turnover),\n",
    "            \"num_months\": len(self.monthly_ret_history),\n",
    "        }\n",
    "        return obs, float(reward), bool(terminated), False, info\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:52.886825Z",
     "start_time": "2025-08-12T04:25:52.698783Z"
    }
   },
   "id": "637f2b485e42e5a6",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Stage 2 — Upgraded driver\n",
    "# ===============================\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_1samp\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "# Add once near imports\n",
    "# ===== ADD: Monthly macro-step wrapper =====\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class MonthlyMacroWrapper(gym.Wrapper):\n",
    "    \"\"\"Each .step() advances a full month internally.\n",
    "    The agent acts on the rebalance day; wrapper forwards zeros for the other days.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, rebalance_period: int):\n",
    "        super().__init__(env)\n",
    "        self.rebalance_period = int(rebalance_period)\n",
    "        self._n = env.action_space.shape[0]\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, trunc, info = self.env.step(action)  # rebalance day\n",
    "        for _ in range(self.rebalance_period - 1):\n",
    "            if done:\n",
    "                break\n",
    "            zeros = np.zeros(self._n, dtype=np.float32)\n",
    "            obs, r, done, trunc, info = self.env.step(zeros)\n",
    "            reward += r  # monthly reward is non-zero on the last day\n",
    "        return obs, reward, done, trunc, info\n",
    "\n",
    "\n",
    "# ===== ADD: VecNormalize in-memory cloning (optional speed-up) =====\n",
    "from copy import deepcopy\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "def copy_vecnorm_stats(src, dst, use_vecnormalize: bool):\n",
    "    \"\"\"Copy normalization stats from src VecNormalize to dst (avoid disk I/O).\"\"\"\n",
    "    if not use_vecnormalize:\n",
    "        return\n",
    "    if not (isinstance(src, VecNormalize) and isinstance(dst, VecNormalize)):\n",
    "        return\n",
    "    dst.obs_rms = deepcopy(src.obs_rms)\n",
    "    dst.ret_rms = deepcopy(src.ret_rms)\n",
    "    dst.clip_obs = src.clip_obs\n",
    "    dst.clip_reward = src.clip_reward\n",
    "    dst.gamma = src.gamma\n",
    "    dst.training = False  # eval mode\n",
    "\n",
    "\n",
    "# ===== ADD: Tilt baseline built from Stage-1 predicted returns =====\n",
    "def make_tilt_baseline_fn(df: pd.DataFrame,\n",
    "                          etf_list: List[str],\n",
    "                          k_base: float = 10.0,            # softmax strength\n",
    "                          vol_col_prefix: str = \"Vol_20_\",  # uses your stable features\n",
    "                          use_inv_vol: bool = True,\n",
    "                          ema_halflife_months: float = 3.0, # smoothing\n",
    "                          floor: float = 0.02,              # min weight\n",
    "                          cap: float = 0.22,                # max weight\n",
    "                          dispersion_ref: float = 0.8) -> callable:\n",
    "    \"\"\"\n",
    "    Returns baseline_fn(date) -> weights aligned to Date in df.\n",
    "    Weights reflect cross-sectional tilts of Predicted_Return_* (optionally / vol),\n",
    "    softmaxed to probabilities, smoothed via EMA, then clipped to [floor, cap].\n",
    "    \"\"\"\n",
    "    pred_cols = [f\"Predicted_Return_{e}\" for e in etf_list]\n",
    "    vol_cols  = [f\"{vol_col_prefix}{e}\"  for e in etf_list]\n",
    "\n",
    "    dates = df[\"Date\"].to_list()\n",
    "    pred_mat = df[pred_cols].values\n",
    "    if use_inv_vol:\n",
    "        vol_mat = np.clip(df[vol_cols].values, 1e-6, None)\n",
    "    else:\n",
    "        vol_mat = np.ones_like(pred_mat)\n",
    "\n",
    "    # per-asset floor/cap projection to sum to 1\n",
    "    def _project_floor_cap(w, lo=floor, hi=cap):\n",
    "        w = np.clip(w, lo, hi)\n",
    "        s = w.sum()\n",
    "        if s <= 0:\n",
    "            w = np.ones_like(w) / len(w)\n",
    "        else:\n",
    "            w = w / s\n",
    "        return w\n",
    "\n",
    "    # EMA smoothing coefficient per month\n",
    "    lam = 0.0\n",
    "    if ema_halflife_months and ema_halflife_months > 0:\n",
    "        lam = 1 - 0.5 ** (1.0 / ema_halflife_months)\n",
    "\n",
    "    baseline_cache = {}\n",
    "    prev_w = np.ones(len(etf_list)) / len(etf_list)\n",
    "\n",
    "    for i, d in enumerate(dates):\n",
    "        x = pred_mat[i] / vol_mat[i]  # signal scaled by risk\n",
    "        m, s = np.nanmean(x), np.nanstd(x)\n",
    "        z = (x - m) / (s if s > 1e-8 else 1.0)\n",
    "        z = np.nan_to_num(z, nan=0.0)\n",
    "\n",
    "        # adapt softmax strength to signal dispersion\n",
    "        disp = float(np.nanstd(z))\n",
    "        k = k_base * np.clip(disp / dispersion_ref, 0.5, 2.0)\n",
    "\n",
    "        raw = np.exp(k * z)\n",
    "        w0 = raw / raw.sum() if np.isfinite(raw).all() and raw.sum() > 0 else np.ones_like(raw)/len(raw)\n",
    "        w0 = _project_floor_cap(w0)\n",
    "\n",
    "        w  = (1 - lam) * prev_w + lam * w0\n",
    "        w  = _project_floor_cap(w)\n",
    "        baseline_cache[d] = w\n",
    "        prev_w = w\n",
    "\n",
    "    def baseline_fn(date):\n",
    "        return baseline_cache.get(date, np.ones(len(etf_list))/len(etf_list))\n",
    "\n",
    "    return baseline_fn\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # Windows\n",
    "    train_window_days: int = 252 * 7\n",
    "    validation_window_days: int = 252\n",
    "    prediction_window_days: int = 252\n",
    "\n",
    "    # Observation & action cadence\n",
    "    lookback_period: int = 21\n",
    "    rebalance_period: int = 21\n",
    "    macro_step: bool = True        # << monthly macro-steps\n",
    "\n",
    "    # PPO budgets (monthly steps)\n",
    "    n_iter_tuning: int = 8\n",
    "    tuning_timesteps: int = 1_500\n",
    "    incremental_timesteps: int = 2_000\n",
    "    max_timesteps: int = 12_000\n",
    "    patience: int = 2\n",
    "\n",
    "    # Network\n",
    "    policy_arch: Tuple[int, int] = (128, 128)\n",
    "    n_epochs: int = 5\n",
    "    device: str = \"auto\"           # \"cuda\" | \"cpu\" | \"auto\"\n",
    "\n",
    "    # Iterations/seeds\n",
    "    num_iterations: int = 30\n",
    "    base_seed: int = 42\n",
    "\n",
    "    # Env economics (slightly looser for more variation)\n",
    "    default_risk_coeff: float = 0.5\n",
    "    desired_long: float = 1.0\n",
    "    desired_short: float = 0.0\n",
    "    weight_bounds: Tuple[float, float] = (0.0  , 1.0)\n",
    "    lambda_hhi: float = 0.04        # ↓ from 0.1\n",
    "    lambda_turnover: float = 0.003  # ↓ from 0.005\n",
    "    transaction_cost_rate: float = 0.0005\n",
    "    alpha: float = 0.05\n",
    "    cvar_warmup_months: int = 12\n",
    "    action_temperature: float = 0.4 # ↓ from 0.6: clearer tilts\n",
    "    exclude_price_levels: bool = True\n",
    "\n",
    "    # VecNormalize\n",
    "    use_vecnormalize: bool = True\n",
    "    vecnorm_clip_obs: float = 10.0\n",
    "\n",
    "    # Model reuse\n",
    "    warm_start_across_windows: bool = True\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def set_global_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def add_stable_features(df: pd.DataFrame, etf_list: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Return-based features only; raw Price_* levels are later excluded in env.\"\"\"\n",
    "    data = df.copy()\n",
    "    for etf in etf_list:\n",
    "        price_col = f\"Price_{etf}\"\n",
    "        if price_col not in data.columns:\n",
    "            raise ValueError(f\"Missing {price_col} in price_data merge.\")\n",
    "        r = data[price_col].pct_change()\n",
    "        data[f\"Vol_20_{etf}\"] = r.rolling(20).std()\n",
    "        data[f\"Mom_5_{etf}\"]  = r.rolling(5).mean()\n",
    "        data[f\"Mom_10_{etf}\"] = r.rolling(10).mean()\n",
    "        data[f\"Mom_20_{etf}\"] = r.rolling(20).mean()\n",
    "        # You can add a rolling market beta here if you want\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "def filter_features(df: pd.DataFrame,\n",
    "                    include_predicted_returns: bool = True,\n",
    "                    include_shap_metrics: bool = True) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if not include_predicted_returns:\n",
    "        cols = [c for c in df.columns if \"Predicted_Return_\" in c]\n",
    "        df.drop(columns=cols, inplace=True)\n",
    "    if not include_shap_metrics:\n",
    "        cols = [c for c in df.columns if \"SHAP\" in c]\n",
    "        df.drop(columns=cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "def infer_etf_list(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Infer ETF universe from Actual_Return_* columns.\"\"\"\n",
    "    etfs = sorted([c.replace(\"Actual_Return_\", \"\") for c in df.columns if c.startswith(\"Actual_Return_\")])\n",
    "    if not etfs:\n",
    "        raise ValueError(\"No Actual_Return_* columns found to infer ETF list.\")\n",
    "    return etfs\n",
    "\n",
    "# -------------------------\n",
    "# Env builders (train/val/pred)\n",
    "# -------------------------\n",
    "def make_env_from_frame(df: pd.DataFrame,\n",
    "                        etf_list: List[str],\n",
    "                        cfg: TrainingConfig,\n",
    "                        risk_coeff: float,\n",
    "                        lambda_turnover: float,\n",
    "                        lambda_hhi: float,\n",
    "                        use_baseline: bool,\n",
    "                        vecnorm_load_path: Optional[str] = None,\n",
    "                        training: bool = True):\n",
    "    \"\"\"Build a (VecNormalized) env from a DataFrame slice.\"\"\"\n",
    "    def _ctor():\n",
    "        baseline_fn = (make_tilt_baseline_fn(df, etf_list,\n",
    "                                             k_base=10.0,\n",
    "                                             ema_halflife_months=3.0,\n",
    "                                             floor=0.02, cap=0.22)\n",
    "                       if use_baseline else None)\n",
    "\n",
    "        env = PortfolioEnv(\n",
    "            df, etf_list,\n",
    "            reward_type=\"mean_cvar\",\n",
    "            risk_coefficient=risk_coeff,\n",
    "            rebalance_period=cfg.rebalance_period,\n",
    "            lookback_period=cfg.lookback_period,\n",
    "            weight_bounds=cfg.weight_bounds,\n",
    "            desired_long=cfg.desired_long,\n",
    "            desired_short=cfg.desired_short,\n",
    "            use_baseline=use_baseline,\n",
    "            baseline_fn=baseline_fn,\n",
    "            transaction_cost_rate=cfg.transaction_cost_rate,\n",
    "            lambda_turnover=lambda_turnover,\n",
    "            lambda_hhi=lambda_hhi,\n",
    "            alpha=cfg.alpha,\n",
    "            cvar_warmup_months=cfg.cvar_warmup_months,\n",
    "            exclude_price_levels=cfg.exclude_price_levels,\n",
    "            action_temperature=cfg.action_temperature,\n",
    "        )\n",
    "        if cfg.macro_step:\n",
    "            env = MonthlyMacroWrapper(env, cfg.rebalance_period)\n",
    "        return env\n",
    "\n",
    "    venv = make_vec_env(_ctor, n_envs=1)\n",
    "\n",
    "    if cfg.use_vecnormalize:\n",
    "        if vecnorm_load_path is None:\n",
    "            venv = VecNormalize(venv, norm_obs=True, norm_reward=False, clip_obs=cfg.vecnorm_clip_obs)\n",
    "        else:\n",
    "            venv = VecNormalize.load(vecnorm_load_path, venv)\n",
    "        venv.training = training\n",
    "\n",
    "    return venv\n",
    "\n",
    "\n",
    "def save_vecnorm_if_needed(venv, path: str, cfg: TrainingConfig):\n",
    "    if cfg.use_vecnormalize:\n",
    "        venv.save(path)\n",
    "\n",
    "# -------------------------\n",
    "# Tuning (smarter ranges)\n",
    "# -------------------------\n",
    "def sample_valid_params(cfg: TrainingConfig, random_seed: int, n: int):\n",
    "    # Monthly steps: smaller, faster\n",
    "    space = {\n",
    "        \"learning_rate\": [3e-4, 2e-4, 1e-4],\n",
    "        \"n_steps\": [64, 96, 128],\n",
    "        \"batch_size\": [32, 64],\n",
    "        \"gamma\": [0.96, 0.985, 0.995],\n",
    "        \"gae_lambda\": [0.90, 0.95, 0.98],\n",
    "        \"ent_coef\": [0.002, 0.005, 0.01],\n",
    "        \"clip_range\": [0.1, 0.2],\n",
    "        \"vf_coef\": [0.5, 0.7, 1.0],\n",
    "        \"max_grad_norm\": [0.3, 0.5],\n",
    "        # env economics\n",
    "        \"risk_coefficient\": [0.01, 0.025, 0.05, 0.1],\n",
    "        # \"risk_coefficient\": [0.1, 0.25, 0.5, 1.0],\n",
    "        \"lambda_turnover\": [0.001, 0.003, 0.005, 0.01],\n",
    "        \"lambda_hhi\": [0.03, 0.04, 0.05, 0.1],\n",
    "        \"seed\": [random_seed, random_seed+11, random_seed+23],\n",
    "    }\n",
    "    raw = list(ParameterSampler(space, n_iter=n, random_state=random_seed))\n",
    "    return [p for p in raw if (p[\"n_steps\"] % p[\"batch_size\"] == 0)] or raw\n",
    "\n",
    "\n",
    "def eval_monthly_metrics(env, model, etf_list):\n",
    "    \"\"\"Evaluate over episode; collect turnover, deviation from EW, eff. N.\"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    N = len(etf_list)\n",
    "    w_eq = np.ones(N) / N\n",
    "\n",
    "    turnovers, devs, effns = [], [], []\n",
    "    last_num_months = 0\n",
    "\n",
    "    while True:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, dones, infos = env.step(action)\n",
    "\n",
    "        r = float(rewards[0]); info = infos[0]\n",
    "        total_reward += r\n",
    "        w = np.asarray(info.get(\"weights\", np.ones(N)/N), dtype=float)\n",
    "        if w.shape[0] != N or not np.all(np.isfinite(w)): w = np.ones(N)/N\n",
    "\n",
    "        num_months = int(info.get(\"num_months\", last_num_months))\n",
    "        boundary = (abs(r) > 1e-12) or (num_months > last_num_months)\n",
    "        if boundary:\n",
    "            hhi = float(np.sum(np.square(w)))\n",
    "            turnovers.append(float(info.get(\"last_turnover\", 0.0)))\n",
    "            devs.append(float(np.sum(np.abs(w - w_eq))))\n",
    "            effns.append(1.0 / hhi if hhi > 1e-12 else np.nan)\n",
    "            last_num_months = num_months\n",
    "\n",
    "        if dones[0]:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"total_reward\": total_reward,\n",
    "        \"avg_turnover\": float(np.mean(turnovers)) if turnovers else 0.0,\n",
    "        \"avg_dev_from_ew\": float(np.mean(devs)) if devs else 0.0,\n",
    "        \"avg_eff_n\": float(np.nanmean(effns)) if effns else np.nan,\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_and_tune(train_df: pd.DataFrame, val_df: pd.DataFrame,\n",
    "                      etf_list: List[str], cfg: TrainingConfig,\n",
    "                      random_seed: int) -> Dict[str, float]:\n",
    "\n",
    "    candidates = sample_valid_params(cfg, random_seed, cfg.n_iter_tuning)\n",
    "\n",
    "    # Encourage variation around EW but not too wild\n",
    "    DEV_EW_MIN, DEV_EW_MAX = 0.25, 0.70\n",
    "    TURNOVER_MAX = 0.60\n",
    "\n",
    "    best_feasible = None   # (score, params, metrics)\n",
    "    best_overall  = None\n",
    "\n",
    "    for params in candidates:\n",
    "        seed = params[\"seed\"]\n",
    "        risk_coeff = params[\"risk_coefficient\"]\n",
    "        lambda_turnover = params[\"lambda_turnover\"]\n",
    "        lambda_hhi = params[\"lambda_hhi\"]\n",
    "\n",
    "        set_global_seed(seed)\n",
    "\n",
    "        # Train env\n",
    "        env_train = make_env_from_frame(train_df, etf_list, cfg, risk_coeff,\n",
    "                                        lambda_turnover, lambda_hhi,\n",
    "                                        use_baseline=True, vecnorm_load_path=None, training=True)\n",
    "        policy_kwargs = dict(net_arch=dict(pi=list(cfg.policy_arch), vf=list(cfg.policy_arch)))\n",
    "        model = PPO(\n",
    "            \"MlpPolicy\", env_train,\n",
    "            device=cfg.device,\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            n_steps=params[\"n_steps\"],\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            gamma=params[\"gamma\"],\n",
    "            gae_lambda=params[\"gae_lambda\"],\n",
    "            ent_coef=params[\"ent_coef\"],\n",
    "            clip_range=params[\"clip_range\"],\n",
    "            vf_coef=params[\"vf_coef\"],\n",
    "            max_grad_norm=params[\"max_grad_norm\"],\n",
    "            n_epochs=cfg.n_epochs,\n",
    "            seed=seed,\n",
    "            verbose=0,\n",
    "            policy_kwargs=policy_kwargs\n",
    "        )\n",
    "        model.learn(total_timesteps=cfg.tuning_timesteps)\n",
    "\n",
    "        # Val env (reuse stats in-memory)\n",
    "        env_val = make_env_from_frame(val_df, etf_list, cfg, risk_coeff,\n",
    "                                      lambda_turnover, lambda_hhi,\n",
    "                                      use_baseline=True, vecnorm_load_path=None, training=False)\n",
    "        copy_vecnorm_stats(env_train, env_val, cfg.use_vecnormalize)\n",
    "\n",
    "        metrics = eval_monthly_metrics(env_val, model, etf_list)\n",
    "        score = metrics[\"total_reward\"]\n",
    "\n",
    "        # Track best overall\n",
    "        if (best_overall is None) or (score > best_overall[0]):\n",
    "            best_overall = (score, params.copy(), metrics)\n",
    "\n",
    "        # Feasible selection\n",
    "        feasible = (DEV_EW_MIN <= metrics[\"avg_dev_from_ew\"] <= DEV_EW_MAX) and (metrics[\"avg_turnover\"] <= TURNOVER_MAX)\n",
    "        if feasible and ((best_feasible is None) or (score > best_feasible[0])):\n",
    "            best_feasible = (score, params.copy(), metrics)\n",
    "\n",
    "    chosen = best_feasible if best_feasible is not None else best_overall\n",
    "    assert chosen is not None, \"No candidates evaluated.\"\n",
    "    score, best_params, metrics = chosen\n",
    "\n",
    "    print(\"\\nTuning selection:\")\n",
    "    print(f\"  score={score:.4f}\")\n",
    "    print(f\"  λ_turnover={best_params['lambda_turnover']}, λ_hhi={best_params['lambda_hhi']}\")\n",
    "    print(f\"  dev(EW)={metrics['avg_dev_from_ew']:.3f}, turnover={metrics['avg_turnover']:.3f}, effN={metrics['avg_eff_n']:.2f}\")\n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Train with early stopping\n",
    "# -------------------------\n",
    "def train_with_early_stopping(train_df, val_df, etf_list, cfg, best_params, window_dir) -> Tuple[str, str]:\n",
    "    seed = best_params[\"seed\"]\n",
    "    risk_coeff = best_params[\"risk_coefficient\"]\n",
    "    lambda_turnover = best_params[\"lambda_turnover\"]\n",
    "    lambda_hhi = best_params[\"lambda_hhi\"]\n",
    "    set_global_seed(seed)\n",
    "\n",
    "    env_train = make_env_from_frame(train_df, etf_list, cfg, risk_coeff,\n",
    "                                    lambda_turnover, lambda_hhi,\n",
    "                                    use_baseline=True, vecnorm_load_path=None, training=True)\n",
    "\n",
    "    policy_kwargs = dict(net_arch=dict(pi=list(cfg.policy_arch), vf=list(cfg.policy_arch)))\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\", env_train,\n",
    "        device=cfg.device,\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        n_steps=best_params[\"n_steps\"],\n",
    "        batch_size=best_params[\"batch_size\"],\n",
    "        gamma=best_params[\"gamma\"],\n",
    "        gae_lambda=best_params[\"gae_lambda\"],\n",
    "        ent_coef=best_params[\"ent_coef\"],\n",
    "        clip_range=best_params[\"clip_range\"],\n",
    "        vf_coef=best_params[\"vf_coef\"],\n",
    "        max_grad_norm=best_params[\"max_grad_norm\"],\n",
    "        n_epochs=cfg.n_epochs,\n",
    "        seed=seed,\n",
    "        verbose=0,\n",
    "        policy_kwargs=policy_kwargs\n",
    "    )\n",
    "\n",
    "    # Build a single val env, copy stats each eval\n",
    "    env_val = make_env_from_frame(val_df, etf_list, cfg, risk_coeff,\n",
    "                                  lambda_turnover, lambda_hhi,\n",
    "                                  use_baseline=True, vecnorm_load_path=None, training=False)\n",
    "\n",
    "    model_path = os.path.join(window_dir, \"best_ppo.zip\")\n",
    "    vecnorm_path = os.path.join(window_dir, \"vecnormalize.pkl\")  # persist for prediction if you prefer disk\n",
    "\n",
    "    best_val_reward, no_improve = -np.inf, 0\n",
    "    log_rows = []\n",
    "\n",
    "    for step in range(0, cfg.max_timesteps, cfg.incremental_timesteps):\n",
    "        model.learn(total_timesteps=cfg.incremental_timesteps)\n",
    "\n",
    "        copy_vecnorm_stats(env_train, env_val, cfg.use_vecnormalize)\n",
    "        obs = env_val.reset()\n",
    "        val_reward = 0.0\n",
    "        while True:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, rewards, dones, infos = env_val.step(action)\n",
    "            val_reward += float(rewards[0])\n",
    "            if dones[0]:\n",
    "                break\n",
    "\n",
    "        log_rows.append({\"training_step\": step + cfg.incremental_timesteps,\n",
    "                         \"validation_reward\": val_reward})\n",
    "\n",
    "        if val_reward > best_val_reward:\n",
    "            best_val_reward = val_reward\n",
    "            no_improve = 0\n",
    "            model.save(model_path)\n",
    "            # If you prefer to keep VecNormalize on disk for pred:\n",
    "            if cfg.use_vecnormalize:\n",
    "                env_train.save(vecnorm_path)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= cfg.patience:\n",
    "                break\n",
    "\n",
    "    pd.DataFrame(log_rows).to_csv(os.path.join(window_dir, \"training_validation_log.csv\"), index=False)\n",
    "    return model_path, vecnorm_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Predict on pred_df & compute OOS wealth\n",
    "# -------------------------\n",
    "def predict_and_log(pred_df, etf_list, cfg, best_params, model_path, vecnorm_path, window_dir):\n",
    "    seed = best_params[\"seed\"]\n",
    "    risk_coeff = best_params[\"risk_coefficient\"]\n",
    "    lambda_turnover = best_params[\"lambda_turnover\"]\n",
    "    lambda_hhi = best_params[\"lambda_hhi\"]\n",
    "    set_global_seed(seed)\n",
    "\n",
    "    # Build pred env; if you persisted VecNormalize, load it here:\n",
    "    if cfg.use_vecnormalize and os.path.exists(vecnorm_path):\n",
    "        env_pred = make_env_from_frame(pred_df, etf_list, cfg, risk_coeff,\n",
    "                                       lambda_turnover, lambda_hhi,\n",
    "                                       use_baseline=True, vecnorm_load_path=vecnorm_path, training=False)\n",
    "    else:\n",
    "        env_pred = make_env_from_frame(pred_df, etf_list, cfg, risk_coeff,\n",
    "                                       lambda_turnover, lambda_hhi,\n",
    "                                       use_baseline=True, vecnorm_load_path=None, training=False)\n",
    "\n",
    "    model = PPO.load(model_path, device=cfg.device)\n",
    "    obs = env_pred.reset()\n",
    "\n",
    "    dates, weights, wealth_series = [], [], []\n",
    "    last_num_months = 0\n",
    "\n",
    "    while True:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, dones, infos = env_pred.step(action)\n",
    "\n",
    "        r = float(rewards[0]); inf = infos[0]\n",
    "        num_months = inf.get(\"num_months\", last_num_months)\n",
    "        boundary = (abs(r) > 1e-12) or (num_months > last_num_months)\n",
    "        if boundary:\n",
    "            dates.append(inf[\"date\"])\n",
    "            weights.append(inf[\"weights\"])\n",
    "            wealth_series.append(inf[\"cumulative_wealth\"])\n",
    "            last_num_months = num_months\n",
    "\n",
    "        if dones[0]:\n",
    "            break\n",
    "\n",
    "    weights_df = pd.DataFrame(weights, columns=etf_list); weights_df.insert(0, \"Date\", dates)\n",
    "    weights_df.to_csv(os.path.join(window_dir, \"weights.csv\"), index=False)\n",
    "\n",
    "    wealth_df = pd.DataFrame({\"Date\": dates, \"Cumulative_Wealth\": wealth_series})\n",
    "    wealth_df.to_csv(os.path.join(window_dir, \"wealth.csv\"), index=False)\n",
    "\n",
    "    wealth_arr = np.array(wealth_series, dtype=float)\n",
    "    month_rets = wealth_arr[1:] / wealth_arr[:-1] - 1.0 if len(wealth_arr) > 1 else np.array([])\n",
    "    if month_rets.size > 0:\n",
    "        mean = month_rets.mean()\n",
    "        vol = month_rets.std(ddof=1)\n",
    "        sharpe = mean / vol * np.sqrt(12) if vol > 0 else np.nan\n",
    "        cum = (1.0 + month_rets).cumprod()\n",
    "        peak = np.maximum.accumulate(cum)\n",
    "        mdd = np.min(cum / peak - 1.0)\n",
    "    else:\n",
    "        mean = vol = sharpe = mdd = np.nan\n",
    "\n",
    "    return float(wealth_arr[-1] - 1.0 if wealth_arr.size else 0.0), float(sharpe), float(mdd)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-12T04:25:58.512182Z",
     "start_time": "2025-08-12T04:25:52.887826Z"
    }
   },
   "id": "3c770edf2acf6b5a",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_26588\\988225617.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Mom_10_{etf}\"] = r.rolling(10).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_26588\\988225617.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Mom_20_{etf}\"] = r.rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_26588\\988225617.py:205: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Vol_20_{etf}\"] = r.rolling(20).std()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_26588\\988225617.py:206: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Mom_5_{etf}\"]  = r.rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_26588\\988225617.py:207: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Mom_10_{etf}\"] = r.rolling(10).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_26588\\988225617.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Mom_20_{etf}\"] = r.rolling(20).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Iteration 1/30 (seed=42) ==\n",
      "\n",
      "Tuning selection:\n",
      "  score=0.0808\n",
      "  λ_turnover=0.005, λ_hhi=0.04\n",
      "  dev(EW)=0.471, turnover=0.416, effN=16.37\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Load & prepare data\n",
    "# -------------------------\n",
    "cfg = TrainingConfig()\n",
    "\n",
    "# Load your prepared Stage‑2 dataset and price data\n",
    "data = pd.read_csv(\"stage2_rl_observations_optimized_DIA_ETF.csv\", parse_dates=[\"Date\"])\n",
    "price_data = pd.read_csv(\"stock_prices_DIA_ETF.csv\")\n",
    "price_data[\"Date\"] = pd.to_datetime(price_data[\"Date\"], utc=True).dt.tz_localize(None)\n",
    "price_cols = {col: f\"Price_{col}\" for col in price_data.columns if col != \"Date\"}\n",
    "price_data.rename(columns=price_cols, inplace=True)\n",
    "\n",
    "merged = pd.merge(data, price_data, on=\"Date\", how=\"inner\").reset_index(drop=True)\n",
    "if len(merged) != len(data):\n",
    "    print(\"Warning: data length mismatch after merge.\")\n",
    "\n",
    "# Universe: infer from Actual_Return_* columns (safer than hand-typing)\n",
    "etf_list = infer_etf_list(merged)\n",
    "\n",
    "# Add stable (return-based) features and optional filters\n",
    "feature_data = add_stable_features(merged, etf_list)\n",
    "feature_data = filter_features(feature_data, include_predicted_returns=True, include_shap_metrics=True)\n",
    "\n",
    "# Impute/scale FEATURES ONLY\n",
    "feature_cols = [c for c in feature_data.columns if c != \"Date\" and not c.startswith(\"Actual_Return_\")]\n",
    "ret_cols = [c for c in feature_data.columns if c.startswith(\"Actual_Return_\")]\n",
    "\n",
    "# Forward/backward fill only features\n",
    "# feature_data[feature_cols] = feature_data[feature_cols].ffill().bfill()\n",
    "# Drop any rows with missing realized returns\n",
    "feature_data.dropna(subset=ret_cols, inplace=True)\n",
    "\n",
    "# Scale features using train fit only (per window below)\n",
    "# (we'll fit StandardScaler inside each window to avoid leakage)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Rolling windows (index-based)\n",
    "# -------------------------\n",
    "total_len = len(feature_data)\n",
    "start_indices = []\n",
    "cur = 0\n",
    "while True:\n",
    "    train_start = cur\n",
    "    train_end = train_start + cfg.train_window_days\n",
    "    val_end = train_end + cfg.validation_window_days\n",
    "    pred_end = val_end + cfg.prediction_window_days\n",
    "    if pred_end > total_len:\n",
    "        break\n",
    "    start_indices.append(cur)\n",
    "    # Overlap windows by (rebalance_period) so months join smoothly\n",
    "    cur += cfg.prediction_window_days - cfg.rebalance_period\n",
    "\n",
    "# Output dir\n",
    "output_dir = \"stage2_iterations\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Run N iterations (seeds)\n",
    "# -------------------------\n",
    "summary_records = []\n",
    "for iter_num in range(cfg.num_iterations):\n",
    "    iter_seed = cfg.base_seed + iter_num\n",
    "    set_global_seed(iter_seed)\n",
    "\n",
    "    iter_dir = os.path.join(output_dir, f\"iteration_{iter_num:02d}\")\n",
    "    os.makedirs(iter_dir, exist_ok=True)\n",
    "\n",
    "    # If warm-starting across windows, remember last model\n",
    "    prev_model_path = None\n",
    "    prev_vecnorm_path = None\n",
    "\n",
    "    iter_window_returns = []\n",
    "    iter_window_sharpes = []\n",
    "    iter_window_mdds = []\n",
    "\n",
    "    print(f\"\\n== Iteration {iter_num+1}/{cfg.num_iterations} (seed={iter_seed}) ==\")\n",
    "\n",
    "    for w_idx, start_idx in enumerate(start_indices):\n",
    "        window_t0 = time.time()\n",
    "        window_dir = os.path.join(iter_dir, f\"window_{w_idx:02d}\")\n",
    "        os.makedirs(window_dir, exist_ok=True)\n",
    "\n",
    "        # Slices\n",
    "        train_start = start_idx\n",
    "        train_end = train_start + cfg.train_window_days\n",
    "        val_start = train_end\n",
    "        val_end = val_start + cfg.validation_window_days\n",
    "        pred_start = val_end\n",
    "        pred_end = pred_start + cfg.prediction_window_days\n",
    "\n",
    "        train_df = feature_data.iloc[train_start:train_end].reset_index(drop=True).copy()\n",
    "        val_df   = feature_data.iloc[val_start:val_end].reset_index(drop=True).copy()\n",
    "        pred_df  = feature_data.iloc[pred_start:pred_end].reset_index(drop=True).copy()\n",
    "\n",
    "        # Scale FEATURES ONLY with train fit\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_df[feature_cols])\n",
    "        for df_ in (train_df, val_df, pred_df):\n",
    "            df_.loc[:, feature_cols] = df_[feature_cols].ffill()\n",
    "            X = scaler.transform(df_[feature_cols])\n",
    "            df_.loc[:, feature_cols] = X\n",
    "\n",
    "        # Tuning (only for first window by default for speed; you can retune per window)\n",
    "        if w_idx == 0:\n",
    "            best_params = validate_and_tune(train_df, val_df, etf_list, cfg, random_seed=iter_seed)\n",
    "            # Save the params for audit\n",
    "            with open(os.path.join(iter_dir, \"best_params.json\"), \"w\") as f:\n",
    "                json.dump(best_params, f, indent=2)\n",
    "        else:\n",
    "            # reuse previous best params\n",
    "            # (optional: small perturbation or periodic re-tune)\n",
    "            pass\n",
    "\n",
    "        # Train with early stopping\n",
    "        model_path, vecnorm_path = train_with_early_stopping(\n",
    "            train_df, val_df, etf_list, cfg, best_params, window_dir\n",
    "        )\n",
    "\n",
    "        # Warm-start next window?\n",
    "        if cfg.warm_start_across_windows:\n",
    "            prev_model_path = model_path\n",
    "            prev_vecnorm_path = vecnorm_path\n",
    "\n",
    "        # Predict & OOS metrics\n",
    "        window_ret, window_sharpe, window_mdd = predict_and_log(\n",
    "            pred_df, etf_list, cfg, best_params, model_path, vecnorm_path, window_dir\n",
    "        )\n",
    "        iter_window_returns.append(window_ret)\n",
    "        iter_window_sharpes.append(window_sharpe)\n",
    "        iter_window_mdds.append(window_mdd)\n",
    "\n",
    "        minutes = (time.time() - window_t0) / 60.0\n",
    "        print(f\"  Window {w_idx+1}/{len(start_indices)} done in {minutes:.2f} min | \"\n",
    "              f\"RET={window_ret:.3%} SH={window_sharpe:.2f} MDD={window_mdd:.2%}\")\n",
    "\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Aggregate across windows in this iteration\n",
    "    mean_ret = float(np.mean(iter_window_returns)) if iter_window_returns else 0.0\n",
    "    mean_sharpe = float(np.nanmean(iter_window_sharpes)) if iter_window_sharpes else np.nan\n",
    "    mean_mdd = float(np.nanmean(iter_window_mdds)) if iter_window_mdds else np.nan\n",
    "\n",
    "    summary_records.append({\n",
    "        \"iteration\": iter_num,\n",
    "        \"seed\": iter_seed,\n",
    "        \"mean_window_return\": mean_ret,\n",
    "        \"mean_window_sharpe\": mean_sharpe,\n",
    "        \"mean_window_mdd\": mean_mdd\n",
    "    })\n",
    "\n",
    "# Save & statistics\n",
    "summary_df = pd.DataFrame(summary_records)\n",
    "summary_df.to_csv(os.path.join(output_dir, \"iterations_summary.csv\"), index=False)\n",
    "\n",
    "t_stat, p_val = ttest_1samp(summary_df[\"mean_window_return\"], 0.0)\n",
    "with open(os.path.join(output_dir, \"t_test_result.csv\"), \"w\") as f:\n",
    "    f.write(f\"t-statistic,{t_stat}\\np-value,{p_val}\\n\")\n",
    "\n",
    "print(\"\\n=== Overall Summary ===\")\n",
    "print(summary_df)\n",
    "print(f\"Overall t-statistic={t_stat:.3f}, p-value={p_val:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-08-12T04:25:58.513174Z"
    }
   },
   "id": "8acf6abff959b252",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "87003584b4f64b8e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "86d1a98b60121652",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "293e8671fe5851b5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "32482645257235b5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "output_dir = 'stage2_iterations'\n",
    "pattern = os.path.join(output_dir, 'iteration_*', 'window_*', 'weights.csv')\n",
    "\n",
    "# Find all weight files\n",
    "files = glob.glob(pattern)\n",
    "\n",
    "# --- Pass 1: Determine the master ETF column order ---\n",
    "etf_columns = None\n",
    "for file_path in files:\n",
    "    df_temp = pd.read_csv(file_path, nrows=1)\n",
    "    possible_etfs = [c for c in df_temp.columns if c not in ['Date', 'Iteration', 'Window']]\n",
    "    if etf_columns is None:\n",
    "        etf_columns = possible_etfs\n",
    "    else:\n",
    "        etf_columns = sorted(set(etf_columns).union(possible_etfs))\n",
    "\n",
    "# --- Pass 2: Read, align, and combine ---\n",
    "all_weights = []\n",
    "for file_path in files:\n",
    "    parts = file_path.split(os.sep)\n",
    "    iteration = int(parts[-3].split('_')[1]) + 1  # shift to start from 1\n",
    "    window = int(parts[-2].split('_')[1])\n",
    "\n",
    "    df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "    # Ensure all ETF columns exist and are in the same order\n",
    "    for col in etf_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    df = df[['Date'] + etf_columns]\n",
    "\n",
    "    # Add window and iteration (iteration will be moved later)\n",
    "    df.insert(0, 'Window', window)\n",
    "    df['Iteration'] = iteration  # temp position at end\n",
    "\n",
    "    all_weights.append(df)\n",
    "\n",
    "# Combine\n",
    "combined_df = pd.concat(all_weights, ignore_index=True)\n",
    "combined_df.sort_values(['Iteration', 'Window', 'Date'], inplace=True)\n",
    "\n",
    "# Move Iteration to end\n",
    "iteration_col = combined_df.pop('Iteration')\n",
    "combined_df['Iteration'] = iteration_col\n",
    "\n",
    "# Save\n",
    "out_path = os.path.join(output_dir, 'combined_weights.csv')\n",
    "combined_df.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"Combined weights saved to: {out_path}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6947d247eaecd896",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3462ab258866c09e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d9e17792fa6118e4",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
