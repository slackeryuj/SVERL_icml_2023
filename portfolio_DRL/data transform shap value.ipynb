{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "file_path = \"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/test_data_shap_metrics.csv\"  # Replace with the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
    "df['End_Date'] = pd.to_datetime(df['End_Date'])\n",
    "\n",
    "# Pivot the table to transform it into a time series format with Start_Date and End_Date\n",
    "df_pivot = df.pivot_table(\n",
    "    index=['Start_Date', 'End_Date'], \n",
    "    columns='Stock', \n",
    "    values=[col for col in df.columns if col not in ['Stock', 'Start_Date', 'End_Date', 'Phase']]\n",
    ")\n",
    "\n",
    "# Flatten the multi-index columns\n",
    "df_pivot.columns = [f\"{col[0]}_{col[1]}\" for col in df_pivot.columns]\n",
    "\n",
    "# Sort the dataframe by Start_Date and End_Date\n",
    "df_pivot.sort_index(inplace=True)\n",
    "\n",
    "# Save the transformed data to a new CSV file\n",
    "df_pivot.to_csv(\"transformed_shap_timeseries.csv\")\n",
    "\n",
    "print(\"Transformation complete. Data saved as 'transformed_shap_timeseries.csv'.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b60092c0db6e0b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the transformed SHAP dataset\n",
    "shap_file_path = \"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/transformed_shap_timeseries.csv\"\n",
    "df_shap = pd.read_csv(shap_file_path, index_col=[0, 1])\n",
    "\n",
    "# Load the ETF daily returns dataset\n",
    "returns_file_path = \"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/daily_returns.csv\"\n",
    "df_returns = pd.read_csv(returns_file_path, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# Ensure ETF tickers are correctly identified\n",
    "etfs = list(set([col.split(\"_\")[-1] for col in df_shap.columns if \"mean_return\" not in col and \"volatility\" not in col]))\n",
    "\n",
    "# Initialize columns for mean return and volatility\n",
    "for etf in etfs:\n",
    "    df_shap[f\"mean_return_{etf}\"] = np.nan\n",
    "    df_shap[f\"volatility_{etf}\"] = np.nan\n",
    "\n",
    "# Compute mean return and volatility for each ETF based on Start_Date and End_Date\n",
    "for (start_date, end_date) in df_shap.index[[0]]:\n",
    "\n",
    "    # Convert to datetime format\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Get mean return over the period\n",
    "    period_returns = df_returns.loc[start_date:end_date]\n",
    "    mean_returns = period_returns.mean()\n",
    "    period_returns_df = pd.DataFrame(period_returns)\n",
    "    # Get volatility using past 60 days from end_date\n",
    "    past_60_returns = df_returns.loc[:end_date].iloc[-60:]\n",
    "    volatilities = past_60_returns.std()\n",
    "\n",
    "    # Assign computed values to df_shap\n",
    "    for etf in etfs:\n",
    "        if etf in mean_returns and etf in volatilities:\n",
    "            df_shap.at[(start_date, end_date), f\"mean_return_{etf}\"] = mean_returns[etf]\n",
    "            df_shap.at[(start_date, end_date), f\"volatility_{etf}\"] = volatilities[etf]\n",
    "\n",
    "# Save the updated dataset\n",
    "updated_shap_file_path = \"transformed_shap_timeseries_updated_test.csv\"\n",
    "df_shap.to_csv(updated_shap_file_path)\n",
    "\n",
    "print(f\"Updated dataset saved as {updated_shap_file_path}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38a43f32252612a9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"transformed_shap_timeseries_updated.csv\"\n",
    "df_shap = pd.read_csv(file_path, index_col=[0, 1])\n",
    "# Sort columns first by feature type, then by ETF\n",
    "feature_order = sorted(df_shap.columns, key=lambda x: (x.split('_')[0], x.split('_')[-1]))\n",
    "df_shap = df_shap[feature_order]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8fd988e2d3c0575",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_shap.to_csv(\"transformed_shap_timeseries_updated2.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2574b4646c90bdb3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the daily returns dataset\n",
    "daily_returns_path = \"daily_returns.csv\"\n",
    "df_returns = pd.read_csv(daily_returns_path, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# Sort daily return columns in the same order as feature_order\n",
    "sorted_etfs =  [col.replace(\"mean_return_\", \"\") for col in feature_order if \"mean_return\" in col]\n",
    "df_returns = df_returns[sorted_etfs]\n",
    "df_returns.to_csv(\"daily_returns2.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f70cbcf3912eaa63",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "etfs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6643d475d373816",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "file_path = \"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/transformed_shap_timeseries_updated_sort.csv\"\n",
    "df_shap = pd.read_csv(file_path, index_col=[0, 1])\n",
    "\n",
    "# Extract ETFs\n",
    "etfs = list(set([col.split(\"_\")[-1] for col in df_shap.columns if \"mean_return\" in col]))\n",
    "print(f'etfs={etfs}')\n",
    "\n",
    "global num_etfs\n",
    "num_etfs = len(etfs)\n",
    "print(f'num_etfs={num_etfs}')\n",
    "daily_returns_path = \"daily_returns_sort.csv\"\n",
    "df_returns = pd.read_csv(daily_returns_path, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "# Train agent with rolling 78-sample window\n",
    "window_size = 78\n",
    "num_samples = len(df_shap)\n",
    "out_of_sample_weights = []\n",
    "\n",
    "for start in range(0, num_samples - window_size, window_size):\n",
    "    end = start + window_size\n",
    "    train_data = df_shap.iloc[start:end]\n",
    "    env = make_vec_env(lambda: PortfolioTradingEnv(train_data, df_returns,etfs), n_envs=1)\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    \n",
    "    # Save trained model\n",
    "    model_path = f\"ppo_portfolio_model_{start}_{end}.zip\"\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "    \n",
    "    # Generate out-of-sample weights\n",
    "    test_data = df_shap.iloc[end:end+1]\n",
    "    obs = test_data.values\n",
    "    weights_list = []\n",
    "    for _ in range(100):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        weights_list.append(action)\n",
    "    avg_weights = np.mean(weights_list, axis=0)\n",
    "    out_of_sample_weights.append(avg_weights)\n",
    "\n",
    "# Save the results\n",
    "out_of_sample_df = pd.DataFrame(out_of_sample_weights, columns=etfs)\n",
    "out_of_sample_df.to_csv(\"out_of_sample_weights.csv\")\n",
    "print(\"Training complete. Models and out-of-sample weights saved.\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4245e0b55284d8d5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "\n",
    "# Load the transformed SHAP dataset\n",
    "file_path = \"transformed_shap_timeseries_updated_sort.csv\"\n",
    "df_shap = pd.read_csv(file_path, index_col=[0, 1])\n",
    "\n",
    "# Load the daily returns dataset\n",
    "daily_returns_path = \"daily_returns_sort.csv\"\n",
    "df_returns = pd.read_csv(daily_returns_path, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# Extract ETFs\n",
    "etfs = list(set([col.split(\"_\")[-1] for col in df_shap.columns if \"mean_return\" in col]))\n",
    "global num_etfs\n",
    "num_etfs = len(etfs)\n",
    "\n",
    "class PortfolioTradingEnv(gym.Env):\n",
    "    def __init__(self, data, returns, initial_balance=1000, etfs=None):\n",
    "        super(PortfolioTradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.returns = returns\n",
    "        self.initial_balance = initial_balance\n",
    "        self.current_step = 0\n",
    "        self.etfs = etfs if etfs else sorted_etfs\n",
    "        self.num_etfs = len(self.etfs)\n",
    "        self.etfs = etfs if etfs else []\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(data.shape[1],), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=0, high=1.0, shape=(12,), dtype=np.float32)\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.weights = np.ones(self.num_etfs) / self.num_etfs\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "    def __init__(self, data, returns, initial_balance=1000):\n",
    "        super(PortfolioTradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.returns = returns\n",
    "        self.initial_balance = initial_balance\n",
    "        self.current_step = 0\n",
    "        self.etfs = etfs if etfs else sorted_etfs\n",
    "        self.num_etfs = len(self.etfs)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(data.shape[1],), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=np.array([0.0] * 12, dtype=np.float32), high=np.array([1.0] * 12, dtype=np.float32), dtype=np.float32)\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.weights = np.ones(self.num_etfs) / self.num_etfs\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.weights = np.ones(self.num_etfs) / self.num_etfs\n",
    "        return self.data.iloc[self.current_step].values\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.array(action, dtype=np.float32)\n",
    "        action = np.clip(action, 0, 1)\n",
    "        action /= np.sum(action)\n",
    "        self.weights = action\n",
    "\n",
    "        start_date, end_date = self.data.index[self.current_step]\n",
    "        period_returns = self.returns.loc[start_date:end_date, self.etfs].values\n",
    "        portfolio_values = [self.portfolio_value]\n",
    "        for daily_return in period_returns:\n",
    "            self.portfolio_value *= (1 + np.dot(daily_return, self.weights))\n",
    "            self.weights *= (1 + daily_return)\n",
    "            self.weights /= np.sum(self.weights)  # Normalize weights after drift\n",
    "            portfolio_values.append(self.portfolio_value)\n",
    "        cumulative_return = (portfolio_values[-1] / portfolio_values[0]) - 1\n",
    "        \n",
    "\n",
    "        self.portfolio_value *= (1 + cumulative_return)\n",
    "        reward = np.log(self.portfolio_value / self.initial_balance)\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        next_state = self.data.iloc[self.current_step].values if not done else np.zeros_like(self.data.iloc[0].values)\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Step: {self.current_step}, Portfolio Value: {self.portfolio_value:.2f}\")\n",
    "\n",
    "# Train agent with rolling 78-sample window\n",
    "window_size = 78\n",
    "num_samples = len(df_shap)\n",
    "out_of_sample_weights = []\n",
    "\n",
    "for start in range(0, num_samples - window_size, window_size):\n",
    "    end = start + window_size\n",
    "    train_data = df_shap.iloc[start:end]\n",
    "    env = make_vec_env(lambda: PortfolioTradingEnv(train_data, df_returns), n_envs=1)\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    \n",
    "    # Save trained model\n",
    "    model_path = f\"ppo_portfolio_model_{start}_{end}.zip\"\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "    \n",
    "    # Generate out-of-sample weights\n",
    "    test_data = df_shap.iloc[end:end+1]\n",
    "    obs = test_data.values\n",
    "    weights_list = []\n",
    "    for _ in range(100):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        weights_list.append(action)\n",
    "    avg_weights = np.mean(weights_list, axis=0)\n",
    "    out_of_sample_weights.append(avg_weights)\n",
    "\n",
    "# Save the results\n",
    "out_of_sample_df = pd.DataFrame(out_of_sample_weights, columns=etfs)\n",
    "out_of_sample_df.to_csv(\"out_of_sample_weights.csv\")\n",
    "print(\"Training complete. Models and out-of-sample weights saved.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8440aae0340d2af",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dd7a270f0915a679"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2007-06-12   -0.005077\n",
      "2007-06-13    0.017762\n",
      "2007-06-14    0.000470\n",
      "2007-06-15    0.004183\n",
      "2007-06-18   -0.001155\n",
      "dtype: float64\n",
      "E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\\tickers_DIA.txt\n",
      "['AAPL', 'DIA', 'IYR']\n",
      "Downloading data for AAPL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for DIA...\n",
      "Downloading data for IYR...\n",
      "Data saved to E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\\stock_prices_DIA.csv\n",
      "Factor returns alignment completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "weights_file = \"port_wt_test.csv\"  # Optimal security weights\n",
    "returns_file = \"port_ret_test.csv\"  # Daily stock returns\n",
    "\n",
    "# Read the files\n",
    "weights_df = pd.read_csv(weights_file)\n",
    "returns_df = pd.read_csv(returns_file)\n",
    "\n",
    "# Convert Date columns to datetime format for proper alignment\n",
    "weights_df[\"Date\"] = pd.to_datetime(weights_df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "returns_df[\"Date\"] = pd.to_datetime(returns_df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "# Set Date as index for easier merging\n",
    "weights_df.set_index(\"Date\", inplace=True)\n",
    "returns_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Sort data to ensure chronological order\n",
    "weights_df.sort_index(inplace=True)\n",
    "returns_df.sort_index(inplace=True)\n",
    "\n",
    "# Initialize drifted weights dataframe with the same structure as returns_df\n",
    "aligned_weights_df = weights_df.reindex(returns_df.index, method='ffill')\n",
    "drifted_weights_df = aligned_weights_df.copy()\n",
    "\n",
    "# Iterate through each day to apply drifted weight calculation\n",
    "for i in range(1, len(drifted_weights_df)):\n",
    "    # If the day is a rebalancing day, keep the given weights\n",
    "    if drifted_weights_df.index[i] in weights_df.index:\n",
    "        continue\n",
    "\n",
    "    # Otherwise, update weights based on previous day's weights and returns\n",
    "    drifted_weights_df.iloc[i] = drifted_weights_df.iloc[i - 1] * (1 + returns_df.iloc[i - 1])\n",
    "\n",
    "    # Normalize weights to ensure they sum to 100%\n",
    "    drifted_weights_df.iloc[i] /= drifted_weights_df.iloc[i].sum()\n",
    "\n",
    "# Compute portfolio returns using drifted weights\n",
    "portfolio_returns_drifted = (returns_df * drifted_weights_df).sum(axis=1)\n",
    "\n",
    "# Save portfolio returns to CSV\n",
    "portfolio_returns_drifted.to_csv(\"portfolio_returns_drifted.csv\")\n",
    "# Display first few rows\n",
    "print(portfolio_returns_drifted.head())\n",
    "\n",
    "\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper\n",
    "input_dir = \"E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\"  # Use the specified directory\n",
    "\n",
    "start_date = \"2007-06-11\"\n",
    "end_date = \"2024-09-17\"\n",
    "output_csv = os.path.join(input_dir, \"stock_prices_DIA.csv\")\n",
    "\n",
    "ticker_file = os.path.join(input_dir, \"tickers_DIA.txt\")  # Text file containing tickers, one per line\n",
    "print(ticker_file)\n",
    "tickers = load_tickers_from_file(ticker_file)\n",
    "print(tickers)\n",
    "price_data = download_stock_prices(tickers, start_date, end_date, output_csv)\n",
    "\n",
    "print(\"Factor returns alignment completed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T02:35:39.377817Z",
     "start_time": "2025-04-01T02:35:36.733065Z"
    }
   },
   "id": "77e78ea74bed9e51",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "drifted_weights_df.to_csv(\"drifted_weights_df.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T02:38:38.090917Z",
     "start_time": "2025-04-01T02:38:37.952829Z"
    }
   },
   "id": "779c95a41ac7290b",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate index in weights_df: False\n",
      "Duplicate index in returns_df: False\n",
      "Date\n",
      "2007-06-12   -0.010149\n",
      "2007-06-13    0.013309\n",
      "2007-06-14    0.002380\n",
      "2007-06-15    0.002928\n",
      "2007-06-18   -0.001720\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "weights_file = \"port_wt_test.csv\"  # Optimal security weights\n",
    "returns_file = \"port_ret_test.csv\"  # Daily stock returns\n",
    "\n",
    "# Read the files\n",
    "weights_df = pd.read_csv(weights_file)\n",
    "returns_df = pd.read_csv(returns_file)\n",
    "\n",
    "# Convert Date columns to datetime format for proper alignment\n",
    "weights_df[\"Date\"] = pd.to_datetime(weights_df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "returns_df[\"Date\"] = pd.to_datetime(returns_df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "# Set Date as index for easier merging\n",
    "weights_df.set_index(\"Date\", inplace=True)\n",
    "returns_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Sort data to ensure chronological order\n",
    "weights_df.sort_index(inplace=True)\n",
    "returns_df.sort_index(inplace=True)\n",
    "print(\"Duplicate index in weights_df:\", weights_df.index.duplicated().any())\n",
    "print(\"Duplicate index in returns_df:\", returns_df.index.duplicated().any())\n",
    "# Initialize drifted weights dataframe with the same structure as returns_df\n",
    "aligned_weights_df = weights_df.reindex(returns_df.index, method='ffill')\n",
    "drifted_weights_df = aligned_weights_df.copy()\n",
    "\n",
    "# Iterate through each day to apply drifted weight calculation\n",
    "for i in range(1, len(drifted_weights_df)):\n",
    "    # If the day is a rebalancing day, keep the given weights\n",
    "    if drifted_weights_df.index[i] in weights_df.index:\n",
    "        continue\n",
    "\n",
    "    # Otherwise, update weights based on previous day's weights and returns\n",
    "    drifted_weights_df.iloc[i] = drifted_weights_df.iloc[i - 1] * (1 + returns_df.iloc[i - 1])\n",
    "\n",
    "    # Normalize weights to ensure they sum to 100%\n",
    "    drifted_weights_df.iloc[i] /= drifted_weights_df.iloc[i].sum()\n",
    "\n",
    "# Compute portfolio returns using drifted weights\n",
    "portfolio_returns_drifted = (returns_df * drifted_weights_df).sum(axis=1)\n",
    "\n",
    "# Save portfolio returns to CSV\n",
    "portfolio_returns_drifted.to_csv(\"portfolio_returns_drifted_test.csv\")\n",
    "drifted_weights_df.to_csv(\"portfolio_returns_driftedweights_test.csv\")\n",
    "# Display first few rows\n",
    "print(portfolio_returns_drifted.head())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T01:52:53.698794Z",
     "start_time": "2025-04-01T01:52:51.311314Z"
    }
   },
   "id": "d0eae632ec2a5e1f",
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
