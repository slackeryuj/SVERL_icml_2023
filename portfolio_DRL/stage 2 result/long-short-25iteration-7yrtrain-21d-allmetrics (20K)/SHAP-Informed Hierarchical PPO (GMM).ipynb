{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, factors, returns, sequence_length, N=1):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - factors: pd.DataFrame of normalized input features (index: date, columns: features)\n",
    "        - returns: pd.DataFrame with 1 column, normalized returns (index aligned with factors)\n",
    "        - sequence_length: number of lookback days for attention model\n",
    "        - N: number of future days to compute mean return for target (default = 1)\n",
    "        \"\"\"\n",
    "        assert factors.shape[0] == returns.shape[0], \"Factors and returns must be aligned\"\n",
    "\n",
    "        self.factors = factors.values\n",
    "        self.returns = returns.values\n",
    "        self.sequence_length = sequence_length\n",
    "        self.N = N\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.factors) - self.sequence_length  # since you're predicting 1 day ahead\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.factors[idx : idx + self.sequence_length]  # Shape: (seq_len, num_features)\n",
    "        y = self.returns[idx + self.sequence_length]  # Scalar target: next-day return\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor([y], dtype=torch.float32)  # Shape: (1,)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_dir = \"E:\\XAI\\RL_with_SHAP\\pythonProject1\\SVERL_icml_2023\\portfolio_DRL\"  # Use the specified directory\n",
    "ticker_file = os.path.join(input_dir, \"tickers.txt\")  # Text file containing tickers, one per line\n",
    "factor_file = os.path.join(input_dir, \"FF_factors.csv\")  # CSV file containing factor returns\n",
    "start_date = \"2005-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "output_csv = os.path.join(input_dir, \"stock_prices.csv\")\n",
    "\n",
    "\n",
    "factors = pd.read_csv(f\"{input_dir}/aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns_raw = pd.read_csv(f\"{input_dir}/daily_returns_DIA.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors = factors.dropna()\n",
    "returns = returns_raw.dropna()\n",
    "\n",
    "# Align dates between factors and returns\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors_raw = factors.loc[dates]\n",
    "returns_raw = returns.loc[dates]\n",
    "factors_raw = factors_raw.iloc[:, :-1]\n",
    "# print(f\"factors_raw = {factors_raw}\")\n",
    "\n",
    "Nday_diff = 1  # Define the future prediction horizon for shift days\n",
    "# limit to 2 stock ONLY\n",
    "returns_raw = returns_raw.iloc[:, :1]\n",
    "\n",
    "# Remove NaN values and align dates\n",
    "factors_raw = factors_raw.dropna()\n",
    "returns_raw = returns_raw.dropna()\n",
    "# factors_raw = pd.concat([factors_raw, returns_raw], axis=1)\n",
    "# Align dates between factors_raw and returns_raw\n",
    "dates = factors_raw.index.intersection(returns_raw.index)\n",
    "factors_raw = factors_raw.loc[dates]\n",
    "returns_raw = returns_raw.loc[dates]\n",
    "\n",
    "# Shift returns to generate target variable (N-day future return)\n",
    "returns_raw_Y = returns_raw.shift(-Nday_diff).dropna()\n",
    "\n",
    "\n",
    "# Align all datasets with available dates in returns_raw_Y\n",
    "dates_final = factors_raw.index.intersection(returns_raw_Y.index)\n",
    "factors_raw = factors_raw.loc[dates_final]\n",
    "returns_raw = returns_raw.loc[dates_final]\n",
    "returns_raw_Y = returns_raw_Y.loc[dates_final]\n",
    "\n",
    "N = 1\n",
    "sequence_length = 10\n",
    "batch_size = 64\n",
    "num_epochs = 500\n",
    "learning_rate = 0.001\n",
    "rolling_window_train = 252  # Reduce for faster adaptation\n",
    "rolling_window_test = 10    # Reduce test window\n",
    "prediction_window = 10      # Align prediction period with test window\n",
    "train_model2 = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Record results\n",
    "results = []\n",
    "prediction_records = []\n",
    "shapley_records = []\n",
    "shapley_metrics_df = []\n",
    "all_test_losses = []\n",
    "all_binary_test_losses = []\n",
    "results = []\n",
    "classification_results = []\n",
    "shap_metrics_results = []\n",
    "test_results = []\n",
    "prediction_results = []\n",
    "combined_test_result = []\n",
    "portfolio_records = []\n",
    "start = 0  # Initialize rolling split index\n",
    "\n",
    "# Rolling split: Train (M days), Test (N days), Predict (W days)\n",
    "# total_samples = len(factors_raw)\n",
    "total_samples = 274\n",
    "# total_samples = 382\n",
    "print(f'total_samples = {total_samples}')\n",
    "print(f'total sum = {rolling_window_train + rolling_window_test + prediction_window}')\n",
    "\n",
    "while start + rolling_window_train + rolling_window_test + prediction_window <= total_samples:\n",
    "\n",
    "    train_start = start\n",
    "    train_end = start + rolling_window_train\n",
    "\n",
    "    test_start = train_end\n",
    "    test_end = test_start + rolling_window_test\n",
    "\n",
    "    predict_start = test_end  # Predictions start right after testing\n",
    "    predict_end = predict_start + prediction_window\n",
    "\n",
    "    # Extract the actual date indices\n",
    "    train_dates = (factors_raw.iloc[train_start].name, factors_raw.iloc[train_end - 1].name)\n",
    "    test_dates = (factors_raw.iloc[test_start].name, factors_raw.iloc[test_end - 1].name)\n",
    "    predict_dates = (factors_raw.iloc[predict_start].name, factors_raw.iloc[predict_end - 1].name)\n",
    "\n",
    "    print(f\"\\nðŸ”¹ Processing Date Window: Train {train_dates}, Test {test_dates}, Predict {predict_dates}\")\n",
    "    print(f'returns_raw.columns = {returns_raw.columns}')\n",
    "    for stock in returns_raw.columns:\n",
    "        print(f\"Training for stock: {stock}\")\n",
    "        stock_returns = returns_raw_Y[[stock]]  # Select the stock column\n",
    "        # stock_returns = returns_raw[[stock]].shift(-1).dropna()\n",
    "        # stock_returns_mean = returns_raw_Y[[stock]].rolling(window=N, min_periods=N).mean().shift(-N+1) # calc average over next N period\n",
    "        # stock_returns = stock_returns_mean.dropna()\n",
    "\n",
    "        stock_returns_unshift = returns_raw[[stock]]\n",
    "            \n",
    "        factors_raw_X = pd.concat([factors_raw, stock_returns_unshift], axis=1)\n",
    "        # Rolling split: Train (M days), Test (N days), Predict (W days)\n",
    "        # total_samples = len(factors_raw_X)\n",
    "        # Initialize the rolling split index\n",
    "\n",
    "        # Print the selected date ranges\n",
    "        print(f\"Training dates: {train_dates}\")\n",
    "        print(f\"Testing dates: {test_dates}\")\n",
    "        print(f\"Prediction dates: {predict_dates}\")\n",
    "\n",
    "        # # Split data into training, testing, and prediction sets\n",
    "        train_factors = factors_raw_X.iloc[train_start:train_end]\n",
    "        train_returns = stock_returns.iloc[train_start:train_end]\n",
    "        test_factors = factors_raw_X.iloc[test_start:test_end]\n",
    "        test_returns = stock_returns.iloc[test_start:test_end]\n",
    "        predict_factors = factors_raw_X.iloc[predict_start:predict_end]\n",
    "        predict_returns = stock_returns.iloc[predict_start:predict_end]\n",
    "        #\n",
    "        # # Normalize train and test data\n",
    "        train_factors_mean, train_factors_std = train_factors.mean(), train_factors.std()\n",
    "        train_returns_mean, train_returns_std = train_returns.mean(), train_returns.std()\n",
    "        # print(f'train_factors_mean ={train_factors_mean}')\n",
    "        # Avoid division by zero\n",
    "        train_factors_std.replace(0, 1e-8, inplace=True)\n",
    "        train_returns_std.replace(0, 1e-8, inplace=True)\n",
    "\n",
    "        train_factors_norm = (train_factors - train_factors_mean) / train_factors_std\n",
    "        train_returns_norm = (train_returns - train_returns_mean) / train_returns_std\n",
    "        test_factors_norm = (test_factors - train_factors_mean) / train_factors_std\n",
    "        test_returns_norm = (test_returns - train_returns_mean) / train_returns_std\n",
    "        predict_factors_norm = (predict_factors - train_factors_mean) / train_factors_std\n",
    "        predict_returns_norm = (predict_returns - train_returns_mean) / train_returns_std\n",
    "        # print(f'train_factors_norm dim: {train_factors_norm.shape}')\n",
    "        # print(f'train_returns_norm dim: {train_returns_norm.shape}')\n",
    "        dataset = StockDataset(train_factors_norm, train_returns_norm, sequence_length, N=1)\n",
    "        \n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        # for batch_x, batch_y in dataloader:\n",
    "        #     print(f\"Batch X Shape: {batch_x.shape}\")  # Expected: (batch_size, sequence_length, num_features)\n",
    "        #     print(f\"Batch Y Shape: {batch_y.shape}\")  # Expected: (batch_size, N, num_features)\n",
    "        #     break  # Only print for the first batch\n",
    "        # # Model initialization\n",
    "        d_model = train_factors.shape[1]\n",
    "        # print(f'd_model: {d_model}')\n",
    "\n",
    "        num_heads = min(4,\n",
    "                        d_model)  # Ensure num_heads does not exceed d_model and d_model is divisible by num_heads\n",
    "        while d_model % num_heads != 0:\n",
    "            num_heads -= 1\n",
    "        # print(f\"d_model: {d_model} num_heads: {num_heads} sequence_length: {sequence_length} and train factor return dim: {train_factors.shape[1]} {train_returns.shape[1]}\")\n",
    "        model = StockPredictAgent(d_model=d_model, num_heads=num_heads, sequence_length=sequence_length, N=1).to(\n",
    "            device)\n",
    "        #\n",
    "        # # Optimizer and loss function\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "        criterion = nn.SmoothL1Loss(beta=1.0)  # Huber Loss for stability\n",
    "        #\n",
    "        # # Training the model\n",
    "        # print(f\"Training window: {train_start} to {train_end}\")\n",
    "        # print(f\"Expected FC Layer Input Shape: {sequence_length * d_model}\")\n",
    "        # print(f\"Flattened Tensor Shape: {batch_size, sequence_length * d_model}\")\n",
    "\n",
    "        train_model_with_logging(model, dataloader, optimizer, criterion, scheduler, num_epochs, device)\n",
    "\n",
    "        print(f'Finished training model 1')\n",
    "        # print(f'train_factors_norm dim: {train_factors_norm.shape}')\n",
    "        # Convert to PyTorch tensors\n",
    "        # print(f'test_factors_tensor: {test_factors_tensor.shape}')\n",
    "        # Run inference on test data\n",
    "\n",
    "        augmented_train_features, shap_metrics_train = generate_augmented_features(\n",
    "            train_factors_norm, train_factors_mean, train_factors_std, model, rolling_window_test, device,\n",
    "            return_metrics=True\n",
    "        )\n",
    "        shap_metrics_train[\"Stock\"] = stock\n",
    "        shap_metrics_train[\"Start_Date\"] = train_dates[0]\n",
    "        shap_metrics_train[\"End_Date\"] = train_dates[1]\n",
    "        shap_metrics_train[\"Phase\"] = 'Train'\n",
    "        shap_metrics_results.append(shap_metrics_train.iloc[0].tolist())\n",
    "\n",
    "        augmented_test_features, shap_metrics_test = generate_augmented_features_testphase(\n",
    "            test_factors_norm, train_factors_norm, train_factors_mean, train_factors_std,\n",
    "            model, prediction_window, device, return_metrics=True\n",
    "        )\n",
    "        shap_metrics_test[\"Stock\"] = stock\n",
    "        shap_metrics_test[\"Start_Date\"] = test_dates[0]\n",
    "        shap_metrics_test[\"End_Date\"] = test_dates[1]\n",
    "        shap_metrics_test[\"Phase\"] = 'Test'\n",
    "        shap_metrics_results.append(shap_metrics_test.iloc[0].tolist())\n",
    "\n",
    "        augmented_predict_features, shap_metrics_predict = generate_augmented_features_testphase(\n",
    "            predict_factors_norm, train_factors_norm, train_factors_mean, train_factors_std,\n",
    "            model, prediction_window, device, return_metrics=True\n",
    "        )\n",
    "        shap_metrics_predict[\"Stock\"] = stock\n",
    "        shap_metrics_predict[\"Start_Date\"] = predict_dates[0]\n",
    "        shap_metrics_predict[\"End_Date\"] = predict_dates[1]\n",
    "        shap_metrics_predict[\"Phase\"] = 'Predict'\n",
    "        shap_metrics_results.append(shap_metrics_predict.iloc[0].tolist())\n",
    "        print(f'---Append Shape value metrics for {stock} and last date is {predict_dates[1]}---')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # start += prediction_window\n",
    "    start += 1\n",
    "\n",
    "shap_metrics_results_df = pd.DataFrame(shap_metrics_results, columns=[\n",
    "            \"mean_abs_shap_Mkt-RF\", \"mean_abs_shap_SMB\", \"mean_abs_shap_HML\", \"mean_abs_shap_RMW\", \"mean_abs_shap_CMA\",\n",
    "            \"mean_abs_shap_Stock\",\n",
    "            \"mean_shap_Mkt-RF\", \"mean_shap_SMB\", \"mean_shap_HML\", \"mean_shap_RMW\", \"mean_shap_CMA\", \"mean_shap_Stock\",\n",
    "            \"shap_std_Mkt-RF\", \"shap_std_SMB\", \"shap_std_HML\", \"shap_std_RMW\", \"shap_std_CMA\", \"shap_std_Stock\",\n",
    "            \"mean_over_std_Mkt-RF\", \"mean_over_std_SMB\", \"mean_over_std_HML\", \"mean_over_std_RMW\", \"mean_over_std_CMA\",\n",
    "            \"mean_over_std_Stock\",\n",
    "            \"mean_abs_over_std_Mkt-RF\", \"mean_abs_over_std_SMB\", \"mean_abs_over_std_HML\", \"mean_abs_over_std_RMW\",\n",
    "            \"mean_abs_over_std_CMA\", \"mean_abs_over_std_Stock\",\n",
    "            \"feature_importance_ranking_Mkt-RF\", \"feature_importance_ranking_SMB\", \"feature_importance_ranking_HML\",\n",
    "            \"feature_importance_ranking_RMW\", \"feature_importance_ranking_CMA\", \"feature_importance_ranking_Stock\",\n",
    "            \"Stock\", \"Start_Date\", \"End_Date\", \"Phase\"\n",
    "        ])\n",
    "shap_metrics_results_df.to_csv(\"shap_value_metrics_export_DIA.csv\", index=False)\n",
    "print(\"shap_value_metrics_export saved successfully!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4e64f52150cf1f6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta\n",
    "\n",
    "# Load DIA price data (replace with your actual path)\n",
    "etf = pd.read_csv(\"stock_prices_DIA.csv\", parse_dates=[\"Date\"])\n",
    "etf.set_index(\"Date\", inplace=True)\n",
    "\n",
    "dia = etf[[stock]].copy()  # âœ… returns DataFrame\n",
    "dia.rename(columns={stock: \"Close\"}, inplace=True)  # Rename to 'Close'\n",
    "\n",
    "# Compute indicators\n",
    "dia[\"Return_1d\"] = dia[\"Close\"].pct_change()\n",
    "dia[\"Return_5d\"] = dia[\"Close\"].pct_change(5)\n",
    "dia[\"Rolling_Return_21d\"] = dia[\"Close\"].pct_change(21)\n",
    "dia[\"Volatility_5d\"] = dia[\"Return_1d\"].rolling(window=5).std()\n",
    "dia[\"Volatility_21d\"] = dia[\"Return_1d\"].rolling(window=21).std()\n",
    "dia[\"Drawdown\"] = dia[\"Close\"] / dia[\"Close\"].cummax() - 1\n",
    "\n",
    "dia[\"SMA_20\"] = ta.trend.sma_indicator(dia[\"Close\"], window=20)\n",
    "dia[\"SMA_50\"] = ta.trend.sma_indicator(dia[\"Close\"], window=50)\n",
    "dia[\"EMA_12\"] = ta.trend.ema_indicator(dia[\"Close\"], window=12)\n",
    "dia[\"EMA_26\"] = ta.trend.ema_indicator(dia[\"Close\"], window=26)\n",
    "dia[\"MACD\"] = ta.trend.macd_diff(dia[\"Close\"])\n",
    "dia[\"RSI_14\"] = ta.momentum.rsi(dia[\"Close\"], window=14)\n",
    "\n",
    "bb = ta.volatility.BollingerBands(close=dia[\"Close\"], window=20)\n",
    "dia[\"BB_high\"] = bb.bollinger_hband()\n",
    "dia[\"BB_low\"] = bb.bollinger_lband()\n",
    "atr = ta.volatility.AverageTrueRange(high=dia[\"Close\"] * 1.01,\n",
    "                                     low=dia[\"Close\"] * 0.99,\n",
    "                                     close=dia[\"Close\"])\n",
    "dia[\"ATR_14\"] = atr.average_true_range()\n",
    "\n",
    "# Reset index\n",
    "dia.reset_index(inplace=True)\n",
    "\n",
    "# Load SHAP metrics\n",
    "shap_df = pd.read_csv(\"shap_value_metrics_export_DIA.csv\")\n",
    "shap_df[\"End_Date\"] = pd.to_datetime(shap_df[\"End_Date\"])\n",
    "\n",
    "# Merge\n",
    "merged = pd.merge(shap_df, dia, how=\"left\", left_on=\"End_Date\", right_on=\"Date\")\n",
    "merged.drop(columns=[\"Date\"], inplace=True)\n",
    "merged.to_csv(\"shap_with_dia_indicators.csv\", index=False)\n",
    "print(\"âœ… Saved: shap_with_dia_indicators.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e88f83f1f24009e2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e2d25f9c6fc7b2a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta\n",
    "\n",
    "# Load DIA price data (replace with your actual path)\n",
    "etf = pd.read_csv(\"stock_prices_DIA.csv\", parse_dates=[\"Date\"])\n",
    "etf.set_index(\"Date\", inplace=True)\n",
    "\n",
    "dia = etf[[stock]].copy()  # âœ… returns DataFrame\n",
    "dia.rename(columns={stock: \"Close\"}, inplace=True)  # Rename to 'Close'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0a2e47fa2677f1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 1: Convert test factors to NumPy and reshape\n",
    "test_factors_np = train_factors_norm.values.astype('float32').reshape(243, rolling_window_test, 6)\n",
    "\n",
    "d_model = test_factors_np.shape[2]  # Number of features\n",
    "\n",
    "# Step 2: Prepare background data for SHAP (using training factors)\n",
    "# background_data = train_factors_norm.values.astype('float32').reshape(-1, rolling_window_test, d_model)\n",
    "# background_data_tensor = torch.tensor(background_data, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "# Step 3: Ensure background_data is a PyTorch tensor\n",
    "# if not isinstance(background_data, torch.Tensor):\n",
    "#     background_data_tensor = torch.tensor(background_data, dtype=torch.float32).to(device)\n",
    "\n",
    "train_factors_np = train_factors_norm.values.astype('float32')\n",
    "background_windows = [\n",
    "    test_factors_np[i:i + rolling_window_test]\n",
    "    for i in range(len(test_factors_np) - rolling_window_test + 1)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67f63751cd318e95",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "background_windows = [\n",
    "    test_factors_np[i:i + rolling_window_test]\n",
    "    for i in range(len(test_factors_np) - rolling_window_test + 1)\n",
    "]\n",
    "\n",
    "background_data = np.stack(background_windows[:10])  # limit to 10 for SHAP speed\n",
    "background_data_tensor = torch.tensor(background_data, dtype=torch.float32).to(device)\n",
    "# background_windows = []\n",
    "# train_factors_np = train_factors_norm.values.astype('float32')\n",
    "# for i in range(0, len(train_factors_np) - rolling_window_test + 1):\n",
    "#     window = train_factors_np[i: i + rolling_window_test]\n",
    "#     background_windows.append(window)\n",
    "#\n",
    "# background_data_tensor = torch.tensor(np.stack(background_windows[:10]), dtype=torch.float32).to(device)\n",
    "\n",
    "# Step 4: Initialize SHAP GradientExplainer\n",
    "explainer = shap.GradientExplainer(model, background_data_tensor)\n",
    "\n",
    "# Step 5: Compute SHAP values for test features\n",
    "shap_values = explainer.shap_values(torch.tensor(test_factors_np, dtype=torch.float32).to(device))\n",
    "\n",
    "# Step 6: Process SHAP values (squeeze unnecessary dimensions)\n",
    "shap_values_squeezed = np.squeeze(shap_values[0])  # Shape: (rolling_window_test, d_model)\n",
    "\n",
    "# Step 7: Normalize SHAP values using training statistics\n",
    "shap_values_normalized = (shap_values_squeezed - train_factors_mean.values) / train_factors_std.values\n",
    "\n",
    "# Step 8: Concatenate normalized SHAP values with test features\n",
    "augmented_features = np.concatenate([test_factors_np[0], shap_values_normalized],\n",
    "                                    axis=1)  # Shape: (rolling_window_test, d_model * 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50e3ec644b57aa86"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_factors_norm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2195ee8ea5f7cd6b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "background_data_tensor = torch.tensor(np.stack(background_windows[:25]), dtype=torch.float32).to(device)\n",
    "background_data_tensor.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37dadd6091cc8cc9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 4: Initialize SHAP GradientExplainer\n",
    "explainer = shap.GradientExplainer(model, background_data_tensor)\n",
    "\n",
    "# Step 5: Compute SHAP values for test features\n",
    "shap_values = explainer.shap_values(\n",
    "    torch.tensor(test_factors_np, dtype=torch.float32).to(device)\n",
    ")\n",
    "\n",
    "# Step 6: Process SHAP values (squeeze unnecessary dimensions)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef2fd6762b80ec15",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "shap_values_squeezed = np.squeeze(shap_values[0])  # Shape: (rolling_window_test, d_model)\n",
    "shap_values_squeezed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92bda9fb23a2b6b6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 7: Normalize SHAP values using training statistics\n",
    "shap_values_normalized = (shap_values_squeezed - train_factors_mean.values) / train_factors_std.values\n",
    "\n",
    "# Step 8: Concatenate normalized SHAP values with test features\n",
    "augmented_features = np.concatenate([test_factors_np[0], shap_values_normalized],\n",
    "                                    axis=1)  # Shape: (rolling_window_test, d_model * 2)\n",
    "\n",
    "\n",
    "# Compute SHAP metrics\n",
    "shap_metrics = {\n",
    "    \"mean_abs_shap\": np.abs(shap_values_squeezed).mean(axis=0),\n",
    "    \"mean_shap\": shap_values_squeezed.mean(axis=0),\n",
    "    \"shap_std\": np.var(shap_values_squeezed, axis=0),\n",
    "    \"mean_over_std\": shap_values_squeezed.mean(axis=0) / np.std(shap_values_squeezed, axis=0),\n",
    "    \"mean_abs_over_std\": np.abs(shap_values_squeezed).mean(axis=0) / np.std(shap_values_squeezed, axis=0),\n",
    "    \"feature_importance_ranking\": np.argsort(-np.abs(shap_values_squeezed).mean(axis=0)),\n",
    "    \"shap_value_distribution\": shap_values_squeezed,\n",
    "    \"top_factors_per_sample\": np.argsort(-np.abs(shap_values_squeezed), axis=1)[:, :3],\n",
    "    \"shap_interaction_values\": explainer.shap_interaction_values(background_data_tensor) if hasattr(explainer,  'shap_interaction_values') else None,\n",
    "    \"cumulative_shap_impact\": np.sum(np.abs(shap_values_squeezed), axis=1),\n",
    "    # \"shap_feature_correlation\": np.corrcoef(test_factors_np.T, shap_values_squeezed.T)[:d_model, d_model:],\n",
    "    \"shap_trend\": np.mean(shap_values_squeezed, axis=1),\n",
    "}\n",
    "\n",
    "metrics_flat = {}\n",
    "for key in [\"mean_abs_shap\", \"mean_shap\", \"shap_std\", \"mean_over_std\",\"mean_abs_over_std\", \"feature_importance_ranking\"]:\n",
    "    for i, value in enumerate(shap_metrics[key]):\n",
    "        metrics_flat[f\"{key}_{i + 1}\"] = value\n",
    "shap_metrics_df = pd.DataFrame([metrics_flat])\n",
    "shap_metrics_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d0f6fd1253bd769",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "shap_values_squeezed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e7d4fb72d8039c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 1: Convert test factors to NumPy and reshape\n",
    "test_factors_np = test_factors_norm.values.astype('float32').reshape(1, rolling_window_test, -1)\n",
    "d_model = test_factors_np.shape[2]  # Number of features\n",
    "\n",
    "# Step 2: Prepare background data for SHAP (using training factors)\n",
    "# background_data = train_factors_norm.values.astype('float32').reshape(-1, rolling_window_test, d_model)\n",
    "# background_data_tensor = torch.tensor(background_data, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "# Step 3: Ensure background_data is a PyTorch tensor\n",
    "# if not isinstance(background_data, torch.Tensor):\n",
    "#     background_data_tensor = torch.tensor(background_data, dtype=torch.float32).to(device)\n",
    "\n",
    "background_windows = []\n",
    "train_factors_np = train_factors_norm.values.astype('float32')\n",
    "for i in range(0, len(train_factors_np) - rolling_window_test + 1):\n",
    "    window = train_factors_np[i: i + rolling_window_test]\n",
    "    background_windows.append(window)\n",
    "\n",
    "background_data_tensor = torch.tensor(np.stack(background_windows[:10]), dtype=torch.float32).to(device)\n",
    "\n",
    "# Step 4: Initialize SHAP GradientExplainer\n",
    "explainer = shap.GradientExplainer(model, background_data_tensor)\n",
    "\n",
    "# Step 5: Compute SHAP values for test features\n",
    "shap_values = explainer.shap_values(torch.tensor(test_factors_np, dtype=torch.float32).to(device))\n",
    "\n",
    "# Step 6: Process SHAP values (squeeze unnecessary dimensions)\n",
    "shap_values_squeezed = np.squeeze(shap_values[0], axis=(0, -1))  # Shape: (rolling_window_test, d_model)\n",
    "\n",
    "# Step 7: Normalize SHAP values using training statistics\n",
    "shap_values_normalized = (shap_values_squeezed - train_factors_mean.values) / train_factors_std.values\n",
    "\n",
    "# Step 8: Concatenate normalized SHAP values with test features\n",
    "augmented_features = np.concatenate([test_factors_np[0], shap_values_normalized],\n",
    "                                    axis=1)  # Shape: (rolling_window_test, d_model * 2)\n",
    "\n",
    "# Compute SHAP metrics\n",
    "shap_metrics = {\n",
    "    \"mean_abs_shap\": np.abs(shap_values_squeezed).mean(axis=0),\n",
    "    \"mean_shap\": shap_values_squeezed.mean(axis=0),\n",
    "    \"shap_std\": np.var(shap_values_squeezed, axis=0),\n",
    "    \"mean_over_std\": shap_values_squeezed.mean(axis=0) / np.std(shap_values_squeezed, axis=0),\n",
    "    \"mean_abs_over_std\": np.abs(shap_values_squeezed).mean(axis=0) / np.std(shap_values_squeezed, axis=0),\n",
    "    \"feature_importance_ranking\": np.argsort(-np.abs(shap_values_squeezed).mean(axis=0)),\n",
    "    \"shap_value_distribution\": shap_values_squeezed,\n",
    "    \"top_factors_per_sample\": np.argsort(-np.abs(shap_values_squeezed), axis=1)[:, :3],\n",
    "    \"shap_interaction_values\": explainer.shap_interaction_values(background_data_tensor) if hasattr(explainer,  'shap_interaction_values') else None,\n",
    "    \"cumulative_shap_impact\": np.sum(np.abs(shap_values_squeezed), axis=1),\n",
    "    # \"shap_feature_correlation\": np.corrcoef(test_factors_np.T, shap_values_squeezed.T)[:d_model, d_model:],\n",
    "    \"shap_trend\": np.mean(shap_values_squeezed, axis=1),\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "982afddd598f664b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "shap_values[0].shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51705b29adaba7ab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "(train_returns - train_returns_mean) / train_returns_std"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a671a324127faed2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_returns_std"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "165b1a2a350ee13d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "returns_raw"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa7c5f89801913fe",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "returns_raw_Y[[stock]]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad1f4bc35a137220",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import ta\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "# Load data\n",
    "factors = pd.read_csv(\"aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns = pd.read_csv(\"daily_returns_10ETFs.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Align dates\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors = factors.loc[dates]\n",
    "returns = returns.loc[dates]\n",
    "\n",
    "# Compute expanded generic technical indicators\n",
    "technical_features = pd.DataFrame(index=returns.index)\n",
    "for etf in returns.columns:\n",
    "    close = (1 + returns[etf]).cumprod()\n",
    "    technical_features[f'{etf}_SMA_5'] = ta.trend.sma_indicator(close, window=5)\n",
    "    technical_features[f'{etf}_SMA_20'] = ta.trend.sma_indicator(close, window=20)\n",
    "    technical_features[f'{etf}_SMA_50'] = ta.trend.sma_indicator(close, window=50)\n",
    "    technical_features[f'{etf}_EMA_12'] = ta.trend.ema_indicator(close, window=12)\n",
    "    technical_features[f'{etf}_EMA_26'] = ta.trend.ema_indicator(close, window=26)\n",
    "    technical_features[f'{etf}_EMA_50'] = ta.trend.ema_indicator(close, window=50)\n",
    "    technical_features[f'{etf}_RSI_7'] = ta.momentum.rsi(close, window=7)\n",
    "    technical_features[f'{etf}_RSI_14'] = ta.momentum.rsi(close, window=14)\n",
    "    technical_features[f'{etf}_MACD'] = ta.trend.macd_diff(close)\n",
    "    technical_features[f'{etf}_ATR'] = ta.volatility.average_true_range(high=close*1.01, low=close*0.99, close=close, window=14)\n",
    "    technical_features[f'{etf}_Volatility_5'] = returns[etf].rolling(window=5).std()\n",
    "    technical_features[f'{etf}_Volatility_20'] = returns[etf].rolling(window=20).std()\n",
    "    technical_features[f'{etf}_Volatility_50'] = returns[etf].rolling(window=50).std()\n",
    "    technical_features[f'{etf}_Momentum_3'] = returns[etf].rolling(window=3).mean()\n",
    "    technical_features[f'{etf}_Momentum_10'] = returns[etf].rolling(window=10).mean()\n",
    "\n",
    "# Combine original factors with technical indicators\n",
    "features = pd.concat([factors, technical_features], axis=1).dropna()\n",
    "\n",
    "# Shift target by 1 day for next-day prediction\n",
    "target_returns = returns.shift(-1).loc[features.index].dropna()\n",
    "features = features.loc[target_returns.index]\n",
    "\n",
    "# Define rolling window parameters\n",
    "train_years = 10\n",
    "valid_years = 2\n",
    "start_year = 1999\n",
    "end_year = 2024\n",
    "\n",
    "# Identify expanded generic feature names\n",
    "generic_features = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA',\n",
    "                    'SMA_5', 'SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', 'EMA_50',\n",
    "                    'RSI_7', 'RSI_14', 'MACD', 'ATR', 'Volatility_5', 'Volatility_20', 'Volatility_50',\n",
    "                    'Momentum_3', 'Momentum_10']\n",
    "\n",
    "# Step 1: Determine common generic important features\n",
    "shap_importances_generic = pd.DataFrame(0, index=generic_features, columns=['SHAP_Value'])\n",
    "\n",
    "for etf in returns.columns:\n",
    "    print(f\"Computing SHAP for generic feature selection ETF: {etf}\")\n",
    "    train_start = pd.Timestamp(2009 - train_years, 1, 1)\n",
    "    train_end = pd.Timestamp(2009 - valid_years - 1, 12, 31)\n",
    "\n",
    "    X_train = features.loc[train_start:train_end, [col for col in features.columns if etf in col or col in factors.columns]]\n",
    "    y_train = target_returns[etf].loc[train_start:train_end]\n",
    "\n",
    "    model = xgb.XGBRegressor(tree_method='hist', device='cuda').fit(X_train, y_train)\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_train)\n",
    "\n",
    "    for generic in generic_features:\n",
    "        cols = [col for col in X_train.columns if generic in col]\n",
    "        shap_importances_generic.loc[generic] += np.mean(np.abs(shap_values.values[:, [X_train.columns.get_loc(c) for c in cols]]))\n",
    "\n",
    "shap_importances_generic /= len(returns.columns)\n",
    "common_generic_features = shap_importances_generic.sort_values('SHAP_Value', ascending=False).head(30).index.tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b007b4191668854",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "common_generic_features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63ddbb6b8e92ee00",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Step 2: Retrain models using generic common important features\n",
    "shap_results = []\n",
    "predictions_results = []\n",
    "\n",
    "for etf in returns.columns[0:1]:\n",
    "    print(f\"Processing ETF: {etf}\")\n",
    "    etf_specific_features = [f for f in features.columns if any(generic in f for generic in common_generic_features) or f in factors.columns]\n",
    "\n",
    "    for year in range(2009, 2010):\n",
    "        print(f\"Processing year: {year}\")\n",
    "        train_start = pd.Timestamp(year - train_years, 1, 1)\n",
    "        train_end = pd.Timestamp(year - valid_years - 1, 12, 31)\n",
    "        valid_start = pd.Timestamp(year - valid_years, 1, 1)\n",
    "        valid_end = pd.Timestamp(year - 1, 12, 31)\n",
    "        test_start = pd.Timestamp(year, 1, 1)\n",
    "        test_end = pd.Timestamp(year, 12, 31)\n",
    "\n",
    "        X_train = features.loc[train_start:train_end, etf_specific_features]\n",
    "        y_train = target_returns[etf].loc[train_start:train_end]\n",
    "\n",
    "        X_valid = features.loc[valid_start:valid_end, etf_specific_features]\n",
    "        y_valid = target_returns[etf].loc[valid_start:valid_end]\n",
    "\n",
    "        X_test = features.loc[test_start:test_end, etf_specific_features]\n",
    "        y_test = target_returns[etf].loc[test_start:test_end]\n",
    "\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42,\n",
    "            tree_method='hist',\n",
    "            device='cuda'\n",
    "        )\n",
    "\n",
    "        params = {'n_estimators': [100, 200], 'max_depth': [3, 4], 'learning_rate': [0.01, 0.05]}\n",
    "\n",
    "        grid_search = GridSearchCV(xgb_model, params, cv=TimeSeriesSplit(n_splits=3), scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=True)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "        daily_preds = best_model.predict(X_test)\n",
    "        \n",
    "        y_true = y_test.values\n",
    "        y_pred = daily_preds\n",
    "        \n",
    "        # Compute metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        directional_accuracy = np.mean((np.sign(y_true) == np.sign(y_pred)).astype(int))\n",
    "        \n",
    "        print(f\"MSE: {mse:.6f}\")\n",
    "        print(f\"RMSE: {rmse:.6f}\")\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        print(f\"R-squared: {r2:.6f}\")\n",
    "        print(f\"Directional Accuracy: {directional_accuracy:.2%}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3962022f090bac6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "common_generic_features[0:10]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b2804a86ff5b35d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "895e7096145459d4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# After prediction loop, example for a single test period:\n",
    "y_true = y_test.values\n",
    "y_pred = daily_preds\n",
    "\n",
    "# Compute metrics\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "directional_accuracy = np.mean((np.sign(y_true) == np.sign(y_pred)).astype(int))\n",
    "\n",
    "print(f\"MSE: {mse:.6f}\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"MAE: {mae:.6f}\")\n",
    "print(f\"R-squared: {r2:.6f}\")\n",
    "print(f\"Directional Accuracy: {directional_accuracy:.2%}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69cd27558c90ed15",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "preds_df = pd.DataFrame({\"Date\": X_test.index, \"ETF\": etf, \"Actual\": y_test, \"Predicted\": daily_preds})\n",
    "predictions_results.append(preds_df)\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.Explainer(best_model)\n",
    "shap_values = explainer(X_test)\n",
    "shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "important_features = np.array(feature_cols)[shap_importance.argsort()[::-1][:30]]  # top 30 features\n",
    "important_features\n",
    "X_train_selected = X_train[important_features]\n",
    "X_valid_selected = X_valid[important_features]\n",
    "X_test_selected = X_test[important_features]\n",
    "\n",
    "X_valid_selected = X_valid[important_features]\n",
    "\n",
    "grid_search.fit(\n",
    "    X_train_selected, y_train,\n",
    "    eval_set=[(X_valid_selected, y_valid)],  # <-- explicitly include validation data\n",
    "    verbose=True\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e35d0d46e26465bc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_selected_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions explicitly using the selected features from X_test\n",
    "predictions_selected = best_selected_model.predict(X_test_selected)\n",
    "\n",
    "y_true = y_test.values\n",
    "y_pred = predictions_selected = best_selected_model.predict(X_test_selected)\n",
    "\n",
    "\n",
    "# Compute metrics\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "directional_accuracy = np.mean((np.sign(y_true) == np.sign(y_pred)).astype(int))\n",
    "\n",
    "print(f\"MSE: {mse:.6f}\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"MAE: {mae:.6f}\")\n",
    "print(f\"R-squared: {r2:.6f}\")\n",
    "print(f\"Directional Accuracy: {directional_accuracy:.2%}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce9debc73044cfd5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "# Load data\n",
    "factors = pd.read_csv(\"aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns = pd.read_csv(\"daily_returns_10ETFs.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Align dates\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors = factors.loc[dates]\n",
    "returns = returns.loc[dates]\n",
    "\n",
    "# Shift target by 1 day for next-day prediction\n",
    "target_returns = returns.shift(-1).dropna()\n",
    "factors = factors.loc[target_returns.index]\n",
    "\n",
    "# Shift target by 1 day for next-day prediction\n",
    "target_returns = returns.shift(-1).dropna()\n",
    "factors = factors.loc[target_returns.index]\n",
    "\n",
    "# Define rolling window parameters\n",
    "train_years = 10\n",
    "valid_years = 2\n",
    "start_year = 1999\n",
    "end_year = 2024\n",
    "\n",
    "# Placeholder for SHAP results\n",
    "shap_results = []\n",
    "\n",
    "# Feature and target columns\n",
    "feature_cols = factors.columns\n",
    "target_cols = target_returns.columns\n",
    "\n",
    "print(f'target_cols={target_cols}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adb5bfdfc9c419d8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8f0500c59fdc3167"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# stage 1 training and prediction \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import ta\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "# Load data\n",
    "factors = pd.read_csv(\"aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns = pd.read_csv(\"daily_returns_10ETFs.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Align dates\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors = factors.loc[dates]\n",
    "returns = returns.loc[dates]\n",
    "\n",
    "# Compute expanded generic technical indicators\n",
    "all_tech_features = []\n",
    "\n",
    "for etf in returns.columns:\n",
    "    close = (1 + returns[etf]).cumprod()\n",
    "    etf_tech_features = pd.DataFrame(index=returns.index)\n",
    "\n",
    "    etf_tech_features[f'{etf}_SMA_5'] = ta.trend.sma_indicator(close, window=5)\n",
    "    etf_tech_features[f'{etf}_SMA_20'] = ta.trend.sma_indicator(close, window=20)\n",
    "    etf_tech_features[f'{etf}_SMA_50'] = ta.trend.sma_indicator(close, window=50)\n",
    "    etf_tech_features[f'{etf}_EMA_12'] = ta.trend.ema_indicator(close, window=12)\n",
    "    etf_tech_features[f'{etf}_EMA_26'] = ta.trend.ema_indicator(close, window=26)\n",
    "    etf_tech_features[f'{etf}_EMA_50'] = ta.trend.ema_indicator(close, window=50)\n",
    "    etf_tech_features[f'{etf}_RSI_7'] = ta.momentum.rsi(close, window=7)\n",
    "    etf_tech_features[f'{etf}_RSI_14'] = ta.momentum.rsi(close, window=14)\n",
    "    etf_tech_features[f'{etf}_MACD'] = ta.trend.macd_diff(close)\n",
    "    etf_tech_features[f'{etf}_ATR'] = ta.volatility.average_true_range(high=close*1.01, low=close*0.99, close=close, window=14)\n",
    "    etf_tech_features[f'{etf}_Volatility_5'] = returns[etf].rolling(window=5).std()\n",
    "    etf_tech_features[f'{etf}_Volatility_20'] = returns[etf].rolling(window=20).std()\n",
    "    etf_tech_features[f'{etf}_Volatility_50'] = returns[etf].rolling(window=50).std()\n",
    "    etf_tech_features[f'{etf}_Momentum_3'] = returns[etf].rolling(window=3).mean()\n",
    "    etf_tech_features[f'{etf}_Momentum_10'] = returns[etf].rolling(window=10).mean()\n",
    "\n",
    "    all_tech_features.append(etf_tech_features)\n",
    "\n",
    "# Concatenate all ETF technical features at once to prevent DataFrame fragmentation\n",
    "technical_features = pd.concat(all_tech_features, axis=1)\n",
    "\n",
    "# Combine original factors with technical indicators\n",
    "features = pd.concat([factors, technical_features], axis=1).dropna()\n",
    "\n",
    "# Shift target by 1 day for next-day prediction\n",
    "target_returns = returns.shift(-1).loc[features.index].dropna()\n",
    "features = features.loc[target_returns.index]\n",
    "\n",
    "# Define rolling window parameters\n",
    "train_years = 10          # Length of training data in years\n",
    "valid_years = 2           # Length of validation data in years\n",
    "test_years = 1            # Length of testing/prediction data in years (configurable)\n",
    "retrain_frequency = 1     # Retrain model every N years (configurable)\n",
    "start_year = 2009\n",
    "end_year = 2024\n",
    "\n",
    "# Generic feature names\n",
    "all_generic_features = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA',\n",
    "                        'SMA_5', 'SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', 'EMA_50',\n",
    "                        'RSI_7', 'RSI_14', 'MACD', 'ATR', 'Volatility_5', 'Volatility_20', 'Volatility_50',\n",
    "                        'Momentum_3', 'Momentum_10']\n",
    "\n",
    "# Step 1: Determine top N generic important features using aggregated SHAP across all ETFs\n",
    "shap_importances = pd.DataFrame(0.0, index=all_generic_features, columns=['SHAP_Value'])\n",
    "\n",
    "for etf in returns.columns:\n",
    "    print(f\"Computing SHAP for ETF: {etf}\")\n",
    "    train_start = pd.Timestamp(2009 - train_years, 1, 1)\n",
    "    train_end = pd.Timestamp(2009 - valid_years - 1, 12, 31)\n",
    "    \n",
    "    etf_features = [col for col in features.columns if etf in col or col in factors.columns]\n",
    "    X_train = features.loc[train_start:train_end, etf_features]\n",
    "    y_train = target_returns[etf].loc[train_start:train_end]\n",
    "\n",
    "    model = xgb.XGBRegressor(tree_method='hist', device='cuda').fit(X_train, y_train)\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_train)\n",
    "\n",
    "    for generic in all_generic_features:\n",
    "        cols = [col for col in X_train.columns if generic in col]\n",
    "        if cols:\n",
    "            idx = [X_train.columns.get_loc(c) for c in cols]\n",
    "            shap_importances.loc[generic] += np.mean(np.abs(shap_values.values[:, idx]))\n",
    "\n",
    "shap_importances /= len(returns.columns)\n",
    "common_generic_features = shap_importances.sort_values('SHAP_Value', ascending=False).head(10).index.tolist()\n",
    "\n",
    "# Step 2: Retrain models using selected top generic important features\n",
    "all_predictions = []\n",
    "for etf in returns.columns:  # Adjust this slice for all ETFs\n",
    "    \n",
    "    print(f\"Processing ETF: {etf}\")\n",
    "    selected_features = [f for f in features.columns if any(generic in f for generic in common_generic_features) or f in factors.columns]\n",
    "\n",
    "    # for year in range(2009, 2010):  # Adjust range for all years\n",
    "    year = start_year\n",
    "    while year <= end_year - test_years + 1:\n",
    "        print(f\"\\nTraining window starting year: {year}\")\n",
    "        start_time = time.time()\n",
    "        train_start = pd.Timestamp(year - train_years, 1, 1)\n",
    "        train_end = pd.Timestamp(year - valid_years - 1, 12, 31)\n",
    "        valid_start = pd.Timestamp(year - valid_years, 1, 1)\n",
    "        valid_end = pd.Timestamp(year - 1, 12, 31)\n",
    "        test_start = pd.Timestamp(year, 1, 1)\n",
    "        test_end = pd.Timestamp(year + test_years - 1, 12, 31)\n",
    "\n",
    "        X_train = features.loc[train_start:train_end, selected_features]\n",
    "        y_train = target_returns[etf].loc[train_start:train_end]\n",
    "\n",
    "        X_valid = features.loc[valid_start:valid_end, selected_features]\n",
    "        y_valid = target_returns[etf].loc[valid_start:valid_end]\n",
    "\n",
    "        X_test = features.loc[test_start:test_end, selected_features]\n",
    "        y_test = target_returns[etf].loc[test_start:test_end]\n",
    "\n",
    "        model = xgb.XGBRegressor(\n",
    "            tree_method='hist',\n",
    "            device='cuda',\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42,\n",
    "            n_jobs=4\n",
    "        )\n",
    "\n",
    "        params = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'learning_rate': [0.01, 0.03, 0.05],\n",
    "            'subsample': [0.7, 0.8],\n",
    "            'colsample_bytree': [0.7, 0.8]\n",
    "        }\n",
    "\n",
    "        grid_search = GridSearchCV(model, params, cv=TimeSeriesSplit(3), scoring='neg_mean_squared_error', verbose=1, n_jobs=4)\n",
    "        grid_search.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "       \n",
    "        # Convert test data explicitly to DMatrix for GPU\n",
    "        dtest = xgb.DMatrix(X_test, enable_categorical=False)\n",
    "        \n",
    "        # Make predictions using the best_model explicitly\n",
    "        preds = best_model.get_booster().predict(dtest)\n",
    "\n",
    "\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "        directional_accuracy = np.mean((np.sign(y_test) == np.sign(preds)).astype(int))\n",
    "\n",
    "        print(f\"MSE: {mse:.6f}, RMSE: {rmse:.6f}, MAE: {mae:.6f}, R2: {r2:.6f}, Directional Accuracy: {directional_accuracy:.2%}\")\n",
    "        joblib.dump(best_model, f\"best_model_{etf}_{year}.joblib\")\n",
    "\n",
    "        # Save daily predictions\n",
    "        predictions_df = pd.DataFrame({'Date': X_test.index, 'ETF': etf, 'Year': year, \n",
    "                                       'Actual_Return': y_test, 'Predicted_Return': preds})\n",
    "\n",
    "        # SHAP values explicitly aligned\n",
    "        explainer_test = shap.Explainer(best_model)\n",
    "        shap_values_test = explainer_test(X_test)\n",
    "        \n",
    "        # Create SHAP DataFrame explicitly with 'Date' to ensure correct merging\n",
    "        shap_df = pd.DataFrame(\n",
    "            shap_values_test.values,\n",
    "            columns=[f'SHAP_{col}' for col in X_test.columns],\n",
    "            index=X_test.index\n",
    "        ).reset_index().rename(columns={'index': 'Date'})\n",
    "        \n",
    "        # Ensure predictions_df has 'Date' column explicitly for merging\n",
    "        predictions_df = predictions_df.reset_index(drop=True)\n",
    "        \n",
    "        # Merge explicitly by 'Date' to align SHAP values correctly\n",
    "        predictions_df = pd.merge(predictions_df, shap_df, on='Date', how='left')\n",
    "        \n",
    "        # Append explicitly for each ETF-year combination\n",
    "        all_predictions.append(predictions_df)\n",
    "        year += retrain_frequency\n",
    "        end_time = time.time()\n",
    "        print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "final_predictions_df = pd.concat(all_predictions).reset_index(drop=True)\n",
    "final_predictions_df.to_csv(\"stage1_predictions_with_shap.csv\", index=False)\n",
    "\n",
    "print(\"Stage 1 completed and data saved for Stage 2.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "110622934baf18b4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import ta\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "# Load data\n",
    "factors = pd.read_csv(\"aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns = pd.read_csv(\"daily_returns_10ETFs.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Align dates\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors = factors.loc[dates]\n",
    "returns = returns.loc[dates]\n",
    "\n",
    "# Compute expanded generic technical indicators\n",
    "all_tech_features = []\n",
    "\n",
    "for etf in returns.columns:\n",
    "    close = (1 + returns[etf]).cumprod()\n",
    "    etf_tech_features = pd.DataFrame(index=returns.index)\n",
    "\n",
    "    etf_tech_features[f'{etf}_SMA_5'] = ta.trend.sma_indicator(close, window=5)\n",
    "    etf_tech_features[f'{etf}_SMA_20'] = ta.trend.sma_indicator(close, window=20)\n",
    "    etf_tech_features[f'{etf}_SMA_50'] = ta.trend.sma_indicator(close, window=50)\n",
    "    etf_tech_features[f'{etf}_EMA_12'] = ta.trend.ema_indicator(close, window=12)\n",
    "    etf_tech_features[f'{etf}_EMA_26'] = ta.trend.ema_indicator(close, window=26)\n",
    "    etf_tech_features[f'{etf}_EMA_50'] = ta.trend.ema_indicator(close, window=50)\n",
    "    etf_tech_features[f'{etf}_RSI_7'] = ta.momentum.rsi(close, window=7)\n",
    "    etf_tech_features[f'{etf}_RSI_14'] = ta.momentum.rsi(close, window=14)\n",
    "    etf_tech_features[f'{etf}_MACD'] = ta.trend.macd_diff(close)\n",
    "    etf_tech_features[f'{etf}_ATR'] = ta.volatility.average_true_range(high=close*1.01, low=close*0.99, close=close, window=14)\n",
    "    etf_tech_features[f'{etf}_Volatility_5'] = returns[etf].rolling(window=5).std()\n",
    "    etf_tech_features[f'{etf}_Volatility_20'] = returns[etf].rolling(window=20).std()\n",
    "    etf_tech_features[f'{etf}_Volatility_50'] = returns[etf].rolling(window=50).std()\n",
    "    etf_tech_features[f'{etf}_Momentum_3'] = returns[etf].rolling(window=3).mean()\n",
    "    etf_tech_features[f'{etf}_Momentum_10'] = returns[etf].rolling(window=10).mean()\n",
    "\n",
    "    all_tech_features.append(etf_tech_features)\n",
    "\n",
    "# Concatenate all ETF technical features at once to prevent DataFrame fragmentation\n",
    "technical_features = pd.concat(all_tech_features, axis=1)\n",
    "\n",
    "# Combine original factors with technical indicators\n",
    "features = pd.concat([factors, technical_features], axis=1).dropna()\n",
    "\n",
    "# Shift target by 1 day for next-day prediction\n",
    "target_returns = returns.shift(-1).loc[features.index].dropna()\n",
    "features = features.loc[target_returns.index]\n",
    "\n",
    "# Define rolling window parameters\n",
    "train_years = 10          # Length of training data in years\n",
    "valid_years = 2           # Length of validation data in years\n",
    "test_years = 1            # Length of testing/prediction data in years (configurable)\n",
    "retrain_frequency = 1     # Retrain model every N years (configurable)\n",
    "start_year = 2009\n",
    "end_year = 2024\n",
    "\n",
    "# Generic feature names\n",
    "all_generic_features = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA',\n",
    "                        'SMA_5', 'SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', 'EMA_50',\n",
    "                        'RSI_7', 'RSI_14', 'MACD', 'ATR', 'Volatility_5', 'Volatility_20', 'Volatility_50',\n",
    "                        'Momentum_3', 'Momentum_10']\n",
    "\n",
    "# Step 1: Determine top N generic important features using aggregated SHAP across all ETFs\n",
    "shap_importances = pd.DataFrame(0.0, index=all_generic_features, columns=['SHAP_Value'])\n",
    "\n",
    "for etf in returns.columns:\n",
    "    print(f\"Computing SHAP for ETF: {etf}\")\n",
    "    train_start = pd.Timestamp(2009 - train_years, 1, 1)\n",
    "    train_end = pd.Timestamp(2009 - valid_years - 1, 12, 31)\n",
    "    \n",
    "    etf_features = [col for col in features.columns if etf in col or col in factors.columns]\n",
    "    X_train = features.loc[train_start:train_end, etf_features]\n",
    "    y_train = target_returns[etf].loc[train_start:train_end]\n",
    "\n",
    "    model = xgb.XGBRegressor(tree_method='hist', device='cuda').fit(X_train, y_train)\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_train)\n",
    "\n",
    "    for generic in all_generic_features:\n",
    "        cols = [col for col in X_train.columns if generic in col]\n",
    "        if cols:\n",
    "            idx = [X_train.columns.get_loc(c) for c in cols]\n",
    "            shap_importances.loc[generic] += np.mean(np.abs(shap_values.values[:, idx]))\n",
    "\n",
    "shap_importances /= len(returns.columns)\n",
    "common_generic_features = shap_importances.sort_values('SHAP_Value', ascending=False).head(10).index.tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2eb4c9f8c79bcf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "common_generic_features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfaef7821b9ba427",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "125b6d6efc751134",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "85336205218912a7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load stage 1 data explicitly\n",
    "stage1_df = pd.read_csv(\"stage1_predictions_with_shap.csv\", parse_dates=['Date'])\n",
    "\n",
    "etfs = stage1_df['ETF'].unique()\n",
    "\n",
    "# Initialize DataFrame for daily aggregated data explicitly\n",
    "dates = sorted(stage1_df['Date'].unique())\n",
    "aggregated_data = pd.DataFrame({'Date': dates})\n",
    "\n",
    "# ETF-specific predicted returns, actual returns, and volatility\n",
    "for etf in etfs:\n",
    "    etf_data = stage1_df[stage1_df['ETF'] == etf].set_index('Date').sort_index()\n",
    "\n",
    "    aggregated_data[f'Predicted_Return_{etf}'] = aggregated_data['Date'].map(etf_data['Predicted_Return'])\n",
    "    aggregated_data[f'Actual_Return_{etf}'] = aggregated_data['Date'].map(etf_data['Actual_Return'])\n",
    "\n",
    "    # Explicit rolling 5-day volatility calculation\n",
    "    volatility = etf_data['Actual_Return'].rolling(window=5).std()\n",
    "    aggregated_data[f'Volatility_{etf}'] = aggregated_data['Date'].map(volatility)\n",
    "\n",
    "# Define explicitly generic SHAP features to aggregate across ETFs\n",
    "# generic_shap_features = [\n",
    "#     'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', \n",
    "#     'SMA_5', 'SMA_20', 'SMA_50',\n",
    "#     'EMA_12', 'EMA_26', 'EMA_50',\n",
    "#     'RSI_7', 'RSI_14', 'MACD', 'ATR',\n",
    "#     'Volatility_5', 'Volatility_20', 'Volatility_50',\n",
    "#     'Momentum_3', 'Momentum_10'\n",
    "# ]\n",
    "generic_shap_features = ['Mkt-RF',\n",
    " 'Volatility_50',\n",
    " 'HML',\n",
    " 'Momentum_3',\n",
    " 'Volatility_5',\n",
    " 'Volatility_20',\n",
    " 'RMW',\n",
    " 'SMB',\n",
    " 'CMA',\n",
    " 'RSI_7']\n",
    "\n",
    "# Aggregate SHAP values explicitly across ETFs by generic feature\n",
    "shap_aggregated = {'Date': dates}\n",
    "shap_df_list = []\n",
    "\n",
    "for feature in generic_shap_features:\n",
    "    # Adjust matching explicitly for 'SHAP_{ETF}_{feature}' format\n",
    "    matching_shap_cols = [col for col in stage1_df.columns if col.startswith('SHAP_') and col.endswith(f'_{feature}')]\n",
    "    \n",
    "    if matching_shap_cols:\n",
    "        # Compute daily mean explicitly across selected SHAP columns\n",
    "        daily_shap_mean = stage1_df.groupby('Date')[matching_shap_cols].mean().mean(axis=1)\n",
    "        shap_df_list.append(daily_shap_mean.rename(f'Avg_SHAP_{feature}'))\n",
    "    else:\n",
    "        print(f\"Warning: No matches found explicitly for feature: {feature}\")\n",
    "\n",
    "# Concatenate aggregated SHAP features explicitly, ensuring alignment\n",
    "shap_aggregated_df = pd.concat(shap_df_list, axis=1).reset_index()\n",
    "\n",
    "# Explicit merge with ETF-specific metrics on Date to ensure alignment\n",
    "aggregated_data = pd.merge(aggregated_data, shap_aggregated_df, on='Date', how='left')\n",
    "\n",
    "# Explicit handling of missing values\n",
    "aggregated_data.sort_values('Date', inplace=True)\n",
    "aggregated_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Remove rows explicitly if initial volatility calculations have NaNs\n",
    "vol_cols = [f'Volatility_{etf}' for etf in etfs]\n",
    "aggregated_data.dropna(subset=vol_cols, inplace=True)\n",
    "\n",
    "# Check if aggregated_data is empty before saving explicitly\n",
    "if aggregated_data.empty:\n",
    "    print(\"Warning: aggregated_data is empty after processing. Please verify input data and alignment explicitly.\")\n",
    "else:\n",
    "    aggregated_data.to_csv(\"stage2_rl_observations_optimized.csv\", index=False)\n",
    "    print(f\"Optimized Stage 2 RL dataset created with shape: {aggregated_data.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "626373434f64d0c4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d8f3f0813e659ead",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# start of stage 2 training\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar', risk_coefficient=0.5, rebalance_period=21, lookback_period=21):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(etf_list),), dtype=np.float32)\n",
    "\n",
    "        # Explicitly select feature columns (excluding Date and returns used only for calculating reward)\n",
    "        self.feature_cols = [col for col in data.columns if col not in ['Date'] and not col.startswith('Actual_Return')]\n",
    "        self.num_features_per_day = len(self.feature_cols)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.num_features_per_day * self.lookback_period,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(etf_list)] * len(etf_list))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(self.etf_list)] * len(self.etf_list))\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        next_step = self.current_step + 1\n",
    "\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            # v2 long short\n",
    "            desired_long = 1.20  # 120% long exposure explicitly\n",
    "            desired_short = 0.20  # 20% short exposure explicitly\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                # Normal 120/20 explicitly0\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                # Only long explicitly: default realistically to 100% long\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                # Only short explicitly (unrealistic), fallback clearly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                # All zeros explicitly: fallback explicitly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            # Apply explicit clipping\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate explicitly after clipping\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            # Final explicit normalization after clipping\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)  # exactly 100% long\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                # Realistic fallback explicitly: equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "            self.current_weights = final_weights\n",
    "            # v1 softmax normalization\n",
    "            # \n",
    "            # temperature = 0.5  # Explicitly lower for higher concentration (try 0.2 to 0.8)\n",
    "            # scaled_action = action / temperature\n",
    "            # self.current_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "\n",
    "        else:\n",
    "            returns_today = np.array([self.data.loc[self.current_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            self.current_weights *= (1 + returns_today)\n",
    "            self.current_weights /= np.sum(self.current_weights)\n",
    "\n",
    "        if next_step >= len(self.data):\n",
    "            terminated = True\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            returns = np.array([self.data.loc[next_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            portfolio_return = np.dot(self.current_weights, returns)\n",
    "            self.cumulative_wealth *= (1 + portfolio_return)\n",
    "            reward = self.calculate_reward(portfolio_return, returns)\n",
    "            terminated = next_step >= len(self.data) - 1\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "        # def _get_obs(self):\n",
    "        #     obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        #     obs_window = obs_window.drop(columns=['Date']).values.flatten().astype(np.float32)\n",
    "        #     return obs_window\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        obs_window = obs_window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "        return obs_window\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns):\n",
    "        if self.reward_type == 'cumulative_return':\n",
    "            return self.cumulative_wealth - 1.0\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            return np.log(self.cumulative_wealth)\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100 * alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            return portfolio_return - self.risk_coefficient * cvar\n",
    "        else:\n",
    "            raise ValueError('Invalid reward type')\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_stable_features(df, etf_list):\n",
    "    data = df.copy()\n",
    "\n",
    "    for etf in etf_list:\n",
    "        price_col = f'Price_{etf}'\n",
    "\n",
    "        # Volatility (20-day)\n",
    "        data[f'Volatility_{etf}'] = data[price_col].pct_change().rolling(20).std()\n",
    "\n",
    "        # Momentum indicators (returns over 5, 10, 20 days)\n",
    "        data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
    "        data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
    "        data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
    "\n",
    "        # Moving averages (5-day and 20-day)\n",
    "        data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
    "        data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
    "\n",
    "        # Moving average crossover (5-day MA - 20-day MA)\n",
    "        data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
    "\n",
    "    # Drop NaN values due to rolling calculations\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # Explicit patterns to identify columns\n",
    "    predicted_return_pattern = 'Predicted_Return'\n",
    "    shap_metric_pattern = 'SHAP'\n",
    "\n",
    "    # Exclude Predicted Returns explicitly if requested\n",
    "    if not include_predicted_returns:\n",
    "        predicted_cols = [col for col in df_filtered.columns if predicted_return_pattern in col]\n",
    "        df_filtered.drop(columns=predicted_cols, inplace=True)\n",
    "        print(f\"Excluded predicted return columns: {predicted_cols}\")\n",
    "\n",
    "    # Exclude SHAP-related metrics explicitly if requested\n",
    "    if not include_shap_metrics:\n",
    "        shap_cols = [col for col in df_filtered.columns if shap_metric_pattern in col]\n",
    "        df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "        print(f\"Excluded SHAP-related columns: {shap_cols}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# ETFs\n",
    "etf_list = ['XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLY', 'XLV', 'XLU']\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 5e-5],\n",
    "    'n_steps': [20, 40],\n",
    "    'batch_size': [5, 10],\n",
    "    'gamma': [0.98, 0.99]\n",
    "}\n",
    "consolidated_file = 'stage2_rl_observations_optimized.csv'\n",
    "reward_type = 'log_wealth'\n",
    "# data = pd.read_csv(consolidated_file, parse_dates=['Date'])\n",
    "# data = data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "data = pd.read_csv('stage2_rl_observations_optimized.csv', parse_dates=['Date'])\n",
    "price_data = pd.read_csv('stock_prices_10ETFs.csv')\n",
    "\n",
    "# Convert the Date column in price data, handling the timezone correctly\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'], utc=True)\n",
    "price_data['Date'] = price_data['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Rename price columns explicitly to 'price_{ticker}'\n",
    "price_cols = {col: f'Price_{col}' for col in price_data.columns if col != 'Date'}\n",
    "price_data.rename(columns=price_cols, inplace=True)\n",
    "\n",
    "# Merge datasets on Date\n",
    "merged_data = pd.merge(data, price_data, on='Date', how='inner')\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "# Check if merge was successful\n",
    "if len(merged_data) != len(data):\n",
    "    print(f\"Warning: Data length mismatch after merging (Original: {len(data)}, Merged: {len(merged_data)}).\")\n",
    "else:\n",
    "    print(\"Merged successfully with aligned dates.\")\n",
    "\n",
    "data_with_features_raw = add_stable_features(merged_data, etf_list)\n",
    "data_with_features_raw.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Usage Example clearly for benchmark (only price metrics, no predicted return or SHAP):\n",
    "data_with_features = filter_features(data_with_features_raw, \n",
    "                                 include_predicted_returns=True, \n",
    "                                 include_shap_metrics=True)\n",
    "################### override data to use SHAP only\n",
    "# data_with_features = data\n",
    "################### END override \n",
    "\n",
    "# Define your rolling window lengths clearly:\n",
    "train_window_days = 252 * 7\n",
    "validation_window_days = 252\n",
    "prediction_window_days = 252\n",
    "lookback_period = 21\n",
    "rebalance_period = 21\n",
    "\n",
    "start_indices = range(0, len(data) - (train_window_days + validation_window_days + prediction_window_days), prediction_window_days)\n",
    "all_weights = []\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "def validate_and_tune(train_data, val_data, reward_type, rebalance_period=10, lookback_period=10, n_iter=8, timesteps=5000):\n",
    "    best_reward, best_params = -np.inf, None\n",
    "\n",
    "    # Narrow and meaningful parameter distribution\n",
    "    param_dist = {\n",
    "        'learning_rate': [3e-4, 1e-4],\n",
    "        'n_steps': [20, 40],\n",
    "        'batch_size': [10, 20],\n",
    "        'gamma': [0.95, 0.98],\n",
    "        'risk_coefficient': [0.1, 0.5, 1.0] if reward_type in ['mean_var', 'mean_cvar'] else [0.5],\n",
    "    }\n",
    "\n",
    "    sampled_params = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=42))\n",
    "\n",
    "    for params in sampled_params:\n",
    "        risk_coeff = params.pop('risk_coefficient', 0.5)\n",
    "\n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "        model = PPO('MlpPolicy', env,\n",
    "                    ent_coef=0.01,    # explicitly encourages exploration\n",
    "                    clip_range=0.2,\n",
    "                    **params, verbose=0)\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        val_env = PortfolioEnv(val_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        obs, _ = val_env.reset()\n",
    "        done, total_reward = False, 0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_params = {**params, 'risk_coefficient': risk_coeff}\n",
    "\n",
    "    return best_params\n",
    "\n",
    "def scale_data(df, feature_cols, scaler):\n",
    "    scaled_features = scaler.transform(df[feature_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=feature_cols, index=df.index)\n",
    "\n",
    "    # Re-add columns that were not scaled (e.g., Date, Actual_Return_*)\n",
    "    for col in df.columns:\n",
    "        if col not in feature_cols:\n",
    "            scaled_df[col] = df[col].values\n",
    "\n",
    "    # Keep original column order\n",
    "    scaled_df = scaled_df[df.columns]\n",
    "    return scaled_df\n",
    "\n",
    "# Main execution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for start_idx in start_indices:\n",
    "    # for start_idx in range(0, 252*2, 252):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Explicit indices for training, validation, and prediction datasets\n",
    "    train_start_idx = start_idx\n",
    "    train_end_idx = train_start_idx + train_window_days\n",
    "\n",
    "    val_start_idx = train_end_idx\n",
    "    val_end_idx = val_start_idx + validation_window_days\n",
    "\n",
    "    pred_start_idx = val_end_idx\n",
    "    pred_end_idx = pred_start_idx + prediction_window_days\n",
    "\n",
    "    # Corresponding dates explicitly\n",
    "    train_start_date = data_with_features.loc[train_start_idx, 'Date']\n",
    "    train_end_date = data_with_features.loc[train_end_idx - 1, 'Date']\n",
    "\n",
    "    val_start_date = data_with_features.loc[val_start_idx, 'Date']\n",
    "    val_end_date = data_with_features.loc[val_end_idx - 1, 'Date']\n",
    "\n",
    "    pred_start_date = data_with_features.loc[pred_start_idx, 'Date']\n",
    "    pred_end_date = data_with_features.loc[pred_end_idx - 1, 'Date']\n",
    "\n",
    "    # Clearly print ranges for clarity\n",
    "    print(f\"Training period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "    print(f\"Validation period: {val_start_date.date()} to {val_end_date.date()}\")\n",
    "    print(f\"Prediction period: {pred_start_date.date()} to {pred_end_date.date()}\")\n",
    "\n",
    "    # Explicitly subset data accordingly\n",
    "    train_data = data_with_features.iloc[train_start_idx:train_end_idx].reset_index(drop=True)\n",
    "    val_data = data_with_features.iloc[val_start_idx:val_end_idx].reset_index(drop=True)\n",
    "    pred_data = data_with_features.iloc[pred_start_idx:pred_end_idx].reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [col for col in train_data.columns if col != 'Date' and not col.startswith('Actual_Return')]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data[feature_cols])\n",
    "\n",
    "    train_data_scaled = scale_data(train_data, feature_cols, scaler)\n",
    "    val_data_scaled = scale_data(val_data, feature_cols, scaler)\n",
    "    pred_data_scaled = scale_data(pred_data, feature_cols, scaler)\n",
    "\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    best_params = validate_and_tune(train_data_scaled, val_data_scaled, reward_type)\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "    incremental_timesteps = 20000\n",
    "    max_timesteps = 100000\n",
    "    patience = 3\n",
    "    \n",
    "    best_val_reward = -np.inf\n",
    "    no_improve_steps = 0\n",
    "\n",
    "    risk_coeff = best_params.pop('risk_coefficient',0.5)\n",
    "    policy_kwargs = dict(net_arch=[256, 256])\n",
    "\n",
    "    env = make_vec_env(lambda: PortfolioEnv(train_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "    model = PPO('MlpPolicy', env,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                ent_coef=0.01,    # explicitly encourages exploration\n",
    "                clip_range=0.2,\n",
    "                **best_params, verbose=0)\n",
    "    # print(\"Starting model training...\")\n",
    "    # model.learn(total_timesteps=20000)\n",
    "    print(\"Starting model training with early stopping...\")\n",
    "\n",
    "    for step in range(0, max_timesteps, incremental_timesteps):\n",
    "        model.learn(total_timesteps=incremental_timesteps)\n",
    "    \n",
    "        # Evaluate on validation environment\n",
    "        val_env = PortfolioEnv(val_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        val_obs, _ = val_env.reset()\n",
    "        val_done = False\n",
    "        val_total_reward = 0.0\n",
    "    \n",
    "        while not val_done:\n",
    "            val_action, _ = model.predict(val_obs, deterministic=True)\n",
    "            val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "            val_total_reward += val_reward\n",
    "    \n",
    "        print(f\"Step: {step + incremental_timesteps}, Validation Total Reward: {val_total_reward:.4f}\")\n",
    "    \n",
    "        # Early stopping check\n",
    "        if val_total_reward > best_val_reward:\n",
    "            best_val_reward = val_total_reward\n",
    "            no_improve_steps = 0\n",
    "            model.save(\"best_ppo_model.zip\")\n",
    "            print(f\"Improved validation reward; model saved at step {step + incremental_timesteps}\")\n",
    "        else:\n",
    "            no_improve_steps += 1\n",
    "            print(f\"No improvement ({no_improve_steps}/{patience})\")\n",
    "    \n",
    "            if no_improve_steps >= patience:\n",
    "                print(\"Early stopping explicitly triggered.\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model explicitly\n",
    "    model = PPO.load(\"best_ppo_model.zip\")\n",
    "    print(\"Loaded the best PPO model explicitly for prediction.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Ensure historical context explicitly available in prediction\n",
    "    full_data = pd.concat([train_data_scaled, val_data_scaled, pred_data_scaled])\n",
    "    pred_data_with_history = full_data[full_data['Date'] >= (pred_start_date - pd.Timedelta(days=lookback_period))].reset_index(drop=True)\n",
    "\n",
    "    pred_env = PortfolioEnv(pred_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "    # pred_env = PortfolioEnv(pred_data_with_history, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "\n",
    "    obs, info = pred_env.reset()\n",
    "    done = False\n",
    "\n",
    "    action = np.zeros(len(etf_list), dtype=np.float32)\n",
    "\n",
    "    while not done:\n",
    "        if pred_env.current_step >= lookback_period and pred_env.current_step % pred_env.rebalance_period == 0:\n",
    "            # obs_for_agent = pred_data_with_history.drop(columns=['Date']).iloc[pred_env.current_step - lookback_period:pred_env.current_step].values.flatten().astype(np.float32)\n",
    "            # action, _ = model.predict(obs_for_agent, deterministic=True)\n",
    "\n",
    "            # v1 normalize weight\n",
    "            # action, _ = model.predict(obs, deterministic=True)\n",
    "            # temperature = 0.5\n",
    "            # scaled_action = action / temperature\n",
    "            # weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "            # rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "            # all_weights.append([rebalance_date] + weights.tolist())\n",
    "\n",
    "\n",
    "            # v2 long short normalization\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "            # Explicitly apply your new 120/20 normalization logic (to match environment step)\n",
    "            desired_long = 1.20  # Explicitly 120% long exposure\n",
    "            desired_short = 0.20  # Explicitly 20% short exposure\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate after clipping explicitly\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "\n",
    "            rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "            all_weights.append([rebalance_date] + final_weights.tolist())\n",
    "\n",
    "        obs, _, done, _, _ = pred_env.step(action)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "columns = ['Date'] + etf_list\n",
    "weights_df = pd.DataFrame(all_weights, columns=columns)\n",
    "weights_df.to_csv('ppo_multi_year_weights.csv', index=False)\n",
    "print(\"Saved predictions to ppo_multi_year_weights.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c770edf2acf6b5a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged successfully with aligned dates.\n",
      "\n",
      "==== Starting Iteration 1/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 6.1312\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 16.7238\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 19.0038\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 20.0287\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 18.5619\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 0 completed in 671.9926 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 19.3706\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 20.9798\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 15.9422\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 13.1084\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 12.1622\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 252 completed in 986.5275 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 7.6564\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 13.1767\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 6.2307\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 2.9360\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 0.4952\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 504 completed in 675.4907 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 8.0884\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 9.5849\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 0.5994\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 3.5206\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 9.8332\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 756 completed in 993.4260 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 59.8956\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 54.5794\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 68.3656\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 64.8094\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 60.0138\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 1008 completed in 991.7886 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 12.3872\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 12.3872\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 12.3872\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 13.3667\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 13.3667\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 1260 completed in 674.3711 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -1.6493\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -6.1381\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -10.4291\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -1.5035\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: -9.4784\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 1512 completed in 674.9438 seconds\n",
      "\n",
      "==== Starting Iteration 2/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 13.1773\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 12.7396\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 11.1880\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 14.6460\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 10.8706\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 0 completed in 671.0903 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 16.7612\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 3.6933\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -0.0771\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -6.0383\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 252 completed in 571.5185 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -10.9148\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -4.1888\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -12.5095\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: -14.6811\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: -4.8106\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 504 completed in 986.7231 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 13.0133\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 5.0329\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 13.5564\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 7.9741\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 11.6585\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 756 completed in 669.8412 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 62.2533\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 70.1161\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 69.8148\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 52.2901\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 54.7956\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 1008 completed in 671.8451 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 15.3966\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 19.3895\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 17.1286\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 17.2765\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 16.6414\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 1260 completed in 987.9866 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -18.1818\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -13.4204\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -12.1913\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: -9.9299\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: -9.8382\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 1512 completed in 675.0309 seconds\n",
      "\n",
      "==== Starting Iteration 3/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 18.5440\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 22.3068\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 17.7045\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 17.8075\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 16.0941\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 0 completed in 669.0184 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 12.6185\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 10.1002\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 8.6563\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 8.2845\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 252 completed in 567.8568 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -3.3273\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -4.5247\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -4.4107\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 4.1637\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: -0.6863\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 504 completed in 991.8251 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 15.3112\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 13.3874\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 4.4863\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 15.6956\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 14.2357\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 756 completed in 991.6259 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 73.2654\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 85.7183\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 55.2321\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 60.4247\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 58.7161\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 1008 completed in 670.9341 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 20.3929\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 19.0036\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 12.2967\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 16.8212\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 1260 completed in 568.0426 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -0.2794\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -38.1610\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -20.9630\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -22.8614\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 1512 completed in 571.7441 seconds\n",
      "\n",
      "==== Starting Iteration 4/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 15.2080\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 9.2255\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 9.8333\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 10.0069\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 0 completed in 568.2486 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 5.1871\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 14.1823\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 21.2044\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 11.5664\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 4.6197\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 252 completed in 670.3186 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -8.6920\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -11.5372\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 3.7490\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 2.6300\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: -0.0949\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 504 completed in 693.1401 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 15.5625\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 10.4934\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 9.3793\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 14.4250\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 756 completed in 570.8754 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 60.6452\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 56.4695\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 74.1052\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 70.7332\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 71.8604\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 1008 completed in 674.6754 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 17.5772\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 16.6712\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 14.9302\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 17.4162\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 1260 completed in 567.1543 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -7.8260\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -1.0777\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 11.3075\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 12.7644\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: -5.8782\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 1512 completed in 670.3082 seconds\n",
      "\n",
      "==== Starting Iteration 5/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 13.5897\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 36.0679\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 12.2327\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 15.1071\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 18.7153\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 0 completed in 673.9722 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 11.0591\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 6.8929\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 4.6206\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 4.9752\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 252 completed in 824.4863 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 10.3225\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 0.1050\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -1.6066\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 0.9929\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 504 completed in 568.1679 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 28.6792\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 16.3346\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 7.5002\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 9.5556\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 756 completed in 568.7149 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 79.3286\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 70.3863\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 70.5037\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 65.6897\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 1008 completed in 567.0197 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 20.2019\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 12.8557\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 15.5737\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 21.5551\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 19.5754\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 1260 completed in 986.4892 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 0.4390\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 0.1810\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -7.2528\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -16.0812\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 1512 completed in 567.2895 seconds\n",
      "\n",
      "==== Starting Iteration 6/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 7.7250\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 10.0803\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 11.6203\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 11.7380\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 11.6825\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 0 completed in 670.1532 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 17.0856\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 23.6671\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 16.3094\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 16.1132\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 14.4587\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 252 completed in 986.4041 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -4.2686\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -18.4891\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -16.9407\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -16.8143\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 504 completed in 567.0328 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 11.2603\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 24.4966\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 7.1204\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 7.5995\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 10.1385\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 756 completed in 986.2843 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 50.1114\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 67.5236\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 67.6343\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 72.4237\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 84.2806\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 1008 completed in 674.4295 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 22.3856\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 20.2985\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 24.6017\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 24.9227\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 14.4667\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 1260 completed in 986.9798 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -19.4065\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 5.1917\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -13.4410\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 23.5318\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 17.3380\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 1512 completed in 992.9469 seconds\n",
      "\n",
      "==== Starting Iteration 7/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 8.1224\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 12.3633\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 13.6240\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 13.0648\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 10.0095\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 0 completed in 985.3890 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 16.5339\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 8.8343\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 7.1096\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 6.8031\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 252 completed in 567.5051 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 2.6352\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 19.4027\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -3.9511\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: -1.3941\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: -16.0003\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 504 completed in 670.2005 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 12.8442\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 5.8523\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 3.9512\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 13.9907\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 8.5255\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 756 completed in 675.0255 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 49.7432\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 71.7180\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 69.3539\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 71.4823\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 73.9551\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 1008 completed in 986.9259 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 14.4792\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 14.7152\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 15.0499\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 15.0499\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 15.0499\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 1260 completed in 991.1727 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -5.1312\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -9.2583\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -10.3353\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -20.9480\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 1512 completed in 571.3424 seconds\n",
      "\n",
      "==== Starting Iteration 8/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 15.3182\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 11.0990\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 3.8360\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 5.7650\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 0 completed in 820.8350 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 3.1650\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 12.6429\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 13.8423\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 13.4230\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 11.1540\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 252 completed in 990.7190 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 4.1952\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 7.2465\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -3.0293\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 2.4434\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 10.0398\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 504 completed in 674.0669 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 12.3247\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 21.0772\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 19.5259\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 17.5484\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 17.7261\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 756 completed in 988.4250 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 50.2070\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 67.8682\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 68.3954\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 66.4687\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 72.3772\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 1008 completed in 669.3897 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 19.6017\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 22.3854\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 20.4547\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 21.1022\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 21.9647\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 1260 completed in 992.5304 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -3.6770\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 19.6928\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 43.6256\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 13.4561\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 21.2263\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 1512 completed in 669.1977 seconds\n",
      "\n",
      "==== Starting Iteration 9/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 18.3793\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 7.1284\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 9.4559\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 6.1010\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 0 completed in 568.1002 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 18.7960\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 14.0639\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 17.0727\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 14.9303\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 252 completed in 571.6381 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -0.8700\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -3.0581\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -9.6542\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -5.0837\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 504 completed in 571.2561 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 20.1426\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 18.0032\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 9.2488\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 9.3708\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 756 completed in 571.5538 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 41.5376\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 49.3984\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 51.4609\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 50.9600\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 61.9275\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 1008 completed in 674.1765 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 11.0600\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 17.2080\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 14.8767\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 14.0366\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 16.0977\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 1260 completed in 668.9169 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 12.6936\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -13.1693\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -2.3585\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -2.9694\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 1512 completed in 567.6295 seconds\n",
      "\n",
      "==== Starting Iteration 10/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 12.8912\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 10.2588\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 12.9003\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 13.2522\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 14.0823\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 0 completed in 992.2779 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 15.6750\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 12.7991\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 11.2899\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 5.4527\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 252 completed in 568.3663 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 5.6102\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -4.1059\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -0.7579\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 0.4675\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 504 completed in 568.5529 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 5.6840\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 17.2036\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 24.7320\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 21.2977\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 18.7586\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 756 completed in 990.9390 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 67.7117\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 73.0976\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 70.7339\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 57.9116\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 58.8202\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 1008 completed in 674.9793 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 15.9046\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 19.7008\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 19.7008\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 18.1460\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 20.4612\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 1260 completed in 991.9778 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -13.0418\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -3.0197\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -2.6482\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 4.8298\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 6.6386\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 1512 completed in 673.9520 seconds\n",
      "\n",
      "==== Starting Iteration 11/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 12.7691\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 18.3255\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 16.5791\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 20.3668\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 22.7066\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 0 completed in 991.9802 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 8.5209\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 9.3117\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 17.4570\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 11.7124\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 16.6436\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 252 completed in 986.9086 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -2.5968\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -3.8527\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -8.7527\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -7.9791\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 504 completed in 819.7636 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 20.7175\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 15.2675\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 15.9348\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 17.4299\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 756 completed in 822.5376 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 90.6466\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 88.5801\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 90.7474\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 88.1017\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 79.3663\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 1008 completed in 675.3172 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 17.6677\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 17.2919\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 19.2957\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 24.2945\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 25.8614\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 1260 completed in 670.2086 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -9.6032\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -0.6971\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -6.0515\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: -12.3299\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: -4.3798\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 1512 completed in 670.0230 seconds\n",
      "\n",
      "==== Starting Iteration 12/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 18.3010\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 15.1647\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 17.7941\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 17.4135\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 0 completed in 825.5955 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 14.7319\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 13.0502\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 18.2683\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 18.4435\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 19.6979\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 252 completed in 990.5866 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -5.2321\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -2.8663\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -3.6834\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: -0.7558\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 4.5726\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 504 completed in 669.1546 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 9.4187\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 15.7690\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 17.5655\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 22.1260\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 18.7759\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 756 completed in 675.2095 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 75.6020\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 62.1762\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 54.6674\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 75.6609\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 73.5862\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 1008 completed in 673.7535 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 16.0846\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 23.2150\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 23.5687\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 22.6119\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 21.1638\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 1260 completed in 991.1223 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -4.0844\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -10.7015\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -17.3347\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -17.0661\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 1512 completed in 570.9726 seconds\n",
      "\n",
      "==== Starting Iteration 13/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 15.2828\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 21.2747\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 13.3755\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 16.5371\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 16.7306\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 0 completed in 671.6005 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 11.6336\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 6.7836\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 12.0996\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 11.0162\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 10.2157\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 252 completed in 999.2245 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 0.3721\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -0.1595\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 1.8923\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: -0.9025\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 4.5685\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 504 completed in 680.8208 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 11.4929\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 10.7654\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 13.2923\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 3.8095\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 10.5025\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 756 completed in 994.2034 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 77.0840\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 62.7031\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 78.2272\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 64.0057\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 74.9547\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 1008 completed in 677.9113 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 23.6541\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 24.2754\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 19.7155\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 18.8006\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 21.5242\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 1260 completed in 995.0545 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -13.7489\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -7.2201\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -6.7646\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: -0.3407\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: -6.9434\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 1512 completed in 991.1356 seconds\n",
      "\n",
      "==== Starting Iteration 14/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -0.9410\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 9.9252\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 9.7275\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 7.4243\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 3.9748\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 0 completed in 671.1948 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 9.5816\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 9.3131\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 7.7337\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 6.0971\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 252 completed in 569.7669 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -1.0640\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -0.5815\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -3.3598\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 6.9911\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 1.0531\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 504 completed in 989.7171 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 14.5148\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -0.0777\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 20.6110\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 7.6982\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 13.9386\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 756 completed in 672.8394 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 76.7120\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 63.9668\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 77.5609\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 71.3256\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 71.2409\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 1008 completed in 999.6965 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 24.9645\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 25.0482\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 21.5677\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 19.7503\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 25.0809\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 1260 completed in 990.1468 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -2.9851\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -9.5826\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -3.2245\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -3.2928\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 1512 completed in 574.6986 seconds\n",
      "\n",
      "==== Starting Iteration 15/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 7.5875\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 14.7704\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 14.0440\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 12.1452\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 16.8545\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 0 completed in 671.3799 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 19.2266\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 16.7862\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 18.1787\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 18.1200\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 252 completed in 824.6317 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -0.9917\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -4.9763\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 4.8323\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 0.7210\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: -2.5492\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 504 completed in 674.2384 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 10.7341\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 9.3995\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 10.1537\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 7.6838\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 756 completed in 566.8461 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 58.6918\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 59.8149\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 21.3552\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 40.9710\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 39.5859\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 1008 completed in 995.6200 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 22.1107\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 16.8762\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 20.3863\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 16.8731\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 1260 completed in 823.4051 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 1.5948\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -4.6202\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 5.3260\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 5.6005\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 11.4492\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 1512 completed in 677.8033 seconds\n",
      "\n",
      "==== Starting Iteration 16/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 13.5759\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 15.3170\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 15.8610\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 16.5762\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 11.3241\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 0 completed in 999.2455 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 6.8988\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 13.2851\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 15.6980\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 14.5134\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 14.4569\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 252 completed in 670.7524 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -6.8556\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -2.1420\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -4.3322\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: -4.0034\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: -8.2199\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 504 completed in 995.2572 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 16.4375\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 13.3317\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 7.1704\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 17.7635\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 22.9321\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 756 completed in 676.3269 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 60.6270\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 61.1208\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 73.9946\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 63.4999\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 57.4868\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 1008 completed in 675.2857 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 20.1386\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 27.5756\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 20.9845\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 21.5978\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 23.2934\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 1260 completed in 993.7260 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -14.2588\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -17.3551\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -18.6110\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -8.3159\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: -9.1990\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 1512 completed in 681.1620 seconds\n",
      "\n",
      "==== Starting Iteration 17/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 16.6085\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 17.3467\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 13.3147\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 16.1283\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 12.5479\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 0 completed in 995.8270 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 8.2290\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 0.1443\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -1.7341\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 5.0822\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 252 completed in 827.9815 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 0.3147\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -6.8641\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -5.1351\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -1.8884\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 504 completed in 572.6662 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 13.4535\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 5.7893\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 8.7404\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 19.5469\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 15.8199\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 756 completed in 995.8946 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 77.9262\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 83.0562\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 82.2989\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 89.9612\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 92.1559\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 1008 completed in 995.7964 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 14.5885\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 11.6518\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 17.4876\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 14.2139\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 13.0117\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 1260 completed in 996.5083 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 8.4512\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -4.0195\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -14.1587\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -5.0516\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 1512 completed in 577.9632 seconds\n",
      "\n",
      "==== Starting Iteration 18/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 15.9987\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 2.7284\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 9.3705\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 8.4005\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 0 completed in 573.6801 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 15.5883\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 18.7263\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 13.1409\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 9.5450\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 11.5385\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 252 completed in 995.8015 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -3.1968\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -10.4096\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 0.8414\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: -6.6276\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: -11.7204\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 504 completed in 993.7070 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 6.4880\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 5.6113\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 2.9812\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 4.0644\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 756 completed in 573.2521 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 73.0883\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 76.7105\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 57.8051\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 65.7073\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 68.9330\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 1008 completed in 680.9183 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 23.1322\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 19.9242\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 20.7931\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 22.2822\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 1260 completed in 577.2702 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -22.2406\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -12.3764\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -21.1297\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: -12.2347\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 0.8637\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 1512 completed in 676.5626 seconds\n",
      "\n",
      "==== Starting Iteration 19/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 7.9743\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 8.1806\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 9.1873\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 11.5098\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 13.3867\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 0 completed in 993.8253 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 11.6195\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 10.0878\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 9.9205\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 8.2537\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 252 completed in 573.6398 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -6.4565\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -3.1889\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -2.4455\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: -2.3414\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: -10.7897\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 504 completed in 707.7041 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 18.0958\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 17.5055\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 12.3576\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 9.8146\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 756 completed in 573.1676 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 58.6173\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 73.6328\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 65.0228\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 74.2570\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 79.9501\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 1008 completed in 680.3346 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 19.3908\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 18.6113\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 23.3668\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 22.8441\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 21.8855\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 1260 completed in 994.0106 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 4.9103\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 23.4908\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 3.6622\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 12.2903\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 12.3327\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 1512 completed in 676.4013 seconds\n",
      "\n",
      "==== Starting Iteration 20/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 16.8445\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 14.1550\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 12.3118\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 12.4642\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 0 completed in 829.0243 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 10.5101\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 14.0194\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 14.1390\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 16.5622\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 2.4125\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 252 completed in 675.5588 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -3.1701\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -4.2099\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -6.5570\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -4.3817\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 504 completed in 576.7152 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 17.2573\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 16.6470\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 7.4273\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 17.0697\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 756 completed in 577.1949 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 51.0668\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 42.9036\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 53.9948\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 57.3705\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 64.4625\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 1008 completed in 677.1779 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 26.4030\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 22.1421\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 17.7510\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 19.7001\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 1260 completed in 577.6457 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -8.9388\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -2.3181\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -12.2446\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: -6.5510\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 4.0097\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 1512 completed in 675.3274 seconds\n",
      "\n",
      "==== Starting Iteration 21/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 20.6252\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 22.6309\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 16.4665\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 12.5095\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 15.1235\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 0 completed in 683.7839 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 16.5051\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 10.7945\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 12.5891\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 13.9698\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 252 completed in 576.4589 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -4.8187\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 5.5225\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 1.1335\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 0.3897\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: -3.3755\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 504 completed in 992.4893 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 2.7294\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 9.9304\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 13.5276\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 15.0245\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 14.7317\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 756 completed in 680.7371 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 64.0513\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 58.5535\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 63.7446\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 54.5666\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 1008 completed in 577.2787 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 22.6037\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 18.5691\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 19.8465\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 16.7100\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 1260 completed in 572.4740 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -14.9482\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -9.9603\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -8.4134\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: -11.1675\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: -20.7874\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 1512 completed in 676.5619 seconds\n",
      "\n",
      "==== Starting Iteration 22/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 13.8903\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 11.2997\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 18.1549\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 14.0341\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 12.4662\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 0 completed in 999.8221 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 9.2952\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 15.6823\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 17.7555\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 14.7673\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 12.2541\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 252 completed in 680.3623 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -7.5546\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -4.0917\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 0.5146\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: -3.4965\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: -3.4841\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 504 completed in 998.9896 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 13.8080\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 15.4678\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 7.5796\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 17.8606\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 27.3401\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 756 completed in 676.2072 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 60.5342\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 70.2518\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 66.0222\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 83.4668\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 85.4172\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 1008 completed in 675.7127 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 18.7824\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 20.1479\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 13.1618\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 12.3557\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 12.9645\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 1260 completed in 995.6765 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -4.3744\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -12.2869\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 12.0628\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: -6.0375\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: -4.0355\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 1512 completed in 690.4116 seconds\n",
      "\n",
      "==== Starting Iteration 23/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0001, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 24.5553\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 16.7229\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 18.6157\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 19.5968\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 0 completed in 572.4711 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 17.4503\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 16.1464\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 14.9602\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 13.5890\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 252 completed in 571.9693 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -4.4838\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -1.2065\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -6.1441\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: -7.1181\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 0.2342\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 504 completed in 995.4425 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 10.3802\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 5.5374\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 13.0283\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 13.3243\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 8.7680\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 756 completed in 677.3725 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 42.2953\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 74.4577\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 67.8529\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 70.2330\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 60.7656\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 1008 completed in 681.2976 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 20.1564\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 15.1252\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 7.6607\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 10.0128\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 1260 completed in 573.1797 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -9.6005\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -0.9842\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -22.1503\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: -22.4946\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: -27.5150\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 1512 completed in 703.8720 seconds\n",
      "\n",
      "==== Starting Iteration 24/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.98, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 19.5339\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 14.9344\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 17.9976\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 19.9559\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 24.3596\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 0 completed in 1094.2093 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 13.8515\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 7.7960\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 9.9793\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 8.4962\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 252 completed in 936.4917 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 3.3255\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 2.0536\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 3.9639\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: -0.4466\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: -5.3995\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 504 completed in 1132.6655 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 8.9669\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 2.4036\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 8.1226\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 18.5033\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 12.7218\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 756 completed in 1143.0783 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 58.2476\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 75.8659\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 68.8513\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 60.8521\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 55.5161\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 1008 completed in 779.2245 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 13.3667\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 18.1591\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 17.3714\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 19.7333\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 16.0620\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 1260 completed in 744.5147 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 11.2943\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 8.4888\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 11.6477\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 0.5850\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 1.7524\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 1512 completed in 1034.6484 seconds\n",
      "\n",
      "==== Starting Iteration 25/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 12.9334\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 9.4072\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 13.8432\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 5.6932\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 11.6117\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 0 completed in 1014.4921 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 15.6121\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 19.1940\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 16.8555\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 18.4719\n",
      "No improvement (2/3)\n",
      "Step: 100000, Validation Total Reward: 13.2679\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 252 completed in 692.4001 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -1.6663\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -3.4763\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: -4.7434\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: -8.9720\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 504 completed in 586.7846 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 1.9674\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 5.0480\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 11.6520\n",
      "Improved validation reward; model saved at step 60000\n",
      "Step: 80000, Validation Total Reward: 11.4690\n",
      "No improvement (1/3)\n",
      "Step: 100000, Validation Total Reward: 9.7352\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 756 completed in 691.5961 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 40, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 10, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 80.2835\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 67.6473\n",
      "No improvement (1/3)\n",
      "Step: 60000, Validation Total Reward: 56.1477\n",
      "No improvement (2/3)\n",
      "Step: 80000, Validation Total Reward: 53.4207\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 1008 completed in 844.3942 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: 12.7588\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: 20.7158\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: 20.6399\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: 22.6496\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: 22.2850\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 1260 completed in 691.2168 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0001, 'gamma': 0.98, 'batch_size': 20, 'risk_coefficient': 0.5}\n",
      "Starting model training with early stopping...\n",
      "Step: 20000, Validation Total Reward: -17.9888\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 40000, Validation Total Reward: -13.3756\n",
      "Improved validation reward; model saved at step 40000\n",
      "Step: 60000, Validation Total Reward: -22.2080\n",
      "No improvement (1/3)\n",
      "Step: 80000, Validation Total Reward: -12.8746\n",
      "Improved validation reward; model saved at step 80000\n",
      "Step: 100000, Validation Total Reward: -1.2867\n",
      "Improved validation reward; model saved at step 100000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 1512 completed in 692.2824 seconds\n",
      "Saved all iterations' allocations to ppo_allocations_multiple_iterations.csv\n"
     ]
    }
   ],
   "source": [
    "# ITERATION\n",
    "# start of stage 2 training\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar', risk_coefficient=0.5, rebalance_period=21, lookback_period=21):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(etf_list),), dtype=np.float32)\n",
    "\n",
    "        # Explicitly select feature columns (excluding Date and returns used only for calculating reward)\n",
    "        self.feature_cols = [col for col in data.columns if col not in ['Date'] and not col.startswith('Actual_Return')]\n",
    "        self.num_features_per_day = len(self.feature_cols)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.num_features_per_day * self.lookback_period,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(etf_list)] * len(etf_list))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(self.etf_list)] * len(self.etf_list))\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        next_step = self.current_step + 1\n",
    "\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            # v2 long short\n",
    "            desired_long = 1.20  # 120% long exposure explicitly\n",
    "            desired_short = 0.20  # 20% short exposure explicitly\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                # Normal 120/20 explicitly0\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                # Only long explicitly: default realistically to 100% long\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                # Only short explicitly (unrealistic), fallback clearly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                # All zeros explicitly: fallback explicitly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            # Apply explicit clipping\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate explicitly after clipping\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            # Final explicit normalization after clipping\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)  # exactly 100% long\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                # Realistic fallback explicitly: equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "            self.current_weights = final_weights\n",
    "            # v1 softmax normalization\n",
    "            # \n",
    "            # temperature = 0.5  # Explicitly lower for higher concentration (try 0.2 to 0.8)\n",
    "            # scaled_action = action / temperature\n",
    "            # self.current_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "\n",
    "        else:\n",
    "            returns_today = np.array([self.data.loc[self.current_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            self.current_weights *= (1 + returns_today)\n",
    "            self.current_weights /= np.sum(self.current_weights)\n",
    "\n",
    "        if next_step >= len(self.data):\n",
    "            terminated = True\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            returns = np.array([self.data.loc[next_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            portfolio_return = np.dot(self.current_weights, returns)\n",
    "            self.cumulative_wealth *= (1 + portfolio_return)\n",
    "            reward = self.calculate_reward(portfolio_return, returns)\n",
    "            terminated = next_step >= len(self.data) - 1\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "        # def _get_obs(self):\n",
    "        #     obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        #     obs_window = obs_window.drop(columns=['Date']).values.flatten().astype(np.float32)\n",
    "        #     return obs_window\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        obs_window = obs_window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "        return obs_window\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns):\n",
    "        if self.reward_type == 'cumulative_return':\n",
    "            return self.cumulative_wealth - 1.0\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            return np.log(self.cumulative_wealth)\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100 * alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            return portfolio_return - self.risk_coefficient * cvar\n",
    "        else:\n",
    "            raise ValueError('Invalid reward type')\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_stable_features(df, etf_list):\n",
    "    data = df.copy()\n",
    "\n",
    "    for etf in etf_list:\n",
    "        price_col = f'Price_{etf}'\n",
    "\n",
    "        # Volatility (20-day)\n",
    "        data[f'Volatility_{etf}'] = data[price_col].pct_change().rolling(20).std()\n",
    "\n",
    "        # Momentum indicators (returns over 5, 10, 20 days)\n",
    "        data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
    "        data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
    "        data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
    "\n",
    "        # Moving averages (5-day and 20-day)\n",
    "        data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
    "        data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
    "\n",
    "        # Moving average crossover (5-day MA - 20-day MA)\n",
    "        data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
    "\n",
    "    # Drop NaN values due to rolling calculations\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # Explicit patterns to identify columns\n",
    "    predicted_return_pattern = 'Predicted_Return'\n",
    "    shap_metric_pattern = 'SHAP'\n",
    "\n",
    "    # Exclude Predicted Returns explicitly if requested\n",
    "    if not include_predicted_returns:\n",
    "        predicted_cols = [col for col in df_filtered.columns if predicted_return_pattern in col]\n",
    "        df_filtered.drop(columns=predicted_cols, inplace=True)\n",
    "        print(f\"Excluded predicted return columns: {predicted_cols}\")\n",
    "\n",
    "    # Exclude SHAP-related metrics explicitly if requested\n",
    "    if not include_shap_metrics:\n",
    "        shap_cols = [col for col in df_filtered.columns if shap_metric_pattern in col]\n",
    "        df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "        print(f\"Excluded SHAP-related columns: {shap_cols}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# ETFs\n",
    "etf_list = ['XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLY', 'XLV', 'XLU']\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 5e-5],\n",
    "    'n_steps': [20, 40],\n",
    "    'batch_size': [5, 10],\n",
    "    'gamma': [0.98, 0.99]\n",
    "}\n",
    "consolidated_file = 'stage2_rl_observations_optimized.csv'\n",
    "reward_type = 'log_wealth'\n",
    "# data = pd.read_csv(consolidated_file, parse_dates=['Date'])\n",
    "# data = data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "data = pd.read_csv('stage2_rl_observations_optimized.csv', parse_dates=['Date'])\n",
    "price_data = pd.read_csv('stock_prices_10ETFs.csv')\n",
    "\n",
    "# Convert the Date column in price data, handling the timezone correctly\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'], utc=True)\n",
    "price_data['Date'] = price_data['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Rename price columns explicitly to 'price_{ticker}'\n",
    "price_cols = {col: f'Price_{col}' for col in price_data.columns if col != 'Date'}\n",
    "price_data.rename(columns=price_cols, inplace=True)\n",
    "\n",
    "# Merge datasets on Date\n",
    "merged_data = pd.merge(data, price_data, on='Date', how='inner')\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "# Check if merge was successful\n",
    "if len(merged_data) != len(data):\n",
    "    print(f\"Warning: Data length mismatch after merging (Original: {len(data)}, Merged: {len(merged_data)}).\")\n",
    "else:\n",
    "    print(\"Merged successfully with aligned dates.\")\n",
    "\n",
    "data_with_features_raw = add_stable_features(merged_data, etf_list)\n",
    "data_with_features_raw.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Usage Example clearly for benchmark (only price metrics, no predicted return or SHAP):\n",
    "data_with_features = filter_features(data_with_features_raw, \n",
    "                                 include_predicted_returns=True, \n",
    "                                 include_shap_metrics=True)\n",
    "################### override data to use SHAP only\n",
    "# data_with_features = data\n",
    "################### END override \n",
    "\n",
    "# Define your rolling window lengths clearly:\n",
    "train_window_days = 252 * 7\n",
    "validation_window_days = 252\n",
    "prediction_window_days = 252\n",
    "lookback_period = 21\n",
    "rebalance_period = 21\n",
    "\n",
    "start_indices = range(0, len(data) - (train_window_days + validation_window_days + prediction_window_days), prediction_window_days)\n",
    "all_weights = []\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "def validate_and_tune(train_data, val_data, reward_type, rebalance_period=10, lookback_period=10, n_iter=8, timesteps=5000):\n",
    "    best_reward, best_params = -np.inf, None\n",
    "\n",
    "    # Narrow and meaningful parameter distribution\n",
    "    param_dist = {\n",
    "        'learning_rate': [3e-4, 1e-4],\n",
    "        'n_steps': [20, 40],\n",
    "        'batch_size': [10, 20],\n",
    "        'gamma': [0.95, 0.98],\n",
    "        'risk_coefficient': [0.1, 0.5, 1.0] if reward_type in ['mean_var', 'mean_cvar'] else [0.5],\n",
    "    }\n",
    "\n",
    "    sampled_params = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=42))\n",
    "\n",
    "    for params in sampled_params:\n",
    "        risk_coeff = params.pop('risk_coefficient', 0.5)\n",
    "\n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "        model = PPO('MlpPolicy', env,\n",
    "                    ent_coef=0.01,    # explicitly encourages exploration\n",
    "                    clip_range=0.2,\n",
    "                    **params, verbose=0)\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        val_env = PortfolioEnv(val_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        obs, _ = val_env.reset()\n",
    "        done, total_reward = False, 0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_params = {**params, 'risk_coefficient': risk_coeff}\n",
    "\n",
    "    return best_params\n",
    "\n",
    "def scale_data(df, feature_cols, scaler):\n",
    "    scaled_features = scaler.transform(df[feature_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=feature_cols, index=df.index)\n",
    "\n",
    "    # Re-add columns that were not scaled (e.g., Date, Actual_Return_*)\n",
    "    for col in df.columns:\n",
    "        if col not in feature_cols:\n",
    "            scaled_df[col] = df[col].values\n",
    "\n",
    "    # Keep original column order\n",
    "    scaled_df = scaled_df[df.columns]\n",
    "    return scaled_df\n",
    "\n",
    "# Main execution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iterations = 25\n",
    "all_weights_iterations = []\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    print(f\"\\n==== Starting Iteration {iteration + 1}/{iterations} ====\")\n",
    "\n",
    "    for start_idx in start_indices:\n",
    "        # for start_idx in range(0, 252*2, 252):\n",
    "        start_time = time.time()\n",
    "    \n",
    "        # Explicit indices for training, validation, and prediction datasets\n",
    "        train_start_idx = start_idx\n",
    "        train_end_idx = train_start_idx + train_window_days\n",
    "    \n",
    "        val_start_idx = train_end_idx\n",
    "        val_end_idx = val_start_idx + validation_window_days\n",
    "    \n",
    "        pred_start_idx = val_end_idx\n",
    "        pred_end_idx = pred_start_idx + prediction_window_days\n",
    "    \n",
    "        # Corresponding dates explicitly\n",
    "        train_start_date = data_with_features.loc[train_start_idx, 'Date']\n",
    "        train_end_date = data_with_features.loc[train_end_idx - 1, 'Date']\n",
    "    \n",
    "        val_start_date = data_with_features.loc[val_start_idx, 'Date']\n",
    "        val_end_date = data_with_features.loc[val_end_idx - 1, 'Date']\n",
    "    \n",
    "        pred_start_date = data_with_features.loc[pred_start_idx, 'Date']\n",
    "        pred_end_date = data_with_features.loc[pred_end_idx - 1, 'Date']\n",
    "    \n",
    "        # Clearly print ranges for clarity\n",
    "        print(f\"Training period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "        print(f\"Validation period: {val_start_date.date()} to {val_end_date.date()}\")\n",
    "        print(f\"Prediction period: {pred_start_date.date()} to {pred_end_date.date()}\")\n",
    "    \n",
    "        # Explicitly subset data accordingly\n",
    "        train_data = data_with_features.iloc[train_start_idx:train_end_idx].reset_index(drop=True)\n",
    "        val_data = data_with_features.iloc[val_start_idx:val_end_idx].reset_index(drop=True)\n",
    "        pred_data = data_with_features.iloc[pred_start_idx:pred_end_idx].reset_index(drop=True)\n",
    "    \n",
    "        feature_cols = [col for col in train_data.columns if col != 'Date' and not col.startswith('Actual_Return')]\n",
    "    \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_data[feature_cols])\n",
    "    \n",
    "        train_data_scaled = scale_data(train_data, feature_cols, scaler)\n",
    "        val_data_scaled = scale_data(val_data, feature_cols, scaler)\n",
    "        pred_data_scaled = scale_data(pred_data, feature_cols, scaler)\n",
    "    \n",
    "        print(\"Starting hyperparameter tuning...\")\n",
    "        best_params = validate_and_tune(train_data_scaled, val_data_scaled, reward_type)\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "        incremental_timesteps = 20000\n",
    "        max_timesteps = 100000\n",
    "        patience = 3\n",
    "        \n",
    "        best_val_reward = -np.inf\n",
    "        no_improve_steps = 0\n",
    "    \n",
    "        risk_coeff = best_params.pop('risk_coefficient',0.5)\n",
    "        policy_kwargs = dict(net_arch=[256, 256])\n",
    "    \n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "        model = PPO('MlpPolicy', env,\n",
    "                    policy_kwargs=policy_kwargs,\n",
    "                    ent_coef=0.01,    # explicitly encourages exploration\n",
    "                    clip_range=0.2,\n",
    "                    **best_params, verbose=0)\n",
    "        # print(\"Starting model training...\")\n",
    "        # model.learn(total_timesteps=20000)\n",
    "        print(\"Starting model training with early stopping...\")\n",
    "    \n",
    "        for step in range(0, max_timesteps, incremental_timesteps):\n",
    "            model.learn(total_timesteps=incremental_timesteps)\n",
    "        \n",
    "            # Evaluate on validation environment\n",
    "            val_env = PortfolioEnv(val_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "            val_obs, _ = val_env.reset()\n",
    "            val_done = False\n",
    "            val_total_reward = 0.0\n",
    "        \n",
    "            while not val_done:\n",
    "                val_action, _ = model.predict(val_obs, deterministic=True)\n",
    "                val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "                val_total_reward += val_reward\n",
    "        \n",
    "            print(f\"Step: {step + incremental_timesteps}, Validation Total Reward: {val_total_reward:.4f}\")\n",
    "        \n",
    "            # Early stopping check\n",
    "            if val_total_reward > best_val_reward:\n",
    "                best_val_reward = val_total_reward\n",
    "                no_improve_steps = 0\n",
    "                model.save(\"best_ppo_model.zip\")\n",
    "                print(f\"Improved validation reward; model saved at step {step + incremental_timesteps}\")\n",
    "            else:\n",
    "                no_improve_steps += 1\n",
    "                print(f\"No improvement ({no_improve_steps}/{patience})\")\n",
    "        \n",
    "                if no_improve_steps >= patience:\n",
    "                    print(\"Early stopping explicitly triggered.\")\n",
    "                    break\n",
    "        \n",
    "        # Load the best model explicitly\n",
    "        model = PPO.load(\"best_ppo_model.zip\")\n",
    "        print(\"Loaded the best PPO model explicitly for prediction.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "        # Ensure historical context explicitly available in prediction\n",
    "        full_data = pd.concat([train_data_scaled, val_data_scaled, pred_data_scaled])\n",
    "        pred_data_with_history = full_data[full_data['Date'] >= (pred_start_date - pd.Timedelta(days=lookback_period))].reset_index(drop=True)\n",
    "    \n",
    "        pred_env = PortfolioEnv(pred_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        # pred_env = PortfolioEnv(pred_data_with_history, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "    \n",
    "        obs, info = pred_env.reset()\n",
    "        done = False\n",
    "    \n",
    "        action = np.zeros(len(etf_list), dtype=np.float32)\n",
    "    \n",
    "        while not done:\n",
    "            if pred_env.current_step >= lookback_period and pred_env.current_step % pred_env.rebalance_period == 0:\n",
    "                # obs_for_agent = pred_data_with_history.drop(columns=['Date']).iloc[pred_env.current_step - lookback_period:pred_env.current_step].values.flatten().astype(np.float32)\n",
    "                # action, _ = model.predict(obs_for_agent, deterministic=True)\n",
    "    \n",
    "                # v1 normalize weight\n",
    "                # action, _ = model.predict(obs, deterministic=True)\n",
    "                # temperature = 0.5\n",
    "                # scaled_action = action / temperature\n",
    "                # weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "                # rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "                # all_weights.append([rebalance_date] + weights.tolist())\n",
    "    \n",
    "    \n",
    "                # v2 long short normalization\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "    \n",
    "                # Explicitly apply your new 120/20 normalization logic (to match environment step)\n",
    "                desired_long = 1.20  # Explicitly 120% long exposure\n",
    "                desired_short = 0.20  # Explicitly 20% short exposure\n",
    "                clip_bounds = (-0.2, 0.8)\n",
    "    \n",
    "                raw_weights = action.copy()\n",
    "    \n",
    "                # Separate explicitly positive (long) and negative (short) actions\n",
    "                long_weights = np.maximum(raw_weights, 0)\n",
    "                short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "    \n",
    "                has_longs = np.sum(long_weights) > 0\n",
    "                has_shorts = np.sum(short_weights) > 0\n",
    "    \n",
    "                if has_longs and has_shorts:\n",
    "                    normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                    normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "                elif has_longs and not has_shorts:\n",
    "                    normalized_long = long_weights / np.sum(long_weights)\n",
    "                    normalized_short = np.zeros_like(short_weights)\n",
    "                elif not has_longs and has_shorts:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    normalized_long = np.ones(num_assets) / num_assets\n",
    "                    normalized_short = np.zeros(num_assets)\n",
    "                else:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    normalized_long = np.ones(num_assets) / num_assets\n",
    "                    normalized_short = np.zeros(num_assets)\n",
    "    \n",
    "                combined_weights = normalized_long - normalized_short\n",
    "                clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "    \n",
    "                # Re-separate after clipping explicitly\n",
    "                long_clipped = np.maximum(clipped_weights, 0)\n",
    "                short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "    \n",
    "                has_long_clipped = np.sum(long_clipped) > 0\n",
    "                has_short_clipped = np.sum(short_clipped) > 0\n",
    "    \n",
    "                if has_long_clipped and has_short_clipped:\n",
    "                    final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                    final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "                elif has_long_clipped and not has_short_clipped:\n",
    "                    final_long = long_clipped / np.sum(long_clipped)\n",
    "                    final_short = np.zeros_like(short_clipped)\n",
    "                else:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    final_long = np.ones(num_assets) / num_assets\n",
    "                    final_short = np.zeros(num_assets)\n",
    "    \n",
    "                final_weights = final_long - final_short\n",
    "    \n",
    "                rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "                # all_weights.append([rebalance_date] + final_weights.tolist())\n",
    "                all_weights_iterations.append([iteration + 1, rebalance_date] + final_weights.tolist())\n",
    "            obs, _, done, _, _ = pred_env.step(action)\n",
    "    \n",
    "        end_time = time.time()\n",
    "        print(f\"Iteration {iteration + 1}, start index {start_idx} completed in {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "columns = ['Iteration', 'Date'] + etf_list\n",
    "weights_df = pd.DataFrame(all_weights_iterations, columns=columns)\n",
    "weights_df.to_csv('ppo_allocations_multiple_iterations.csv', index=False)\n",
    "print(\"Saved all iterations' allocations to ppo_allocations_multiple_iterations.csv\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-11T16:49:29.690127Z",
     "start_time": "2025-07-10T03:45:04.739824Z"
    }
   },
   "id": "86d1a98b60121652",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data_scaled"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6947d247eaecd896",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "72545f994ad96b0c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # Explicit patterns to identify columns\n",
    "    predicted_return_pattern = 'Predicted_Return'\n",
    "    shap_metric_pattern = 'SHAP'\n",
    "\n",
    "    # Exclude Predicted Returns explicitly if requested\n",
    "    if not include_predicted_returns:\n",
    "        predicted_cols = [col for col in df_filtered.columns if predicted_return_pattern in col]\n",
    "        df_filtered.drop(columns=predicted_cols, inplace=True)\n",
    "        print(f\"Excluded predicted return columns: {predicted_cols}\")\n",
    "\n",
    "    # Exclude SHAP-related metrics explicitly if requested\n",
    "    if not include_shap_metrics:\n",
    "        shap_cols = [col for col in df_filtered.columns if shap_metric_pattern in col]\n",
    "        df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "        print(f\"Excluded SHAP-related columns: {shap_cols}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# Usage Example clearly for benchmark (only price metrics, no predicted return or SHAP):\n",
    "benchmark_data = filter_features(data_with_features, \n",
    "                                 include_predicted_returns=False, \n",
    "                                 include_shap_metrics=False)\n",
    "\n",
    "print(\"Benchmark data shape:\", benchmark_data.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95d42dbfc8f309b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_with_features.to_csv('test.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c323d38dcca1eb9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns = ['Date'] + etf_list\n",
    "weights_df = pd.DataFrame(all_weights, columns=columns)\n",
    "weights_df.to_csv('ppo_multi_year_weights.csv', index=False)\n",
    "print(\"Saved predictions to ppo_multi_year_weights.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff982b6575c90242",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "merged_data.columns\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f659f40adae5a90",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# end of the stage 2 portfolio allocation "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fac0604c1efae67d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install --upgrade numba"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2fb82da60c5d317",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_factors_np = train_factors_norm.values.astype('float32')\n",
    "num_samples, d_model = train_factors_np.shape  # Extract number of samples and feature count\n",
    "print(f'num_samples: {num_samples}, d_model:{d_model}')\n",
    "background_data = train_factors_norm.values.astype('float32').reshape(-1, 126, d_model)\n",
    "background_data_tensor = torch.tensor(background_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# âœ… Debugging SHAP input shape\n",
    "print(\n",
    "    f\"âœ… Background Data Shape Before SHAP: {background_data_tensor.shape}\")  # Expected: (1, rolling_window_test, 6)\n",
    "\n",
    "# âœ… Step 3: Initialize SHAP GradientExplainer using training data\n",
    "explainer = shap.GradientExplainer(model, background_data_tensor)\n",
    "\n",
    "# âœ… Step 4: Compute SHAP values for the training set\n",
    "shap_values = explainer.shap_values(torch.tensor(background_data, dtype=torch.float32).to(device))\n",
    "print(f'shap_values: {shap_values}')\n",
    "\n",
    "# âœ… Step 5: Debug SHAP value shape before squeezing\n",
    "print(f\"ðŸš€ SHAP Raw Output Shape: {shap_values[0].shape}\")  # Example: (1, rolling_window_test, 6, 1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbc4a4aa7aac3e90",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# +-------------------------------+           +-------------------------------+\n",
    "# | DIA SHAP + Technical Indicators|           |    Daily Returns (28 Stocks)  |\n",
    "# | (from shap_with_DIA_indicators.csv)        |  (from daily_returns.csv)     |\n",
    "# +--------------+----------------+           +---------------+---------------+\n",
    "#                |                                            |\n",
    "#                v                                            v\n",
    "#         +---------------------+                  +--------------------------+\n",
    "#         |  High-Level Agent   |----------------->|   Low-Level PPO Agents   |\n",
    "#         |  (PPO classifier)   | Regime Signal    | (trained per market regime)|\n",
    "#         +---------------------+                  +--------------------------+\n",
    "#                |                                            |\n",
    "#                v                                            v\n",
    "#         Regime prediction                         Optimal portfolio weights\n",
    "#                                                   (Mean-CVaR reward explicitly benchmarked vs naive)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# === PPO Environments ===\n",
    "class HighLevelEnv(gym.Env):\n",
    "    def __init__(self, X, regimes):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.regimes = regimes\n",
    "        self.index = 0\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(X.shape[1],))\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        return self.X[self.index]\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 1 if action == self.regimes[self.index] else -1\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.X)-1\n",
    "        obs = self.X[self.index] if not done else np.zeros(self.X.shape[1])\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "class LowLevelBenchmarkEnv(gym.Env):\n",
    "    def __init__(self, returns, window_size=21, lambda_risk=0.5, alpha=0.05):\n",
    "        super().__init__()\n",
    "        self.returns = returns\n",
    "        self.window_size = window_size\n",
    "        self.lambda_risk = lambda_risk\n",
    "        self.alpha = alpha\n",
    "        self.n_assets = returns.shape[1]\n",
    "        self.index = window_size\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(window_size, self.n_assets))\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,))\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = self.window_size\n",
    "        return self.returns[self.index-self.window_size:self.index]\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, 0, 1)\n",
    "        action /= np.sum(action + 1e-8)\n",
    "\n",
    "        # Agent and naive returns\n",
    "        agent_return = np.dot(action, self.returns[self.index])\n",
    "        naive_weights = np.array([1/self.n_assets] * self.n_assets)\n",
    "        naive_return = np.dot(naive_weights, self.returns[self.index])\n",
    "\n",
    "        # Excess return explicitly benchmarking naive portfolio\n",
    "        excess_return = agent_return - naive_return\n",
    "\n",
    "        historical_returns = self.returns[self.index-self.window_size:self.index] @ action\n",
    "        cvar_threshold = np.percentile(historical_returns, 100*self.alpha)\n",
    "        cvar_penalty = max(cvar_threshold - agent_return, 0)\n",
    "\n",
    "        # Reward explicitly includes naive portfolio benchmark\n",
    "        reward = excess_return - self.lambda_risk * cvar_penalty\n",
    "\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.returns)-1\n",
    "        obs = self.returns[self.index-self.window_size:self.index] if not done else np.zeros((self.window_size, self.n_assets))\n",
    "\n",
    "        return obs, reward, done, {\n",
    "            \"agent_return\": agent_return,\n",
    "            \"naive_return\": naive_return,\n",
    "            \"excess_return\": excess_return,\n",
    "            \"cvar_penalty\": cvar_penalty\n",
    "        }\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd5bbd7c46972ae2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# start from here\n",
    "# start from here\n",
    "# start from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta\n",
    "\n",
    "# === Load daily returns ===\n",
    "returns_df = pd.read_csv(\"daily_returns.csv\", parse_dates=[\"Date\"])\n",
    "returns_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Convert daily returns explicitly into price series (starting at 100)\n",
    "price_df = (1 + returns_df).cumprod() * 100\n",
    "\n",
    "# Define function explicitly to generate indicators per stock\n",
    "def generate_stock_features(df_prices):\n",
    "    df = pd.DataFrame(index=df_prices.index)\n",
    "    df['Price'] = df_prices\n",
    "    \n",
    "    # Daily returns explicitly\n",
    "    df['Return_1d'] = df['Price'].pct_change()\n",
    "    df['Return_5d'] = df['Price'].pct_change(5)\n",
    "    df['Return_21d'] = df['Price'].pct_change(21)\n",
    "    \n",
    "    # Volatility explicitly calculated\n",
    "    df['Volatility_5d'] = df['Return_1d'].rolling(window=5).std()\n",
    "    df['Volatility_21d'] = df['Return_1d'].rolling(window=21).std()\n",
    "    \n",
    "    # Drawdown explicitly calculated\n",
    "    df['Drawdown'] = df['Price'] / df['Price'].cummax() - 1\n",
    "    \n",
    "    # Technical indicators explicitly calculated\n",
    "    df['SMA_20'] = ta.trend.sma_indicator(df['Price'], window=20)\n",
    "    df['SMA_50'] = ta.trend.sma_indicator(df['Price'], window=50)\n",
    "    df['EMA_12'] = ta.trend.ema_indicator(df['Price'], window=12)\n",
    "    df['EMA_26'] = ta.trend.ema_indicator(df['Price'], window=26)\n",
    "    df['MACD'] = ta.trend.macd_diff(df['Price'])\n",
    "    df['RSI_14'] = ta.momentum.rsi(df['Price'], window=14)\n",
    "    \n",
    "    bb_indicator = ta.volatility.BollingerBands(df['Price'], window=20)\n",
    "    df['BB_High'] = bb_indicator.bollinger_hband()\n",
    "    df['BB_Low'] = bb_indicator.bollinger_lband()\n",
    "    \n",
    "    atr_indicator = ta.volatility.AverageTrueRange(\n",
    "        high=df['Price'] * 1.01,\n",
    "        low=df['Price'] * 0.99,\n",
    "        close=df['Price'],\n",
    "        window=14\n",
    "    )\n",
    "    df['ATR_14'] = atr_indicator.average_true_range()\n",
    "    \n",
    "    # Drop NaNs explicitly (initial periods with insufficient data)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate features explicitly for all 28 stocks\n",
    "all_features = {}\n",
    "for ticker in price_df.columns:\n",
    "    print(f\"Generating factors explicitly for {ticker}...\")\n",
    "    stock_prices = price_df[ticker]\n",
    "    stock_features = generate_stock_features(stock_prices)\n",
    "    stock_features.columns = [f\"{ticker}_{col}\" for col in stock_features.columns]\n",
    "    all_features[ticker] = stock_features\n",
    "\n",
    "# Combine explicitly into single DataFrame\n",
    "features_df = pd.concat(all_features.values(), axis=1).dropna()\n",
    "\n",
    "# Save explicitly to CSV\n",
    "features_df.to_csv(\"stock_factors_28_stocks.csv\")\n",
    "print(\"âœ… Explicit factors generated and saved as 'stock_factors_28_stocks.csv'.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f5f5bbaaa007231",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# class HighLevelEnv(gym.Env):\n",
    "#     def __init__(self, X, regimes, returns):\n",
    "#         self.X = X.astype(np.float32)\n",
    "#         self.regimes = regimes\n",
    "#         self.returns = returns\n",
    "#         self.index = 0\n",
    "#         self.observation_space = spaces.Box(-np.inf, np.inf, shape=(X.shape[1],), dtype=np.float32)\n",
    "#         self.action_space = spaces.Discrete(3)  # explicit matching n_components\n",
    "# \n",
    "#     def reset(self, seed=None, options=None):\n",
    "#         self.index = 0\n",
    "#         return self.X[self.index], {}\n",
    "# \n",
    "#     def step(self, action):\n",
    "#         classification_reward = 1 if action == self.regimes[self.index] else -1\n",
    "#         financial_reward = np.mean(self.returns[self.index:self.index+predict_window])\n",
    "#         reward = 0.7 * classification_reward + 0.3 * financial_reward\n",
    "#         self.index += 1\n",
    "#         terminated = self.index >= len(self.X) - predict_window - 1\n",
    "#         obs = self.X[self.index] if not terminated else np.zeros(self.X.shape[1], dtype=np.float32)\n",
    "#         return obs, reward, terminated, False, {}\n",
    "\n",
    "class HighLevelEnv(gym.Env):\n",
    "    def __init__(self, X, regimes, consistency_window=5):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.regimes = regimes\n",
    "        self.index = 0\n",
    "        self.consistency_window = consistency_window\n",
    "        self.prev_actions = []\n",
    "        \n",
    "        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(X.shape[1],), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.index = 0\n",
    "        self.prev_actions = []\n",
    "        return self.X[self.index], {}\n",
    "\n",
    "    def step(self, action):\n",
    "        action = int(action) \n",
    "        classification_reward = 1 if action == self.regimes[self.index] else -1\n",
    "        \n",
    "        # Append the current action\n",
    "        self.prev_actions.append(action)\n",
    "        \n",
    "        # Compute consistency reward\n",
    "        consistency_reward = 0\n",
    "        if len(self.prev_actions) >= self.consistency_window:\n",
    "            recent_actions = self.prev_actions[-self.consistency_window:]\n",
    "            recent_regimes = self.regimes[self.index - self.consistency_window + 1 : self.index + 1]\n",
    "            \n",
    "            # Check consistency (all correct)\n",
    "            if np.array_equal(recent_actions, recent_regimes):\n",
    "                consistency_reward = 0.5  # bonus for consistent accuracy\n",
    "        \n",
    "        reward = classification_reward + consistency_reward\n",
    "        \n",
    "        self.index += 1\n",
    "        terminated = self.index >= len(self.X) - 1\n",
    "        obs = self.X[self.index] if not terminated else np.zeros(self.X.shape[1], dtype=np.float32)\n",
    "        \n",
    "        return obs, reward, terminated, False, {}\n",
    "    \n",
    "# class LowLevelPortfolioEnv(gym.Env):\n",
    "#     def __init__(self, features, returns, regimes, window_size=21, predict_window=5, lambda_risk=0.5, alpha=0.05):\n",
    "#         self.features = features.astype(np.float32)\n",
    "#         self.returns = returns.astype(np.float32)\n",
    "#         self.regimes = regimes\n",
    "#         self.window_size = window_size\n",
    "#         self.predict_window = predict_window\n",
    "#         self.lambda_risk = lambda_risk\n",
    "#         self.alpha = alpha\n",
    "#         self.n_assets = returns.shape[1]\n",
    "#         self.index = window_size\n",
    "# \n",
    "#         obs_dim = window_size * features.shape[1] + 3\n",
    "#         self.observation_space = spaces.Box(-np.inf, np.inf, shape=(obs_dim,), dtype=np.float32)\n",
    "#         self.action_space = spaces.Box(low=-1, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "#         self.regime_encoding = {0: [1,0,0], 1: [0,1,0], 2: [0,0,1]}\n",
    "# \n",
    "#     def reset(self, seed=None, options=None):\n",
    "#         super().reset(seed=seed)\n",
    "#         self.index = self.window_size\n",
    "#         return self._get_obs(), {}\n",
    "# \n",
    "#     def _get_obs(self):\n",
    "#         recent_features = self.features[self.index-self.window_size:self.index].flatten()\n",
    "#         regime_onehot = self.regime_encoding[self.regimes[self.index - 1]]  # Fixed indexing explicitly\n",
    "#         return np.concatenate([recent_features, regime_onehot]).astype(np.float32)\n",
    "# \n",
    "#     def step(self, action):\n",
    "#         action = (action + 1) / 2\n",
    "#         action = np.clip(action, 0, 1)\n",
    "#         action /= np.sum(action + 1e-8)\n",
    "# \n",
    "#         start_idx = self.index\n",
    "#         end_idx = min(self.index + self.predict_window, len(self.returns))\n",
    "# \n",
    "#         next_period_returns = self.returns[start_idx:end_idx]\n",
    "#         port_returns = next_period_returns @ action\n",
    "# \n",
    "#         # Use cumulative return explicitly as the reward\n",
    "#         cumulative_return = np.prod(1 + port_returns) - 1\n",
    "# \n",
    "#         # Metrics explicitly retained for additional analysis\n",
    "#         naive_returns = np.mean(next_period_returns, axis=1)\n",
    "#         cumulative_naive_return = np.prod(1 + naive_returns) - 1\n",
    "#         excess_return = cumulative_return - cumulative_naive_return\n",
    "#         cvar_threshold = np.percentile(port_returns, self.alpha * 100)\n",
    "#         mean_cvar_reward = np.mean(port_returns) - self.lambda_risk * max(cvar_threshold - np.mean(port_returns), 0)\n",
    "# \n",
    "#         reward = excess_return  # explicitly changed here\n",
    "# \n",
    "#         self.index += self.predict_window\n",
    "#         terminated = self.index >= len(self.returns) - self.predict_window\n",
    "# \n",
    "#         obs = self._get_obs() if not terminated else np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "# \n",
    "#         info = {\n",
    "#             \"cumulative_return\": cumulative_return,\n",
    "#             \"mean_cvar_reward\": mean_cvar_reward,\n",
    "#             \"excess_return\": excess_return\n",
    "#         }\n",
    "# \n",
    "#         return obs, float(reward), terminated, False, info\n",
    "# incoporate Shap in ENV\n",
    "class LowLevelPortfolioEnv(gym.Env):\n",
    "    def __init__(self, features, returns, shap_features, regimes, window_size=21, predict_window=5, lambda_risk=0.5, alpha=0.05):\n",
    "        self.features = features.astype(np.float32)\n",
    "        self.shap_features = shap_features.astype(np.float32)\n",
    "        self.returns = returns.astype(np.float32)\n",
    "        self.regimes = regimes\n",
    "        self.window_size = window_size\n",
    "        self.predict_window = predict_window\n",
    "        self.lambda_risk = lambda_risk\n",
    "        self.alpha = alpha\n",
    "        self.n_assets = returns.shape[1]\n",
    "        self.index = window_size\n",
    "\n",
    "        obs_dim = (window_size * features.shape[1]) + shap_features.shape[1] + 3\n",
    "        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(obs_dim,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "        self.regime_encoding = {0: [1,0,0], 1: [0,1,0], 2: [0,0,1]}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.index = self.window_size\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        recent_features = self.features[self.index - self.window_size:self.index].flatten()\n",
    "        current_shap_features = self.shap_features[self.index - 1]  # explicitly using the latest SHAP/DIA features\n",
    "        regime_onehot = self.regime_encoding[self.regimes[self.index - 1]]\n",
    "        return np.concatenate([recent_features, current_shap_features, regime_onehot]).astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = (action + 1) / 2\n",
    "        action = np.clip(action, 0, 1)\n",
    "        action /= np.sum(action + 1e-8)\n",
    "\n",
    "        start_idx = self.index\n",
    "        end_idx = min(self.index + self.predict_window, len(self.returns))\n",
    "\n",
    "        next_period_returns = self.returns[start_idx:end_idx]\n",
    "        port_returns = next_period_returns @ action\n",
    "\n",
    "        cumulative_return = np.prod(1 + port_returns) - 1\n",
    "        reward = cumulative_return\n",
    "\n",
    "        self.index += self.predict_window\n",
    "        terminated = self.index >= len(self.returns) - self.predict_window\n",
    "\n",
    "        obs = self._get_obs() if not terminated else np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "        info = {\"cumulative_return\": cumulative_return}\n",
    "\n",
    "        return obs, float(reward), terminated, False, info\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3a88542501c064b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import os\n",
    "\n",
    "# === Load Data Explicitly ===\n",
    "shap_df = pd.read_csv(\"shap_with_DIA_indicators.csv\", parse_dates=[\"End_Date\"])\n",
    "returns_df = pd.read_csv(\"daily_returns_DIA.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "\n",
    "data = pd.merge(shap_df, returns_df, left_on=\"End_Date\", right_index=True).dropna()\n",
    "\n",
    "features = [\n",
    "    'mean_abs_shap_Mkt-RF', 'mean_abs_shap_SMB', 'mean_abs_shap_HML',\n",
    "    'mean_abs_shap_RMW', 'mean_abs_shap_CMA', 'mean_abs_shap_Stock',\n",
    "    'mean_shap_Mkt-RF', 'mean_shap_SMB', 'mean_shap_HML', 'mean_shap_RMW',\n",
    "    'mean_shap_CMA', 'mean_shap_Stock', 'shap_std_Mkt-RF', 'shap_std_SMB',\n",
    "    'shap_std_HML', 'shap_std_RMW', 'shap_std_CMA', 'shap_std_Stock',\n",
    "    'mean_over_std_Mkt-RF', 'mean_over_std_SMB', 'mean_over_std_HML',\n",
    "    'mean_over_std_RMW', 'mean_over_std_CMA', 'mean_over_std_Stock',\n",
    "    'mean_abs_over_std_Mkt-RF', 'mean_abs_over_std_SMB',\n",
    "    'mean_abs_over_std_HML', 'mean_abs_over_std_RMW',\n",
    "    'mean_abs_over_std_CMA', 'mean_abs_over_std_Stock',\n",
    "    'Return_1d', 'Return_5d', 'Rolling_Return_21d',\n",
    "    'Volatility_5d', 'Volatility_21d', 'Drawdown',\n",
    "    'SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', 'MACD', 'RSI_14'\n",
    "]\n",
    "\n",
    "# Parameters\n",
    "train_window, predict_window, step_size, consistency_window = 360, 5, 5, 5\n",
    "model_dir = \"high_level_saved_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "retrain = False\n",
    "# Explicit PPO training function\n",
    "def train_or_load_agent(env, model_path, total_timesteps, continue_training=False):\n",
    "    if continue_training and os.path.exists(model_path):\n",
    "        print(f\"âœ… Loading existing model from {model_path}\")\n",
    "        agent = PPO.load(model_path, env=env)\n",
    "    else:\n",
    "        print(\"âœ… Initializing new PPO agent.\")\n",
    "        agent = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "    print(f\"âœ… Training for {total_timesteps} timesteps.\")\n",
    "    agent.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "    print(f\"âœ… Saving model explicitly to {model_path}\")\n",
    "    agent.save(model_path)\n",
    "\n",
    "    return agent\n",
    "\n",
    "regime_results, accuracy_log = [], []\n",
    "# Rolling-window explicitly defined\n",
    "for start in range(0, len(data) - train_window - predict_window, step_size):\n",
    "# for start in range(0, 400, step_size):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_data = data.iloc[start:start + train_window].copy()\n",
    "    predict_data = data.iloc[start + train_window:start + train_window + predict_window].copy()\n",
    "\n",
    "    X_train_hl = train_data[features].values\n",
    "    X_predict_hl = predict_data[features].values\n",
    "\n",
    "    scaler_hl = StandardScaler()\n",
    "    X_train_hl_scaled = scaler_hl.fit_transform(X_train_hl)\n",
    "    X_predict_hl_scaled = scaler_hl.transform(X_predict_hl)\n",
    "\n",
    "    gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "    regimes_train = gmm.fit_predict(X_train_hl_scaled)\n",
    "\n",
    "    regimes_train_next = np.roll(regimes_train, -step_size)\n",
    "    X_train_aligned = X_train_hl_scaled[:-step_size]\n",
    "    regimes_train_aligned = regimes_train_next[:-step_size]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    clf.fit(X_train_aligned, regimes_train_aligned)\n",
    "    supervised_pred = clf.predict(X_predict_hl_scaled)\n",
    "\n",
    "    env_high = HighLevelEnv(X_train_hl_scaled, regimes_train_next, consistency_window)\n",
    "\n",
    "    model_path = os.path.join(model_dir, f\"ppo_model_step_{start}.zip\")\n",
    "    initial_iterations = 2000\n",
    "\n",
    "    high_level_agent = train_or_load_agent(\n",
    "        env=env_high,\n",
    "        model_path=model_path,\n",
    "        total_timesteps=initial_iterations,\n",
    "        # continue_training=os.path.exists(model_path)\n",
    "        continue_training=retrain\n",
    "        \n",
    "    )\n",
    "\n",
    "    predicted_regimes = [int(high_level_agent.predict(obs, deterministic=True)[0]) for obs in X_predict_hl_scaled]\n",
    "\n",
    "    final_predicted_regimes = [\n",
    "        ppo_pred if ppo_pred == sup_pred else sup_pred\n",
    "        for ppo_pred, sup_pred in zip(predicted_regimes, supervised_pred)\n",
    "    ]\n",
    "\n",
    "    regimes_predict_actual = gmm.predict(X_predict_hl_scaled)\n",
    "    accuracy = np.mean(final_predicted_regimes == regimes_predict_actual)\n",
    "    accuracy_log.append(accuracy)\n",
    "\n",
    "    regime_results.append(pd.DataFrame({\n",
    "        'Date': predict_data['End_Date'].values,\n",
    "        'Predicted_Regime': final_predicted_regimes,\n",
    "        'Actual_Regime': regimes_predict_actual\n",
    "    }))\n",
    "\n",
    "    print(f\"âœ… Step {start}: Trained from {train_data['End_Date'].iloc[0].date()} \"\n",
    "          f\"to {train_data['End_Date'].iloc[-1].date()}, \"\n",
    "          f\"predicted starting {predict_data['End_Date'].iloc[0].date()}, Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "final_results = pd.concat(regime_results, ignore_index=True)\n",
    "final_results.to_csv(\"high_level_rolling_predictions_with_actual.csv\", index=False)\n",
    "\n",
    "accuracy_overall = np.mean(accuracy_log)\n",
    "print(f\"âœ… Overall accuracy: {accuracy_overall:.2%}\")\n",
    "print(\"Results saved to 'high_level_rolling_predictions_with_actual.csv'\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "924b00e151578be0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cee1601d1ea041e7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from stable_baselines3 import PPO\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load Data Explicitly\n",
    "shap_df = pd.read_csv(\"shap_with_DIA_indicators.csv\", parse_dates=[\"End_Date\"])\n",
    "returns_df = pd.read_csv(\"daily_returns_DIA.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "data = pd.merge(shap_df, returns_df, left_on=\"End_Date\", right_index=True).dropna()\n",
    "\n",
    "features = [\n",
    "    'mean_abs_shap_Mkt-RF', 'mean_abs_shap_SMB', 'mean_abs_shap_HML',\n",
    "    'mean_abs_shap_RMW', 'mean_abs_shap_CMA', 'mean_abs_shap_Stock',\n",
    "    'mean_shap_Mkt-RF', 'mean_shap_SMB', 'mean_shap_HML', 'mean_shap_RMW',\n",
    "    'mean_shap_CMA', 'mean_shap_Stock', 'shap_std_Mkt-RF', 'shap_std_SMB',\n",
    "    'shap_std_HML', 'shap_std_RMW', 'shap_std_CMA', 'shap_std_Stock',\n",
    "    'Return_1d', 'Return_5d', 'Rolling_Return_21d',\n",
    "    'Volatility_5d', 'Volatility_21d', 'Drawdown',\n",
    "    'SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', 'MACD', 'RSI_14'\n",
    "]\n",
    "\n",
    "train_window, predict_window, step_size = 360, 10, 10\n",
    "model_dir = \"high_level_saved_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "class HighLevelEnv(gym.Env):\n",
    "    def __init__(self, X, next_regime, consistency_window=5):\n",
    "        self.X = X\n",
    "        self.next_regime = next_regime\n",
    "        self.consistency_window = consistency_window\n",
    "        self.index = 0\n",
    "        self.prev_actions = []\n",
    "\n",
    "        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(X.shape[1],), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.index = 0\n",
    "        self.prev_actions = []\n",
    "        return self.X[self.index], {}\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 1 if action == self.next_regime[self.index] else -1\n",
    "\n",
    "        self.prev_actions.append(action)\n",
    "        if len(self.prev_actions) >= self.consistency_window:\n",
    "            if np.all(self.prev_actions[-self.consistency_window:] == \n",
    "                      self.next_regime[self.index - self.consistency_window + 1: self.index + 1]):\n",
    "                reward += 0.5\n",
    "\n",
    "        self.index += 1\n",
    "        terminated = self.index >= len(self.X) - 1\n",
    "        obs = self.X[self.index] if not terminated else np.zeros(self.X.shape[1], dtype=np.float32)\n",
    "        return obs, reward, terminated, False, {}\n",
    "\n",
    "regime_results, accuracy_log = [], []\n",
    "\n",
    "# Define absolute thresholds explicitly\n",
    "negative_threshold, positive_threshold = -0.001, 0.001  # Customize these thresholds as needed\n",
    "\n",
    "for start in range(0, len(data) - train_window - predict_window, step_size):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_data = data.iloc[start:start + train_window].copy()\n",
    "    predict_data = data.iloc[start + train_window:start + train_window + predict_window].copy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(train_data[features].values)\n",
    "    X_predict = scaler.transform(predict_data[features].values)\n",
    "    \n",
    "    # Explicit regime definition (absolute returns)\n",
    "    regime_features = train_data[['Return_1d']].values.flatten()\n",
    "    regimes_current = np.where(regime_features < negative_threshold, 0,\n",
    "                        np.where(regime_features < positive_threshold, 1, 2))\n",
    "    \n",
    "    # Shifted target explicitly for future prediction\n",
    "    regimes_next_period = np.roll(regimes_current, -predict_window)\n",
    "    regimes_next_period_aligned = regimes_next_period[:-predict_window]\n",
    "    X_train_aligned = X_train[:-predict_window]\n",
    "    \n",
    "    env_high = HighLevelEnv(X_train_aligned, regimes_next_period_aligned)\n",
    "    model_path = os.path.join(model_dir, f\"ppo_model_step_{start}.zip\")\n",
    "\n",
    "    agent = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env_high,\n",
    "        learning_rate=1e-4,\n",
    "        gamma=0.98,\n",
    "        n_steps=1024,\n",
    "        ent_coef=0.01,\n",
    "        verbose=0\n",
    "    )\n",
    "    agent.learn(total_timesteps=5000)\n",
    "    agent.save(model_path)\n",
    "\n",
    "    predicted_regimes = []\n",
    "    for obs in X_predict:\n",
    "        obs_tensor = agent.policy.obs_to_tensor(obs.reshape(1, -1))[0]\n",
    "        distribution = agent.policy.get_distribution(obs_tensor)\n",
    "        action_probs = distribution.distribution.probs.detach().cpu().numpy()[0]\n",
    "\n",
    "        predicted_regimes.append(np.argmax(action_probs))\n",
    "\n",
    "    true_future_returns = predict_data['DIA'].values\n",
    "    true_future_regimes = np.where(true_future_returns < negative_threshold, 0,\n",
    "                             np.where(true_future_returns < positive_threshold, 1, 2))\n",
    "\n",
    "    accuracy = np.mean(predicted_regimes == true_future_regimes)\n",
    "    accuracy_log.append(accuracy)\n",
    "\n",
    "    regime_results.append(pd.DataFrame({\n",
    "        'Date': predict_data['End_Date'],\n",
    "        'Predicted_Regime': predicted_regimes,\n",
    "        'Actual_Regime': true_future_regimes,\n",
    "        'True_DIA_Return': true_future_returns\n",
    "    }))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Trained from {train_data['End_Date'].iloc[0].date()} to {train_data['End_Date'].iloc[-1].date()}, \"\n",
    "          f\"Predicted starting {predict_data['End_Date'].iloc[0].date()}, Accuracy: {accuracy:.2%}, \"\n",
    "          f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "final_results = pd.concat(regime_results, ignore_index=True)\n",
    "final_results.to_csv(\"high_level_predictions_with_actual.csv\", index=False)\n",
    "print(f\"Overall accuracy: {np.mean(accuracy_log):.2%}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36d50b9c9127767d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = data.iloc[start:start + train_window].copy()\n",
    "predict_data = data.iloc[start + train_window:start + train_window + predict_window].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data[features].values)\n",
    "X_predict = scaler.transform(predict_data[features].values)\n",
    "\n",
    "# Explicit regime definition (absolute returns)\n",
    "regime_features = train_data[['Return_1d']].values.flatten()\n",
    "regimes_current = np.where(regime_features < negative_threshold, 0,\n",
    "                    np.where(regime_features < positive_threshold, 1, 2))\n",
    "\n",
    "# Shifted target explicitly for future prediction\n",
    "regimes_next_period = np.roll(regimes_current, -predict_window)\n",
    "regimes_next_period_aligned = regimes_next_period[:-predict_window]\n",
    "X_train_aligned = X_train[:-predict_window]\n",
    "\n",
    "env_high = HighLevelEnv(X_train_aligned, regimes_next_period_aligned)\n",
    "model_path = os.path.join(model_dir, f\"ppo_model_step_{start}.zip\")\n",
    "\n",
    "agent = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env_high,\n",
    "    learning_rate=1e-4,\n",
    "    gamma=0.98,\n",
    "    n_steps=1024,\n",
    "    ent_coef=0.01,\n",
    "    verbose=0\n",
    ")\n",
    "agent.learn(total_timesteps=5000)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "989b44d9a007d8a3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predict_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61d5ad9587165b64",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predicted_regimes = []\n",
    "for obs in X_predict:\n",
    "    obs_tensor = agent.policy.obs_to_tensor(obs.reshape(1, -1))[0]\n",
    "    distribution = agent.policy.get_distribution(obs_tensor)\n",
    "    action_probs = distribution.distribution.probs.detach().cpu().numpy()[0]\n",
    "\n",
    "    predicted_regimes.append(np.argmax(action_probs))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a7f4ee151ca06d9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# === Explicit Data Loading ===\n",
    "stock_factors = pd.read_csv(\"stock_factors_28_stocks.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "daily_returns = pd.read_csv(\"daily_returns.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "high_level_preds = pd.read_csv(\"high_level_rolling_predictions_with_actual_sequence.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "\n",
    "stock_factors.index = pd.to_datetime(stock_factors.index)\n",
    "daily_returns.index = pd.to_datetime(daily_returns.index)\n",
    "high_level_preds.index = pd.to_datetime(high_level_preds.index)\n",
    "\n",
    "# Parameters\n",
    "train_window = 21*6\n",
    "predict_window = 5\n",
    "step_size = 5\n",
    "window_size = 21\n",
    "model_dir = \"ppo_low_level_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Explicit retraining indicator\n",
    "retrain_existing_model = False   # Set False for initial training, True to continue training existing models\n",
    "initial_iterations = 10000       # Initial training steps\n",
    "additional_iterations = 5000   # Incremental retraining steps\n",
    "\n",
    "\n",
    "portfolio_results = []\n",
    "\n",
    "predicted_regimes_series = high_level_preds['Predicted_Regime'].reindex(stock_factors.index, method='ffill')\n",
    "predicted_regimes_series = predicted_regimes_series.ffill().dropna()\n",
    "\n",
    "# Explicitly align stock_factors and daily_returns with regimes to ensure identical dates\n",
    "common_dates = stock_factors.index.intersection(predicted_regimes_series.index)\n",
    "stock_factors = stock_factors.loc[common_dates]\n",
    "daily_returns = daily_returns.loc[common_dates]\n",
    "predicted_regimes_series = predicted_regimes_series.loc[common_dates]\n",
    "dates = stock_factors.index.unique()\n",
    "# Explicit PPO training function\n",
    "def train_or_load_agent(env, model_path, iterations, retrain=False):\n",
    "    if retrain and os.path.exists(model_path):\n",
    "        print(f\"âœ… Loading existing model explicitly from {model_path}\")\n",
    "        agent = PPO.load(model_path, env=env)\n",
    "    else:\n",
    "        print(\"âœ… Initializing new PPO agent explicitly.\")\n",
    "        agent = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "    print(f\"âœ… Explicitly training for {iterations} timesteps.\")\n",
    "    agent.learn(total_timesteps=iterations)\n",
    "\n",
    "    print(f\"âœ… Saving model explicitly to {model_path}\")\n",
    "    agent.save(model_path)\n",
    "    return agent\n",
    "\n",
    "for start in range(window_size, len(dates) - train_window - predict_window, step_size):\n",
    "# for start in range(window_size, 100, step_size):    \n",
    "    start_time = time.time()\n",
    "    train_dates = dates[start : start + train_window]\n",
    "    predict_dates = dates[start + train_window : start + train_window + predict_window]\n",
    "\n",
    "    # Explicit training data (no leakage)\n",
    "    X_train = stock_factors.loc[train_dates].values\n",
    "    y_train = daily_returns.loc[train_dates].values\n",
    "    train_regime = predicted_regimes_series.loc[train_dates[-1]]\n",
    "\n",
    "    # Explicit prediction data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    env_train = LowLevelPortfolioEnv(\n",
    "        features=X_train_scaled,\n",
    "        returns=y_train,\n",
    "        regimes=np.full(len(X_train_scaled), train_regime),\n",
    "        window_size=window_size,\n",
    "        predict_window=predict_window\n",
    "    )\n",
    "    check_env(env_train)\n",
    "\n",
    "    # Explicit model path naming\n",
    "    model_path = os.path.join(model_dir, f\"low_level_ppo_step_{start}.zip\")\n",
    "\n",
    "    iterations = additional_iterations if retrain_existing_model else initial_iterations\n",
    "    low_level_agent = train_or_load_agent(\n",
    "        env=env_train,\n",
    "        model_path=model_path,\n",
    "        iterations=iterations,\n",
    "        retrain=retrain_existing_model\n",
    "    )\n",
    "\n",
    "    # Explicit prediction preparation (no data leakage)\n",
    "    hist_start_idx = start + train_window - window_size\n",
    "    hist_end_idx = start + train_window\n",
    "    X_predict_hist = stock_factors.iloc[hist_start_idx:hist_end_idx].values\n",
    "    y_predict_hist = daily_returns.iloc[hist_start_idx:hist_end_idx].values\n",
    "    X_predict_hist_scaled = scaler.transform(X_predict_hist)\n",
    "    predict_regime = predicted_regimes_series.iloc[hist_end_idx - 1]\n",
    "\n",
    "    env_predict = LowLevelPortfolioEnv(\n",
    "        features=X_predict_hist_scaled,\n",
    "        returns=y_predict_hist,\n",
    "        regimes=np.full(window_size, predict_regime),\n",
    "        window_size=window_size,\n",
    "        predict_window=predict_window\n",
    "    )\n",
    "\n",
    "    obs, _ = env_predict.reset()\n",
    "\n",
    "    action, _ = low_level_agent.predict(obs, deterministic=True)\n",
    "    action = np.clip(action, 0, 1)\n",
    "    action /= np.sum(action + 1e-8)\n",
    "\n",
    "    # Evaluate explicit reward and metrics using predicted action\n",
    "    y_predict_next_period = daily_returns.loc[predict_dates].values\n",
    "\n",
    "    next_period_returns = y_predict_next_period @ action\n",
    "    cumulative_return = np.prod(1 + next_period_returns) - 1\n",
    "    cvar_threshold = np.percentile(next_period_returns, env_predict.alpha * 100)\n",
    "    mean_cvar_reward = np.mean(next_period_returns) - env_predict.lambda_risk * max(cvar_threshold - np.mean(next_period_returns), 0)\n",
    "    \n",
    "    # Explicitly store comprehensive results in DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        'Train_Start_Date': [train_dates[0].date()],\n",
    "        'Train_End_Date': [train_dates[-1].date()],\n",
    "        'Predict_Start_Date': [predict_dates[0].date()],\n",
    "        'Predict_End_Date': [predict_dates[-1].date()],\n",
    "        'Mean_CVaR_Reward': [mean_cvar_reward],\n",
    "        'Cumulative_Return': [cumulative_return],\n",
    "    })\n",
    "    \n",
    "    # Explicitly add portfolio weights\n",
    "    for i, asset in enumerate(daily_returns.columns):\n",
    "        result_df[f'Weight_{asset}'] = action[i]\n",
    "    \n",
    "    portfolio_results.append(result_df)\n",
    "\n",
    "    print(f\"âœ… Trained from {train_dates[0].date()} to {train_dates[-1].date()}, \"\n",
    "          f\"allocated portfolio from {predict_dates[0].date()} to {predict_dates[-1].date()}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "# Explicit consolidation and export\n",
    "if portfolio_results:\n",
    "    final_allocations = pd.concat(portfolio_results)\n",
    "    final_allocations.to_csv(\"low_level_optimal_portfolio_allocations.csv\")\n",
    "    print(\"âœ… Portfolio allocations explicitly saved to 'low_level_optimal_portfolio_allocations.csv'.\")\n",
    "else:\n",
    "    print(\"âš ï¸ No allocations generated; verify data alignment and parameters.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da37784e42d5cd55",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "## low level agent to include shap related metrics in the training and prediction ##\n",
    "####################################################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# === Explicit Data Loading ===\n",
    "stock_factors = pd.read_csv(\"stock_factors_28_stocks.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "daily_returns = pd.read_csv(\"daily_returns.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "high_level_preds = pd.read_csv(\"high_level_rolling_predictions_with_actual_sequence.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "\n",
    "stock_factors.index = pd.to_datetime(stock_factors.index)\n",
    "daily_returns.index = pd.to_datetime(daily_returns.index)\n",
    "high_level_preds.index = pd.to_datetime(high_level_preds.index)\n",
    "\n",
    "# Parameters\n",
    "train_window = 21*6\n",
    "predict_window = 5\n",
    "step_size = 5\n",
    "window_size = 21\n",
    "model_dir = \"ppo_low_level_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Explicit retraining indicator\n",
    "retrain_existing_model = False   # Set False for initial training, True to continue training existing models\n",
    "initial_iterations = 5000       # Initial training steps\n",
    "additional_iterations = 5000   # Incremental retraining steps\n",
    "\n",
    "\n",
    "portfolio_results = []\n",
    "\n",
    "predicted_regimes_series = high_level_preds['Predicted_Regime'].reindex(stock_factors.index, method='ffill')\n",
    "predicted_regimes_series = predicted_regimes_series.ffill().dropna()\n",
    "\n",
    "shap_df = pd.read_csv(\"shap_with_DIA_indicators.csv\", parse_dates=[\"End_Date\"]).set_index(\"End_Date\")\n",
    "shap_df.index = pd.to_datetime(shap_df.index)\n",
    "\n",
    "# Explicitly define the specific numeric columns you need\n",
    "selected_shap_columns = [\n",
    "    'mean_abs_shap_Mkt-RF', 'mean_abs_shap_SMB', 'mean_abs_shap_HML',\n",
    "    'mean_abs_shap_RMW', 'mean_abs_shap_CMA', 'mean_abs_shap_Stock',\n",
    "    'mean_shap_Mkt-RF', 'mean_shap_SMB', 'mean_shap_HML', 'mean_shap_RMW',\n",
    "    'mean_shap_CMA', 'mean_shap_Stock', 'shap_std_Mkt-RF', 'shap_std_SMB',\n",
    "    'shap_std_HML', 'shap_std_RMW', 'shap_std_CMA', 'shap_std_Stock',\n",
    "    'mean_over_std_Mkt-RF', 'mean_over_std_SMB', 'mean_over_std_HML',\n",
    "    'mean_over_std_RMW', 'mean_over_std_CMA', 'mean_over_std_Stock',\n",
    "    'mean_abs_over_std_Mkt-RF', 'mean_abs_over_std_SMB',\n",
    "    'mean_abs_over_std_HML', 'mean_abs_over_std_RMW',\n",
    "    'mean_abs_over_std_CMA', 'mean_abs_over_std_Stock',\n",
    "    'Return_1d', 'Return_5d', 'Rolling_Return_21d',\n",
    "    'Volatility_5d', 'Volatility_21d', 'Drawdown',\n",
    "    'SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', 'MACD', 'RSI_14'\n",
    "]\n",
    "\n",
    "# Explicitly filtering the dataframe\n",
    "shap_df = shap_df[selected_shap_columns]\n",
    "# Explicitly align all datasets\n",
    "common_dates = stock_factors.index.intersection(shap_df.index).intersection(predicted_regimes_series.index)\n",
    "\n",
    "stock_factors = stock_factors.loc[common_dates]\n",
    "daily_returns = daily_returns.loc[common_dates]\n",
    "predicted_regimes_series = predicted_regimes_series.loc[common_dates]\n",
    "shap_df = shap_df.loc[common_dates]\n",
    "\n",
    "dates = stock_factors.index.unique()\n",
    "# Explicit PPO training function\n",
    "def train_or_load_agent(env, model_path, iterations, retrain=False):\n",
    "    if retrain and os.path.exists(model_path):\n",
    "        print(f\"âœ… Loading existing model explicitly from {model_path}\")\n",
    "        agent = PPO.load(model_path, env=env)\n",
    "    else:\n",
    "        print(\"âœ… Initializing new PPO agent explicitly.\")\n",
    "        agent = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "    print(f\"âœ… Explicitly training for {iterations} timesteps.\")\n",
    "    agent.learn(total_timesteps=iterations)\n",
    "\n",
    "    print(f\"âœ… Saving model explicitly to {model_path}\")\n",
    "    agent.save(model_path)\n",
    "    return agent\n",
    "\n",
    "for start in range(window_size, len(dates) - train_window - predict_window, step_size):\n",
    "# for start in range(window_size, 100, step_size):    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_dates = dates[start : start + train_window]\n",
    "    predict_dates = dates[start + train_window : start + train_window + predict_window]\n",
    "\n",
    "    # Explicit training data (no leakage)\n",
    "    X_train = stock_factors.loc[train_dates].values\n",
    "    y_train = daily_returns.loc[train_dates].values\n",
    "    shap_train = shap_df.loc[train_dates].values\n",
    "    train_regime = predicted_regimes_series.loc[train_dates[-1]]\n",
    "\n",
    "    # Scale stock features explicitly (SHAP features typically don't require scaling, but can be scaled if preferred)\n",
    "    scaler_stock = StandardScaler()\n",
    "    X_train_scaled = scaler_stock.fit_transform(X_train)\n",
    "\n",
    "    env_train = LowLevelPortfolioEnv(\n",
    "        features=X_train_scaled,\n",
    "        returns=y_train,\n",
    "        shap_features=shap_train,\n",
    "        regimes=np.full(len(X_train_scaled), train_regime),\n",
    "        window_size=window_size,\n",
    "        predict_window=predict_window\n",
    "    )\n",
    "    check_env(env_train)\n",
    "\n",
    "    model_path = os.path.join(model_dir, f\"low_level_ppo_step_{start}.zip\")\n",
    "    iterations = additional_iterations if retrain_existing_model else initial_iterations\n",
    "\n",
    "    low_level_agent = train_or_load_agent(\n",
    "        env=env_train,\n",
    "        model_path=model_path,\n",
    "        iterations=iterations,\n",
    "        retrain=retrain_existing_model\n",
    "    )\n",
    "\n",
    "    # Explicit prediction preparation (historical window)\n",
    "    hist_start_idx = start + train_window - window_size\n",
    "    hist_end_idx = start + train_window\n",
    "\n",
    "    X_predict_hist = stock_factors.iloc[hist_start_idx:hist_end_idx].values\n",
    "    shap_predict_hist = shap_df.iloc[hist_start_idx:hist_end_idx].values\n",
    "    y_predict_hist = daily_returns.iloc[hist_start_idx:hist_end_idx].values\n",
    "\n",
    "    X_predict_hist_scaled = scaler_stock.transform(X_predict_hist)\n",
    "    predict_regime = predicted_regimes_series.iloc[hist_end_idx - 1]\n",
    "\n",
    "    env_predict = LowLevelPortfolioEnv(\n",
    "        features=X_predict_hist_scaled,\n",
    "        returns=y_predict_hist,\n",
    "        shap_features=shap_predict_hist,\n",
    "        regimes=np.full(window_size, predict_regime),\n",
    "        window_size=window_size,\n",
    "        predict_window=predict_window\n",
    "    )\n",
    "\n",
    "    obs, _ = env_predict.reset()\n",
    "    action, _ = low_level_agent.predict(obs, deterministic=True)\n",
    "    action = np.clip(action, 0, 1)\n",
    "    action /= np.sum(action + 1e-8)\n",
    "\n",
    "    # Evaluate reward explicitly\n",
    "    y_predict_next_period = daily_returns.loc[predict_dates].values\n",
    "    next_period_returns = y_predict_next_period @ action\n",
    "    cumulative_return = np.prod(1 + next_period_returns) - 1\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        'Train_Start_Date': [train_dates[0].date()],\n",
    "        'Train_End_Date': [train_dates[-1].date()],\n",
    "        'Predict_Start_Date': [predict_dates[0].date()],\n",
    "        'Predict_End_Date': [predict_dates[-1].date()],\n",
    "        'Cumulative_Return': [cumulative_return],\n",
    "    })\n",
    "\n",
    "    for i, asset in enumerate(daily_returns.columns):\n",
    "        result_df[f'Weight_{asset}'] = action[i]\n",
    "\n",
    "    portfolio_results.append(result_df)\n",
    "\n",
    "    print(f\"âœ… Trained from {train_dates[0].date()} to {train_dates[-1].date()}, \"\n",
    "          f\"allocated portfolio from {predict_dates[0].date()} to {predict_dates[-1].date()}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "# Explicit consolidation and export\n",
    "if portfolio_results:\n",
    "    final_allocations = pd.concat(portfolio_results)\n",
    "    final_allocations.to_csv(\"low_level_optimal_portfolio_allocations.csv\")\n",
    "    print(\"âœ… Portfolio allocations explicitly saved to 'low_level_optimal_portfolio_allocations.csv'.\")\n",
    "else:\n",
    "    print(\"âš ï¸ No allocations generated; verify data alignment and parameters.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b063bebf9dd5b264",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "shap_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11bb9dd2338a3574",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "85688412984eae51"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a2b4b6b479fa9662"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "492ca5d40135f0d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(dates) - train_window - predict_window"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a254cce636395d10",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# combined code use this!!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import time\n",
    "\n",
    "# Explicit Data Loading\n",
    "shap_df = pd.read_csv(\"shap_with_DIA_indicators.csv\", parse_dates=[\"End_Date\"])\n",
    "dia_returns_df = pd.read_csv(\"daily_returns_DIA.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "stock_returns_df = pd.read_csv(\"daily_returns.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "stock_factors = pd.read_csv(\"stock_factors_28_stocks.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "\n",
    "data = pd.merge(shap_df, dia_returns_df[['DIA']], left_on=\"End_Date\", right_index=True).dropna()\n",
    "\n",
    "# features = [col for col in shap_df.columns if col not in [\"End_Date\", \"DIA\"]]\n",
    "features = [\n",
    "    'mean_abs_shap_Mkt-RF', 'mean_abs_shap_SMB', 'mean_abs_shap_HML',\n",
    "    'mean_abs_shap_RMW', 'mean_abs_shap_CMA', 'mean_abs_shap_Stock',\n",
    "    'mean_shap_Mkt-RF', 'mean_shap_SMB', 'mean_shap_HML', 'mean_shap_RMW',\n",
    "    'mean_shap_CMA', 'mean_shap_Stock', 'shap_std_Mkt-RF', 'shap_std_SMB',\n",
    "    'shap_std_HML', 'shap_std_RMW', 'shap_std_CMA', 'shap_std_Stock',\n",
    "    'mean_over_std_Mkt-RF', 'mean_over_std_SMB', 'mean_over_std_HML',\n",
    "    'mean_over_std_RMW', 'mean_over_std_CMA', 'mean_over_std_Stock',\n",
    "    'mean_abs_over_std_Mkt-RF', 'mean_abs_over_std_SMB',\n",
    "    'mean_abs_over_std_HML', 'mean_abs_over_std_RMW',\n",
    "    'mean_abs_over_std_CMA', 'mean_abs_over_std_Stock',\n",
    "    'Return_1d', 'Return_5d', 'Rolling_Return_21d',\n",
    "    'Volatility_5d', 'Volatility_21d', 'Drawdown',\n",
    "    'SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', 'MACD', 'RSI_14'\n",
    "]\n",
    "# Parameters\n",
    "train_window, predict_window, step_size, consistency_window, window_size = 360, 5, 5, 5, 21\n",
    "portfolio_results, accuracy_log, regime_results = [], [], []\n",
    "\n",
    "# for start in range(0, len(data) - train_window - predict_window - window_size, step_size):\n",
    "for start in range(0, 60, step_size):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Data slicing (no leakage)\n",
    "    train_data = data.iloc[start:start + train_window].copy()\n",
    "    predict_data = data.iloc[start + train_window:start + train_window + predict_window].copy()\n",
    "\n",
    "    X_train_hl = train_data[features].values\n",
    "    X_predict_hl = predict_data[features].values\n",
    "\n",
    "    scaler_hl = StandardScaler()\n",
    "    X_train_hl_scaled = scaler_hl.fit_transform(X_train_hl)\n",
    "    X_predict_hl_scaled = scaler_hl.transform(X_predict_hl)\n",
    "\n",
    "    # Regime modeling explicitly on training data\n",
    "    gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "    regimes_train = gmm.fit_predict(X_train_hl_scaled)\n",
    "\n",
    "    regimes_train_next = np.roll(regimes_train, -step_size)\n",
    "    X_train_aligned = X_train_hl_scaled[:-step_size]\n",
    "    regimes_train_aligned = regimes_train_next[:-step_size]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    clf.fit(X_train_aligned, regimes_train_aligned)\n",
    "    supervised_pred = clf.predict(X_predict_hl_scaled)\n",
    "\n",
    "    env_high = HighLevelEnv(X_train_hl_scaled, regimes_train_next, consistency_window)\n",
    "    high_level_agent = PPO(\"MlpPolicy\", env_high, verbose=0)\n",
    "    high_level_agent.learn(total_timesteps=500)\n",
    "\n",
    "    predicted_regimes = []\n",
    "    for obs in X_predict_hl_scaled:\n",
    "        pred_regime, _ = high_level_agent.predict(obs, deterministic=True)\n",
    "        predicted_regimes.append(int(pred_regime))\n",
    "\n",
    "    final_predicted_regimes = [\n",
    "        ppo_pred if ppo_pred == sup_pred else sup_pred\n",
    "        for ppo_pred, sup_pred in zip(predicted_regimes, supervised_pred)\n",
    "    ]\n",
    "\n",
    "    # Accuracy logging\n",
    "    regimes_predict_actual = gmm.predict(X_predict_hl_scaled)\n",
    "    accuracy = np.mean(final_predicted_regimes == regimes_predict_actual)\n",
    "    accuracy_log.append(accuracy)\n",
    "\n",
    "    # Store predicted and actual regimes explicitly\n",
    "    regime_results.append(pd.DataFrame({\n",
    "        'Date': predict_data['End_Date'].values,\n",
    "        'Predicted_Regime': final_predicted_regimes,\n",
    "        'Actual_Regime': regimes_predict_actual\n",
    "    }))\n",
    "\n",
    "    # Low-level Data Alignment\n",
    "    train_dates = train_data['End_Date']\n",
    "    predict_dates = predict_data['End_Date']\n",
    "\n",
    "    X_train_ll = stock_factors.loc[train_dates].values\n",
    "    y_train_ll = stock_returns_df.loc[train_dates].values\n",
    "\n",
    "    X_predict_ll = stock_factors.loc[predict_dates].values\n",
    "    y_predict_ll = stock_returns_df.loc[predict_dates].values\n",
    "\n",
    "    scaler_ll = StandardScaler()\n",
    "    X_train_ll_scaled = scaler_ll.fit_transform(X_train_ll)\n",
    "    X_predict_ll_scaled = scaler_ll.transform(X_predict_ll)\n",
    "\n",
    "    train_regime = final_predicted_regimes[0]\n",
    "    predict_regime = final_predicted_regimes[-1]\n",
    "    print(f'train_regime = {train_regime} and predict_regime = {predict_regime}')\n",
    "    # env_low_train = LowLevelPortfolioEnv(\n",
    "    #     X_train_ll_scaled, y_train_ll,\n",
    "    #     regimes=np.full(len(X_train_ll_scaled), train_regime),\n",
    "    #     predict_window=predict_window, window_size=window_size\n",
    "    # )\n",
    "    # check_env(env_low_train)\n",
    "    # \n",
    "    # low_level_agent = PPO(\"MlpPolicy\", env_low_train, verbose=0)\n",
    "    # low_level_agent.learn(total_timesteps=500)\n",
    "    # \n",
    "    # predict_start_idx = stock_factors.index.get_loc(predict_dates.iloc[0])\n",
    "    # \n",
    "    # X_predict_hist = stock_factors.iloc[\n",
    "    #     predict_start_idx - window_size : predict_start_idx\n",
    "    # ].values\n",
    "    # \n",
    "    # y_predict_hist = stock_returns_df.iloc[\n",
    "    #     predict_start_idx - window_size : predict_start_idx\n",
    "    # ].values\n",
    "    # \n",
    "    # X_predict_hist_scaled = scaler_ll.transform(X_predict_hist)\n",
    "    # \n",
    "    # env_low_predict = LowLevelPortfolioEnv(\n",
    "    #     X_predict_hist_scaled, y_predict_hist,\n",
    "    #     regimes=np.full(len(X_predict_hist_scaled), predict_regime),\n",
    "    #     predict_window=predict_window, window_size=window_size\n",
    "    # )\n",
    "    # \n",
    "    # obs, _ = env_low_predict.reset()\n",
    "    # action, _ = low_level_agent.predict(obs, deterministic=True)\n",
    "    # action = np.clip(action, 0, 1)\n",
    "    # action /= np.sum(action + 1e-8)\n",
    "    # \n",
    "    # allocation_df = pd.DataFrame(\n",
    "    #     [action] * len(predict_dates),\n",
    "    #     columns=stock_returns_df.columns,\n",
    "    #     index=predict_dates\n",
    "    # )\n",
    "    # portfolio_results.append(allocation_df)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Logging\n",
    "    print(f\"Step {start}: High-level accuracy: {accuracy:.2%}, Time elapsed: {end_time - start_time:.2f}s\")\n",
    "\n",
    "# Final consolidation\n",
    "# final_allocations = pd.concat(portfolio_results)\n",
    "# final_allocations.to_csv(\"combined_portfolio_allocations.csv\")\n",
    "# \n",
    "# accuracy_overall = np.mean(accuracy_log)\n",
    "# print(f\"Overall high-level prediction accuracy: {accuracy_overall:.2%}\")\n",
    "# \n",
    "# final_regime_results = pd.concat(regime_results)\n",
    "# final_regime_results.to_csv(\"regime_predictions_actuals.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "166d995d5e1d945f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final_predicted_regimes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c187d2e4f488e1a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "HighLevelEnv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4f343e74ca9c6f6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Correctly shift regimes explicitly to predict future regime\n",
    "regimes_train_next = np.roll(regimes_train, -step_size)\n",
    "\n",
    "# Explicitly drop the last 'step_size' samples with unknown future regimes\n",
    "X_train_aligned = X_train[:-step_size]  # Explicitly drop last rows to align\n",
    "regimes_train_aligned = regimes_train_next[:-step_size]\n",
    "\n",
    "# Explicit supervised baseline without look-ahead bias\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "clf.fit(X_train_aligned, regimes_train_aligned)\n",
    "\n",
    "# Explicitly predict with your fitted classifier\n",
    "supervised_pred = clf.predict(X_predict)\n",
    "supervised_pred"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75d573751e620ab6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "hmm.fit(X_train)\n",
    "\n",
    "\n",
    "regimes_train = hmm.predict(X_train)\n",
    "regimes_predict_actual = hmm.predict(X_predict)\n",
    "\n",
    "\n",
    "# Correctly shift regimes explicitly to predict future regime\n",
    "regimes_train_next = np.roll(regimes_train, -step_size)\n",
    "\n",
    "# Explicitly drop the last 'step_size' samples with unknown future regimes\n",
    "X_train_aligned = X_train[:-step_size]  # Explicitly drop last rows to align\n",
    "regimes_train_aligned = regimes_train_next[:-step_size]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f1f08d940dbef9d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import hmmlearn\n",
    "print(\"hmmlearn updated version:\", hmmlearn.__version__)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63e2e400b38b9726",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Explicit Rolling-Window Training & Prediction\n",
    "portfolio_results = []\n",
    "\n",
    "# Ensure explicitly proper datetime indexing\n",
    "data.index = pd.to_datetime(data.index)\n",
    "dates = data.index.unique()\n",
    "\n",
    "train_window = 252\n",
    "predict_window = 5\n",
    "step_size = 5\n",
    "\n",
    "for start in range(0, len(dates) - train_window - predict_window, step_size):\n",
    "    train_dates = dates[start : start + train_window]\n",
    "    predict_dates = dates[start + train_window : start + train_window + predict_window]\n",
    "\n",
    "    # Check explicitly if data slices are empty\n",
    "    if len(train_dates) < train_window or len(predict_dates) < predict_window:\n",
    "        print(f\"Insufficient data at start index {start}, skipping explicitly.\")\n",
    "        continue\n",
    "\n",
    "    # Explicitly slice data\n",
    "    train_data = data.loc[train_dates].dropna()\n",
    "    predict_data = data.loc[predict_dates].dropna()\n",
    "\n",
    "    if train_data.empty or predict_data.empty:\n",
    "        print(f\"No available data in train or predict window at index {start}, explicitly skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Split explicitly into features, returns, and regimes\n",
    "    scaler = StandardScaler()\n",
    "    X_train_features = scaler.fit_transform(train_data.filter(like=\"_factors\"))\n",
    "    y_train_returns = train_data.filter(like=\"_returns\").values\n",
    "    train_regimes = train_data[\"Predicted_Regime\"].values\n",
    "\n",
    "    X_predict_features = scaler.transform(predict_data.filter(like=\"_factors\"))\n",
    "    y_predict_returns = predict_data.filter(like=\"_returns\").values\n",
    "    predict_regimes = predict_data[\"Predicted_Regime\"].values\n",
    "\n",
    "    if len(train_regimes) <= 5 or len(y_train_returns) <= 5:\n",
    "        print(f\"Not enough data points after filtering at index {start}, explicitly skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Create and train low-level agent explicitly\n",
    "    env = LowLevelPortfolioEnv(X_train_features, y_train_returns, train_regimes)\n",
    "    check_env(env)\n",
    "\n",
    "    low_level_agent = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "    low_level_agent.learn(total_timesteps=50000)\n",
    "\n",
    "    # Explicit out-of-sample portfolio allocation\n",
    "    env_predict = LowLevelPortfolioEnv(X_predict_features, y_predict_returns, predict_regimes)\n",
    "    obs, _ = env_predict.reset()\n",
    "\n",
    "    allocations = []\n",
    "    for day in range(len(predict_dates)):\n",
    "        action, _ = low_level_agent.predict(obs, deterministic=True)\n",
    "        action /= np.sum(action + 1e-8)\n",
    "        allocations.append(action)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env_predict.step(action)\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    if allocations:\n",
    "        allocations_df = pd.DataFrame(\n",
    "            allocations,\n",
    "            columns=predict_data.filter(like=\"_returns\").columns,\n",
    "            index=predict_dates[:len(allocations)]\n",
    "        )\n",
    "\n",
    "        portfolio_results.append(allocations_df)\n",
    "\n",
    "        print(f\"âœ… Trained explicitly from {train_dates[0].date()} to {train_dates[-1].date()}, \"\n",
    "              f\"allocated portfolio starting {predict_dates[0].date()}\")\n",
    "\n",
    "# Final explicit consolidation with explicit check\n",
    "if portfolio_results:\n",
    "    final_allocations = pd.concat(portfolio_results)\n",
    "    final_allocations.to_csv(\"low_level_portfolio_allocations.csv\")\n",
    "    print(\"âœ… Explicit rolling-window training and allocations completed.\")\n",
    "else:\n",
    "    print(\"âš ï¸ No allocations generated explicitly; check data alignment and window settings.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5db28dd7227bbfc1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# start from here\n",
    "# +-------------------------------+           +-------------------------------+\n",
    "# | DIA SHAP + Technical Indicators|           |    Daily Returns (28 Stocks)  |\n",
    "# | (from shap_with_DIA_indicators.csv)        |  (from daily_returns.csv)     |\n",
    "# +--------------+----------------+           +---------------+---------------+\n",
    "#                |                                            |\n",
    "#                v                                            v\n",
    "#         +---------------------+                  +--------------------------+\n",
    "#         |  High-Level Agent   |----------------->|   Low-Level PPO Agents   |\n",
    "#         |  (PPO classifier)   | Regime Signal    | (trained per market regime)|\n",
    "#         +---------------------+                  +--------------------------+\n",
    "#                |                                            |\n",
    "#                v                                            v\n",
    "#         Regime prediction                         Optimal portfolio weights\n",
    "#                                                   (Mean-CVaR reward explicitly benchmarked vs naive)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hmmlearn.hmm import GaussianHMM  # explicitly re-import after restart\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import time\n",
    "import logging\n",
    "# Load Data Explicitly\n",
    "shap_df = pd.read_csv(\"shap_with_DIA_indicators.csv\", parse_dates=[\"End_Date\"])\n",
    "returns_df = pd.read_csv(\"daily_returns.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "stock_factors = pd.read_csv(\"stock_factors_28_stocks.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "\n",
    "data = pd.merge(shap_df, returns_df, left_on=\"End_Date\", right_index=True).dropna()\n",
    "data.index = pd.to_datetime(data['End_Date'])\n",
    "\n",
    "# Explicit alignment of all data by dates\n",
    "combined_data = data.join(stock_factors, how='inner').dropna()\n",
    "\n",
    "features = [\n",
    "    'mean_abs_shap_Mkt-RF', 'mean_abs_shap_SMB', 'mean_abs_shap_HML',\n",
    "    'mean_abs_shap_RMW', 'mean_abs_shap_CMA', 'mean_abs_shap_Stock',\n",
    "    'mean_shap_Mkt-RF', 'mean_shap_SMB', 'mean_shap_HML', 'mean_shap_RMW',\n",
    "    'mean_shap_CMA', 'mean_shap_Stock', 'shap_std_Mkt-RF', 'shap_std_SMB',\n",
    "    'shap_std_HML', 'shap_std_RMW', 'shap_std_CMA', 'shap_std_Stock',\n",
    "    'mean_over_std_Mkt-RF', 'mean_over_std_SMB', 'mean_over_std_HML',\n",
    "    'mean_over_std_RMW', 'mean_over_std_CMA', 'mean_over_std_Stock',\n",
    "    'mean_abs_over_std_Mkt-RF', 'mean_abs_over_std_SMB',\n",
    "    'mean_abs_over_std_HML', 'mean_abs_over_std_RMW',\n",
    "    'mean_abs_over_std_CMA', 'mean_abs_over_std_Stock',\n",
    "    'Return_1d', 'Return_5d', 'Rolling_Return_21d',\n",
    "    'Volatility_5d', 'Volatility_21d', 'Drawdown',\n",
    "    'SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', 'MACD', 'RSI_14'\n",
    "]\n",
    "\n",
    "train_window = 252\n",
    "predict_window = 21\n",
    "step_size = 21\n",
    "\n",
    "portfolio_results = []\n",
    "high_level_results = []\n",
    "\n",
    "# for start in range(0, len(combined_data) - train_window - predict_window, step_size):\n",
    "for start in range(0, 200, step_size):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # === Explicit Data slicing ===\n",
    "    train_data = combined_data.iloc[start : start + train_window].copy()\n",
    "    predict_data = combined_data.iloc[start + train_window : start + train_window + predict_window].copy()\n",
    "\n",
    "    # High-Level HMM explicitly fitted on historical data\n",
    "    # === Step 2: Explicit HMM regime labeling ONLY on past training data ===\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(train_data[features])\n",
    "    X_predict = scaler.transform(predict_data[features])\n",
    "    \n",
    "    X_train = np.nan_to_num(X_train)\n",
    "    X_predict = np.nan_to_num(X_predict)\n",
    "    \n",
    "    hmm = GaussianHMM(\n",
    "        n_components=3,         # explicitly fewer states, more stable\n",
    "        covariance_type=\"diag\", # explicitly more stable covariance\n",
    "        n_iter=5000,            # explicit sufficient iterations for convergence\n",
    "        tol=1e-4,               # explicit convergence tolerance\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    hmm.fit(X_train)\n",
    "    \n",
    "    \n",
    "    regimes_train = hmm.predict(X_train)\n",
    "    regimes_predict_actual = hmm.predict(X_predict)\n",
    "    \n",
    "    \n",
    "    # Correctly shift regimes explicitly to predict future regime\n",
    "    regimes_train_next = np.roll(regimes_train, -step_size)\n",
    "    \n",
    "    # Explicitly drop the last 'step_size' samples with unknown future regimes\n",
    "    X_train_aligned = X_train[:-step_size]  # Explicitly drop last rows to align\n",
    "    regimes_train_aligned = regimes_train_next[:-step_size]\n",
    "    \n",
    "    # Explicit supervised baseline without look-ahead bias\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    clf.fit(X_train_aligned, regimes_train_aligned)\n",
    "    \n",
    "    # Explicitly predict with your fitted classifier\n",
    "    supervised_pred = clf.predict(X_predict)\n",
    "    # PPO Environment explicitly trained to predict the regime 5-days ahead\n",
    "\n",
    "    env_high = HighLevelEnv(X_train, regimes_train_next, train_returns)\n",
    "\n",
    "    high_level_agent = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env_high,\n",
    "        learning_rate=1e-4,\n",
    "        gamma=0.95,\n",
    "        ent_coef=0.01,\n",
    "        n_steps=512,\n",
    "        verbose=0\n",
    "    )\n",
    "    high_level_agent.learn(total_timesteps=100000)\n",
    "    \n",
    "    # Explicit predictions\n",
    "    predicted_regimes = []\n",
    "    for obs in X_predict.astype(np.float32):\n",
    "        pred_regime, _ = high_level_agent.predict(obs, deterministic=True)\n",
    "        predicted_regimes.append(int(pred_regime))\n",
    "    \n",
    "    # Ensemble explicitly with supervised baseline for robustness\n",
    "    final_predicted_regimes = [\n",
    "        ppo_pred if ppo_pred == sup_pred else sup_pred\n",
    "        for ppo_pred, sup_pred in zip(predicted_regimes, supervised_pred)\n",
    "    ]\n",
    "\n",
    "    predict_dates = predict_data['End_Date'].values\n",
    "\n",
    "    \n",
    "    high_level_results.append(pd.DataFrame({\n",
    "        'Date': predict_dates,\n",
    "        'Predicted_Regime': predicted_regimes,\n",
    "        'Actual_Regime': regimes_predict_actual\n",
    "    }))\n",
    "\n",
    "    # === Low-Level Agent explicitly uses predicted regimes ===\n",
    "    factor_cols = stock_factors.columns\n",
    "    return_cols = returns_df.columns\n",
    "\n",
    "    train_stock_features = scaler.fit_transform(train_data[factor_cols])\n",
    "    train_returns = train_data[return_cols].values\n",
    "    train_predicted_regimes = regimes_train_next  # predicted regimes from high-level\n",
    "\n",
    "    predict_stock_features = scaler.transform(predict_data[factor_cols])\n",
    "    predict_returns = predict_data[return_cols].values\n",
    "    predict_regimes = predicted_regimes  # explicitly predicted from high-level model\n",
    "\n",
    "    # Low-Level PPO Environment explicitly\n",
    "    class LowLevelEnv(gym.Env):\n",
    "        def __init__(self, features, returns, regimes, window_size=21):\n",
    "            super().__init__()\n",
    "            min_len = min(len(features), len(returns), len(regimes))\n",
    "            self.features = features[:min_len].astype(np.float32)\n",
    "            self.returns = returns[:min_len].astype(np.float32)\n",
    "            self.regimes = regimes[:min_len]\n",
    "    \n",
    "            self.window_size = window_size\n",
    "            self.n_assets = returns.shape[1]\n",
    "            self.index = self.window_size\n",
    "    \n",
    "            obs_shape = (window_size * self.features.shape[1]) + 3\n",
    "            self.observation_space = spaces.Box(-np.inf, np.inf, shape=(obs_shape,), dtype=np.float32)\n",
    "            self.action_space = spaces.Box(0, 1, shape=(self.n_assets,), dtype=np.float32)\n",
    "            self.regime_encoding = {0: [1,0,0], 1: [0,1,0], 2: [0,0,1]}\n",
    "    \n",
    "            logging.info(f\"Initialized LowLevelEnv: features length {len(self.features)}, returns length {len(self.returns)}, regimes length {len(self.regimes)}.\")\n",
    "    \n",
    "        def reset(self, seed=None, options=None):\n",
    "            super().reset(seed=seed)\n",
    "            self.index = self.window_size\n",
    "            if self.index >= len(self.features):\n",
    "                logging.error(f\"Reset failed: index {self.index} >= features length {len(self.features)}\")\n",
    "                raise IndexError(\"Reset index out of bounds.\")\n",
    "            return self._get_obs(), {}\n",
    "    \n",
    "        def _get_obs(self):\n",
    "            recent_features = self.features[self.index - self.window_size:self.index].flatten()\n",
    "            regime_idx = min(self.index, len(self.regimes)-1)  # Ensure in bounds explicitly\n",
    "            regime_onehot = self.regime_encoding[self.regimes[regime_idx]]\n",
    "            return np.concatenate([recent_features, regime_onehot]).astype(np.float32)\n",
    "    \n",
    "        def step(self, action):\n",
    "            action = np.clip(action, 0, 1)\n",
    "            action /= action.sum() + 1e-8\n",
    "            ret = np.dot(action, self.returns[self.index])\n",
    "            reward = ret\n",
    "            self.index += 1\n",
    "            terminated = self.index >= len(self.returns)-1\n",
    "            obs = self._get_obs() if not terminated else np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "            return obs, reward, terminated, False, {}\n",
    "\n",
    "\n",
    "# Add these checks explicitly before initializing LowLevelEnv\n",
    "    if len(train_predicted_regimes) <= 21 or len(train_stock_features) <= 21:\n",
    "        logging.warning(f\"Skipping low-level training at iteration {start} due to insufficient training data.\")\n",
    "        continue\n",
    "    \n",
    "    if len(predict_regimes) <= 21 or len(predict_stock_features) <= 21:\n",
    "        logging.warning(f\"Skipping low-level prediction at iteration {start} due to insufficient prediction data.\")\n",
    "        continue\n",
    "    \n",
    "    env_low = LowLevelEnv(train_stock_features, train_returns, train_predicted_regimes)\n",
    "    low_level_agent = PPO(\"MlpPolicy\", env_low, verbose=0)\n",
    "    low_level_agent.learn(total_timesteps=50000)\n",
    "    \n",
    "    env_low_pred = LowLevelEnv(predict_stock_features, predict_returns, predict_regimes)\n",
    "    obs, _ = env_low_pred.reset()\n",
    "    \n",
    "    allocations = []\n",
    "    for _ in range(len(predict_dates)):\n",
    "        action, _ = low_level_agent.predict(obs, deterministic=True)\n",
    "        action /= np.sum(action + 1e-8)\n",
    "        allocations.append(action)\n",
    "        obs, reward, terminated, truncated, _ = env_low_pred.step(action)\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "\n",
    "    alloc_df = pd.DataFrame(allocations, columns=return_cols, index=predict_dates[:len(allocations)])\n",
    "    portfolio_results.append(alloc_df)\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "# Final Results Saving explicitly\n",
    "final_allocations = pd.concat(portfolio_results)\n",
    "final_allocations.to_csv(\"integrated_portfolio_allocations.csv\")\n",
    "final_high_level = pd.concat(high_level_results)\n",
    "final_high_level.to_csv(\"integrated_high_level_predictions.csv\")\n",
    "\n",
    "print(\"âœ… Fully explicit integrated rolling-window training complete.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ed4a51022ca87a2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "high_level_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a1855b12def58c3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# === Data Load ===\n",
    "shap_file = \"shap_with_DIA_indicators.csv\"\n",
    "returns_file = \"daily_returns.csv\"\n",
    "\n",
    "shap_df = pd.read_csv(shap_file, parse_dates=[\"End_Date\"])\n",
    "returns_df = pd.read_csv(returns_file, parse_dates=[\"Date\"])\n",
    "returns_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "data = pd.merge(shap_df, returns_df, how=\"inner\", left_on=\"End_Date\", right_index=True)\n",
    "\n",
    "# === Market Regime Detection using HMM ===\n",
    "features = [\n",
    "    \"mean_abs_shap_Mkt-RF\", \"mean_abs_shap_SMB\", \"mean_abs_shap_HML\", \n",
    "    \"mean_abs_shap_RMW\", \"mean_abs_shap_CMA\", \"mean_abs_shap_Stock\",\n",
    "    \"Return_1d\", \"Volatility_5d\", \"RSI_14\", \"MACD\", \"ATR_14\"\n",
    "]\n",
    "\n",
    "X = data[features].fillna(method=\"ffill\").dropna()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "hmm = GaussianHMM(n_components=3, covariance_type=\"full\", n_iter=2000, random_state=42)\n",
    "regime_labels = hmm.fit_predict(X_scaled)\n",
    "\n",
    "data = data.iloc[-len(regime_labels):].copy()\n",
    "data[\"Regime\"] = regime_labels\n",
    "\n",
    "# === PPO Environments ===\n",
    "class HighLevelEnv(gym.Env):\n",
    "    def __init__(self, X, regimes):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.regimes = regimes\n",
    "        self.index = 0\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(X.shape[1],))\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        return self.X[self.index]\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 1 if action == self.regimes[self.index] else -1\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.X)-1\n",
    "        obs = self.X[self.index] if not done else np.zeros(self.X.shape[1])\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "class LowLevelEnv(gym.Env):\n",
    "    def __init__(self, returns):\n",
    "        super().__init__()\n",
    "        self.returns = returns\n",
    "        self.n_assets = returns.shape[1]\n",
    "        self.index = 0\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_assets,))\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,))\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        return self.returns[self.index]\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, 0, 1)\n",
    "        action /= np.sum(action + 1e-8)\n",
    "        portfolio_return = np.dot(action, self.returns[self.index])\n",
    "        cvar_threshold = np.percentile(self.returns[self.index], 5)\n",
    "        reward = portfolio_return - 0.5 * max(cvar_threshold - portfolio_return, 0)\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.returns)-1\n",
    "        obs = self.returns[self.index] if not done else np.zeros(self.n_assets)\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "# === Train High-Level PPO ===\n",
    "high_level_env = HighLevelEnv(X_scaled, regime_labels)\n",
    "check_env(high_level_env)\n",
    "\n",
    "high_level_model = PPO(\"MlpPolicy\", high_level_env, verbose=1)\n",
    "high_level_model.learn(total_timesteps=20000)  # Recommended steps for robust regime prediction\n",
    "high_level_model.save(\"high_level_ppo_agent\")\n",
    "\n",
    "# === Train Low-Level PPO (one per regime) ===\n",
    "low_level_agents = {}\n",
    "returns_only = data.iloc[:, -29:-1]  # Ensure exactly 28 stocks, excluding 'Regime' column\n",
    "returns_scaled = StandardScaler().fit_transform(returns_only)\n",
    "\n",
    "for regime in range(3):\n",
    "    regime_data = returns_scaled[data[\"Regime\"] == regime]\n",
    "    if len(regime_data) < 10:\n",
    "        print(f\"Regime {regime} has insufficient data; skipping.\")\n",
    "        continue\n",
    "\n",
    "    env = LowLevelEnv(regime_data)\n",
    "    check_env(env)\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "    model.learn(total_timesteps=20000)  # Recommended for financial stability\n",
    "    model.save(f\"low_level_ppo_agent_regime_{regime}\")\n",
    "    low_level_agents[regime] = model\n",
    "    print(f\"âœ… Saved PPO for regime {regime}\")\n",
    "\n",
    "# === Evaluation Logging ===\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(len(X_scaled)):\n",
    "    obs = X_scaled[i]\n",
    "    predicted_regime, _ = high_level_model.predict(obs)\n",
    "    if predicted_regime in low_level_agents:\n",
    "        agent = low_level_agents[predicted_regime]\n",
    "        stock_obs = returns_scaled[i].reshape(1, -1)\n",
    "        allocation, _ = agent.predict(stock_obs)\n",
    "        allocation = np.clip(allocation, 0, 1)\n",
    "        allocation /= np.sum(allocation + 1e-8)\n",
    "        daily_return = np.dot(allocation, returns_scaled[i])\n",
    "        evaluation_results.append({\n",
    "            \"Date\": data[\"End_Date\"].iloc[i],\n",
    "            \"Predicted_Regime\": predicted_regime,\n",
    "            \"True_Regime\": regime_labels[i],\n",
    "            \"Daily_Return\": daily_return\n",
    "        })\n",
    "\n",
    "evaluation_df = pd.DataFrame(evaluation_results)\n",
    "evaluation_df.to_csv(\"hierarchical_rl_evaluation.csv\", index=False)\n",
    "\n",
    "print(\"âœ… HRL Pipeline complete. Models and evaluation saved.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "361ad17abb9e0be1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class HighLevelEnv(gym.Env):\n",
    "    def __init__(self, X, regimes, returns_scaled, low_level_agents, alpha=0.5, lambda_risk=0.5):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.regimes = regimes\n",
    "        self.returns_scaled = returns_scaled\n",
    "        self.low_level_agents = low_level_agents\n",
    "        self.alpha = alpha\n",
    "        self.lambda_risk = lambda_risk\n",
    "        self.index = 0\n",
    "        self.n_features = X.shape[1]\n",
    "\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_features,))\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        return self.X[self.index]\n",
    "\n",
    "    def step(self, action):\n",
    "        true_regime = self.regimes[self.index]\n",
    "\n",
    "        # Classification accuracy reward component\n",
    "        classification_reward = 1 if action == true_regime else -1\n",
    "\n",
    "        # Financial reward component from low-level agent\n",
    "        if action in self.low_level_agents:\n",
    "            low_level_agent = self.low_level_agents[action]\n",
    "            stock_obs = self.returns_scaled[self.index].reshape(1, -1)\n",
    "            allocation, _ = low_level_agent.predict(stock_obs, deterministic=True)\n",
    "            allocation = np.clip(allocation, 0, 1)\n",
    "            allocation /= np.sum(allocation + 1e-8)\n",
    "\n",
    "            daily_return = np.dot(allocation, self.returns_scaled[self.index])\n",
    "            cvar_threshold = np.percentile(self.returns_scaled[self.index], 5)\n",
    "            cvar_penalty = max(cvar_threshold - daily_return, 0)\n",
    "\n",
    "            financial_reward = daily_return - self.lambda_risk * cvar_penalty\n",
    "        else:\n",
    "            financial_reward = -1  # Penalize heavily if no matching low-level agent\n",
    "\n",
    "        # Combine rewards\n",
    "        reward = self.alpha * classification_reward + (1 - self.alpha) * financial_reward\n",
    "\n",
    "        # Proceed to next step\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.X) - 1\n",
    "        obs = self.X[self.index] if not done else np.zeros(self.n_features)\n",
    "\n",
    "        return obs, reward, done, {}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c455371a684901f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# === Train Low-Level PPO Agents ===\n",
    "low_level_agents = {}\n",
    "returns_only = data.iloc[:, -29:-1]  # exactly 28 stocks\n",
    "returns_scaled = StandardScaler().fit_transform(returns_only)\n",
    "\n",
    "for regime in range(3):\n",
    "    regime_data = returns_scaled[data[\"Regime\"] == regime]\n",
    "    if len(regime_data) < 10:\n",
    "        print(f\"Regime {regime} insufficient data; skipping.\")\n",
    "        continue\n",
    "\n",
    "    env = LowLevelEnv(regime_data)\n",
    "    check_env(env)\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "    model.learn(total_timesteps=20000)\n",
    "    model.save(f\"low_level_ppo_agent_regime_{regime}\")\n",
    "    low_level_agents[regime] = model\n",
    "    print(f\"âœ… Saved low-level PPO agent for regime {regime}\")\n",
    "\n",
    "# === Train Enhanced High-Level PPO ===\n",
    "alpha = 0.5  # Balanced accuracy vs financial return\n",
    "lambda_risk = 0.5  # Risk aversion hyperparameter\n",
    "\n",
    "high_level_env = HighLevelEnv(X_scaled, regime_labels, returns_scaled, low_level_agents, alpha, lambda_risk)\n",
    "check_env(high_level_env)\n",
    "\n",
    "high_level_model = PPO(\"MlpPolicy\", high_level_env, verbose=1)\n",
    "high_level_model.learn(total_timesteps=20000)\n",
    "high_level_model.save(\"enhanced_high_level_ppo_agent\")\n",
    "print(\"âœ… Enhanced high-level PPO agent trained and saved.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "481a302b167f6a2c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# === Enhanced Evaluation Logging ===\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(len(X_scaled)):\n",
    "    obs = X_scaled[i]\n",
    "    predicted_regime, _ = high_level_model.predict(obs, deterministic=True)\n",
    "    true_regime = regime_labels[i]\n",
    "\n",
    "    # Get financial reward from selected low-level agent\n",
    "    if predicted_regime in low_level_agents:\n",
    "        low_level_agent = low_level_agents[predicted_regime]\n",
    "        stock_obs = returns_scaled[i].reshape(1, -1)\n",
    "        allocation, _ = low_level_agent.predict(stock_obs, deterministic=True)\n",
    "        allocation = np.clip(allocation, 0, 1)\n",
    "        allocation /= np.sum(allocation + 1e-8)\n",
    "        daily_return = np.dot(allocation, returns_scaled[i])\n",
    "\n",
    "        # Risk-adjusted measure\n",
    "        cvar_threshold = np.percentile(returns_scaled[i], 5)\n",
    "        cvar_penalty = max(cvar_threshold - daily_return, 0)\n",
    "        financial_reward = daily_return - lambda_risk * cvar_penalty\n",
    "    else:\n",
    "        daily_return = financial_reward = None\n",
    "\n",
    "    evaluation_results.append({\n",
    "        \"Date\": data[\"End_Date\"].iloc[i],\n",
    "        \"Predicted_Regime\": predicted_regime,\n",
    "        \"True_Regime\": true_regime,\n",
    "        \"Classification_Reward\": 1 if predicted_regime == true_regime else -1,\n",
    "        \"Daily_Return\": daily_return,\n",
    "        \"Financial_Reward\": financial_reward,\n",
    "        \"Combined_High_Level_Reward\": alpha * (1 if predicted_regime == true_regime else -1) + (1 - alpha) * (financial_reward if financial_reward is not None else -1)\n",
    "    })\n",
    "\n",
    "evaluation_df = pd.DataFrame(evaluation_results)\n",
    "evaluation_df.to_csv(\"enhanced_hierarchical_rl_evaluation.csv\", index=False)\n",
    "print(\"âœ… Enhanced evaluation logged and saved.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c83357f2767cd46d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# full code implement\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# === Data Load ===\n",
    "shap_file = \"shap_with_DIA_indicators.csv\"\n",
    "returns_file = \"daily_returns.csv\"\n",
    "\n",
    "shap_df = pd.read_csv(shap_file, parse_dates=[\"End_Date\"])\n",
    "returns_df = pd.read_csv(returns_file, parse_dates=[\"Date\"])\n",
    "returns_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "data = pd.merge(shap_df, returns_df, how=\"inner\", left_on=\"End_Date\", right_index=True)\n",
    "\n",
    "# === Market Regime Detection using HMM ===\n",
    "features = [\n",
    "    \"mean_abs_shap_Mkt-RF\", \"mean_abs_shap_SMB\", \"mean_abs_shap_HML\",\n",
    "    \"mean_abs_shap_RMW\", \"mean_abs_shap_CMA\", \"mean_abs_shap_Stock\",\n",
    "    \"Return_1d\", \"Volatility_5d\", \"RSI_14\", \"MACD\", \"ATR_14\"\n",
    "]\n",
    "\n",
    "X = data[features].fillna(method=\"ffill\").dropna()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "hmm = GaussianHMM(n_components=3, covariance_type=\"full\", n_iter=2000, random_state=42)\n",
    "regime_labels = hmm.fit_predict(X_scaled)\n",
    "\n",
    "data = data.iloc[-len(regime_labels):].copy()\n",
    "data[\"Regime\"] = regime_labels\n",
    "\n",
    "# === PPO Environments ===\n",
    "class HighLevelEnv(gym.Env):\n",
    "    def __init__(self, X, regimes):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.regimes = regimes\n",
    "        self.index = 0\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(X.shape[1],))\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        return self.X[self.index]\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 1 if action == self.regimes[self.index] else -1\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.X)-1\n",
    "        obs = self.X[self.index] if not done else np.zeros(self.X.shape[1])\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "class LowLevelBenchmarkEnv(gym.Env):\n",
    "    def __init__(self, returns, window_size=21, lambda_risk=0.5, alpha=0.05):\n",
    "        super().__init__()\n",
    "        self.returns = returns\n",
    "        self.window_size = window_size\n",
    "        self.lambda_risk = lambda_risk\n",
    "        self.alpha = alpha\n",
    "        self.n_assets = returns.shape[1]\n",
    "        self.index = window_size\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(window_size, self.n_assets))\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,))\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = self.window_size\n",
    "        return self.returns[self.index-self.window_size:self.index]\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, 0, 1)\n",
    "        action /= np.sum(action + 1e-8)\n",
    "\n",
    "        # Agent and naive returns\n",
    "        agent_return = np.dot(action, self.returns[self.index])\n",
    "        naive_weights = np.array([1/self.n_assets] * self.n_assets)\n",
    "        naive_return = np.dot(naive_weights, self.returns[self.index])\n",
    "\n",
    "        # Excess return explicitly benchmarking naive portfolio\n",
    "        excess_return = agent_return - naive_return\n",
    "\n",
    "        historical_returns = self.returns[self.index-self.window_size:self.index] @ action\n",
    "        cvar_threshold = np.percentile(historical_returns, 100*self.alpha)\n",
    "        cvar_penalty = max(cvar_threshold - agent_return, 0)\n",
    "\n",
    "        # Reward explicitly includes naive portfolio benchmark\n",
    "        reward = excess_return - self.lambda_risk * cvar_penalty\n",
    "\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.returns)-1\n",
    "        obs = self.returns[self.index-self.window_size:self.index] if not done else np.zeros((self.window_size, self.n_assets))\n",
    "\n",
    "        return obs, reward, done, {\n",
    "            \"agent_return\": agent_return,\n",
    "            \"naive_return\": naive_return,\n",
    "            \"excess_return\": excess_return,\n",
    "            \"cvar_penalty\": cvar_penalty\n",
    "        }\n",
    "\n",
    "# === Train High-Level PPO ===\n",
    "high_level_env = HighLevelEnv(X_scaled, regime_labels)\n",
    "check_env(high_level_env)\n",
    "high_level_model = PPO(\"MlpPolicy\", high_level_env, verbose=0)\n",
    "high_level_model.learn(total_timesteps=20000)\n",
    "high_level_model.save(\"high_level_ppo_agent\")\n",
    "\n",
    "# === Train Low-Level PPO (one per regime) ===\n",
    "low_level_agents = {}\n",
    "returns_only = data.iloc[:, -29:-1]  # exactly 28 stocks\n",
    "returns_scaled = StandardScaler().fit_transform(returns_only)\n",
    "\n",
    "for regime in range(3):\n",
    "    regime_data = returns_scaled[data[\"Regime\"] == regime]\n",
    "    if len(regime_data) < 50:\n",
    "        print(f\"Regime {regime} has insufficient data; skipping.\")\n",
    "        continue\n",
    "\n",
    "    env = LowLevelBenchmarkEnv(regime_data)\n",
    "    check_env(env)\n",
    "    eval_callback = EvalCallback(env, best_model_save_path=f\"./best_low_level_agent_{regime}\",\n",
    "                                 eval_freq=5000, deterministic=True, render=False)\n",
    "\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "    model.learn(total_timesteps=50000, callback=eval_callback)\n",
    "    model.save(f\"low_level_ppo_agent_regime_{regime}\")\n",
    "    low_level_agents[regime] = model\n",
    "    print(f\"âœ… Saved PPO for regime {regime}\")\n",
    "\n",
    "# === Evaluation Logging (Explicit Benchmark) ===\n",
    "evaluation_results = []\n",
    "\n",
    "for i in range(len(X_scaled)):\n",
    "    obs = X_scaled[i]\n",
    "    predicted_regime, _ = high_level_model.predict(obs, deterministic=True)\n",
    "    true_regime = regime_labels[i]\n",
    "\n",
    "    if predicted_regime in low_level_agents:\n",
    "        low_level_agent = low_level_agents[predicted_regime]\n",
    "        stock_obs = returns_scaled[i:i+21]\n",
    "        if len(stock_obs) < 21:\n",
    "            continue\n",
    "\n",
    "        action, _ = low_level_agent.predict(stock_obs, deterministic=True)\n",
    "        action /= np.sum(action + 1e-8)\n",
    "        agent_return = np.dot(action, returns_scaled[i])\n",
    "\n",
    "        naive_weights = np.array([1/28]*28)\n",
    "        naive_return = np.dot(naive_weights, returns_scaled[i])\n",
    "\n",
    "        evaluation_results.append({\n",
    "            \"Date\": data[\"End_Date\"].iloc[i],\n",
    "            \"Predicted_Regime\": predicted_regime,\n",
    "            \"True_Regime\": true_regime,\n",
    "            \"Agent_Return\": agent_return,\n",
    "            \"Naive_Return\": naive_return,\n",
    "            \"Excess_Return\": agent_return - naive_return\n",
    "        })\n",
    "\n",
    "evaluation_df = pd.DataFrame(evaluation_results)\n",
    "evaluation_df.to_csv(\"hierarchical_rl_evaluation_with_benchmark.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Training and evaluation complete. Results saved explicitly.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9a8fcd474c9fa59"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import gym\n",
    "from gym import spaces\n",
    "import os\n",
    "\n",
    "# === Step 1: Data Preprocessing ===\n",
    "shap_df = pd.read_csv(\"shap_with_DIA_indicators.csv\", parse_dates=[\"End_Date\"])\n",
    "returns_df = pd.read_csv(\"daily_returns.csv\", parse_dates=[\"Date\"])\n",
    "returns_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "data = pd.merge(shap_df, returns_df, how=\"inner\", left_on=\"End_Date\", right_index=True)\n",
    "\n",
    "# === Step 2: Market Regime Identification (HMM) ===\n",
    "features = [\n",
    "    \"mean_abs_shap_Mkt-RF\", \"mean_abs_shap_SMB\", \"mean_abs_shap_HML\",\n",
    "    \"mean_abs_shap_RMW\", \"mean_abs_shap_CMA\", \"mean_abs_shap_Stock\",\n",
    "    \"Return_1d\", \"Volatility_5d\", \"RSI_14\", \"MACD\", \"ATR_14\"\n",
    "]\n",
    "\n",
    "X = data[features].fillna(method=\"ffill\").dropna()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "hmm = GaussianHMM(n_components=3, covariance_type=\"full\", n_iter=2000, random_state=42)\n",
    "regime_labels = hmm.fit_predict(X_scaled)\n",
    "\n",
    "data = data.iloc[-len(regime_labels):].copy()\n",
    "data[\"Regime\"] = regime_labels\n",
    "\n",
    "# === Step 3: High-Level PPO Agent ===\n",
    "class HighLevelEnv(gym.Env):\n",
    "    def __init__(self, X, regimes):\n",
    "        self.X = X\n",
    "        self.regimes = regimes\n",
    "        self.index = 0\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(X.shape[1],))\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        return self.X[self.index]\n",
    "\n",
    "    def step(self, action):\n",
    "        classification_reward = 1 if action == self.regimes[self.index] else -1\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.X) - 1\n",
    "        obs = self.X[self.index] if not done else np.zeros(self.X.shape[1])\n",
    "        return obs, classification_reward, done, {}\n",
    "\n",
    "high_env = HighLevelEnv(X_scaled, regime_labels)\n",
    "check_env(high_env)\n",
    "high_model = PPO(\"MlpPolicy\", high_env, verbose=1)\n",
    "high_model.learn(total_timesteps=20000)\n",
    "high_model.save(\"high_level_agent\")\n",
    "\n",
    "# === Step 4: Low-Level PPO Agents with Benchmark ===\n",
    "class LowLevelEnv(gym.Env):\n",
    "    def __init__(self, returns, window_size=21, lambda_risk=0.5, alpha=0.05):\n",
    "        self.returns = returns\n",
    "        self.window_size = window_size\n",
    "        self.lambda_risk = lambda_risk\n",
    "        self.alpha = alpha\n",
    "        self.n_assets = returns.shape[1]\n",
    "        self.index = window_size\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(window_size, self.n_assets))\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,))\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = self.window_size\n",
    "        return self.returns[self.index - self.window_size:self.index]\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, 0, 1)\n",
    "        action /= np.sum(action + 1e-8)\n",
    "        agent_return = np.dot(action, self.returns[self.index])\n",
    "        naive_return = np.mean(self.returns[self.index])\n",
    "\n",
    "        excess_return = agent_return - naive_return\n",
    "        historical_returns = self.returns[self.index-self.window_size:self.index] @ action\n",
    "        cvar_threshold = np.percentile(historical_returns, 100*self.alpha)\n",
    "        cvar_penalty = max(cvar_threshold - agent_return, 0)\n",
    "\n",
    "        reward = excess_return - self.lambda_risk * cvar_penalty\n",
    "\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.returns)-1\n",
    "        obs = self.returns[self.index-self.window_size:self.index] if not done else np.zeros((self.window_size, self.n_assets))\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "low_agents = {}\n",
    "returns_scaled = StandardScaler().fit_transform(data.iloc[:, -29:-1])\n",
    "\n",
    "for regime in range(3):\n",
    "    regime_data = returns_scaled[data[\"Regime\"] == regime]\n",
    "    if len(regime_data) < 50:\n",
    "        continue\n",
    "\n",
    "    env = LowLevelEnv(regime_data)\n",
    "    check_env(env)\n",
    "    eval_callback = EvalCallback(env, best_model_save_path=f\"./best_low_agent_{regime}\",\n",
    "                                 eval_freq=5000, deterministic=True)\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "    model.learn(total_timesteps=50000, callback=eval_callback)\n",
    "    model.save(f\"low_agent_regime_{regime}\")\n",
    "    low_agents[regime] = model\n",
    "\n",
    "# === Step 5: End-to-End Evaluation ===\n",
    "evaluation = []\n",
    "for i in range(len(X_scaled)):\n",
    "    regime_pred, _ = high_model.predict(X_scaled[i], deterministic=True)\n",
    "    if regime_pred not in low_agents:\n",
    "        continue\n",
    "    low_agent = low_agents[regime_pred]\n",
    "    stock_obs = returns_scaled[i:i+21]\n",
    "    if len(stock_obs) < 21:\n",
    "        continue\n",
    "    weights, _ = low_agent.predict(stock_obs, deterministic=True)\n",
    "    weights /= np.sum(weights + 1e-8)\n",
    "    agent_ret = np.dot(weights, returns_scaled[i])\n",
    "    naive_ret = np.mean(returns_scaled[i])\n",
    "    evaluation.append({\n",
    "        \"Date\": data[\"End_Date\"].iloc[i],\n",
    "        \"Pred_Regime\": regime_pred,\n",
    "        \"True_Regime\": regime_labels[i],\n",
    "        \"Agent_Return\": agent_ret,\n",
    "        \"Naive_Return\": naive_ret,\n",
    "        \"Excess_Return\": agent_ret - naive_ret\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation)\n",
    "eval_df.to_csv(\"final_hierarchical_rl_evaluation.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Explicit hierarchical RL pipeline complete. Results saved clearly.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e760383b13fe2e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
