{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "895e7096145459d4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "adb5bfdfc9c419d8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8f0500c59fdc3167",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ff812fee4b59a927",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# stage 1 training and prediction \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import ta\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "# Load data\n",
    "factors = pd.read_csv(\"aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns = pd.read_csv(\"daily_returns_10ETFs.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Align dates\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors = factors.loc[dates]\n",
    "returns = returns.loc[dates]\n",
    "\n",
    "# Compute expanded generic technical indicators\n",
    "all_tech_features = []\n",
    "\n",
    "for etf in returns.columns:\n",
    "    close = (1 + returns[etf]).cumprod()\n",
    "    etf_tech_features = pd.DataFrame(index=returns.index)\n",
    "\n",
    "    etf_tech_features[f'{etf}_SMA_5'] = ta.trend.sma_indicator(close, window=5)\n",
    "    # etf_tech_features[f'{etf}_SMA_20'] = ta.trend.sma_indicator(close, window=20)\n",
    "    # etf_tech_features[f'{etf}_SMA_50'] = ta.trend.sma_indicator(close, window=50)\n",
    "    etf_tech_features[f'{etf}_EMA_12'] = ta.trend.ema_indicator(close, window=12)\n",
    "    # etf_tech_features[f'{etf}_EMA_26'] = ta.trend.ema_indicator(close, window=26)\n",
    "    # etf_tech_features[f'{etf}_EMA_50'] = ta.trend.ema_indicator(close, window=50)\n",
    "    etf_tech_features[f'{etf}_RSI_7'] = ta.momentum.rsi(close, window=7)\n",
    "    # etf_tech_features[f'{etf}_RSI_14'] = ta.momentum.rsi(close, window=14)\n",
    "    etf_tech_features[f'{etf}_MACD'] = ta.trend.macd_diff(close)\n",
    "    etf_tech_features[f'{etf}_ATR'] = ta.volatility.average_true_range(high=close*1.01, low=close*0.99, close=close, window=10)\n",
    "    etf_tech_features[f'{etf}_Volatility_5'] = returns[etf].rolling(window=5).std()\n",
    "    # etf_tech_features[f'{etf}_Volatility_20'] = returns[etf].rolling(window=20).std()\n",
    "    # etf_tech_features[f'{etf}_Volatility_50'] = returns[etf].rolling(window=50).std()\n",
    "    etf_tech_features[f'{etf}_Momentum_3'] = returns[etf].rolling(window=3).mean()\n",
    "    # etf_tech_features[f'{etf}_Momentum_10'] = returns[etf].rolling(window=10).mean()\n",
    "    \n",
    "    # Add lagged returns explicitly\n",
    "    for lag in [1, 2, 3]:\n",
    "        etf_tech_features[f'{etf}_LagReturn_{lag}'] = returns[etf].shift(lag)\n",
    "\n",
    "    all_tech_features.append(etf_tech_features)\n",
    "    \n",
    "# Concatenate all ETF technical features at once to prevent DataFrame fragmentation\n",
    "technical_features = pd.concat(all_tech_features, axis=1)\n",
    "\n",
    "for factor in ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']:\n",
    "    for lag in [1, 2, 3]:\n",
    "        factors[f'{factor}_lag_{lag}'] = factors[factor].shift(lag)\n",
    "\n",
    "factors = factors.dropna()\n",
    "\n",
    "# Combine original factors with technical indicators\n",
    "features = pd.concat([factors, technical_features], axis=1).dropna()\n",
    "vix = pd.read_csv(\"VIX_History.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Align dates explicitly with forward-fill to ensure no intermediate NA\n",
    "vix_aligned = vix['CLOSE'].reindex(features.index).ffill()\n",
    "\n",
    "# Compute VIX daily pct_change explicitly without implicit filling\n",
    "features['VIX'] = vix_aligned.pct_change(fill_method=None).shift(1)\n",
    "\n",
    "# Explicitly fill any remaining leading NA values after pct_change\n",
    "features['VIX'] = features['VIX'].fillna(0)\n",
    "# Shift target by 1 day for next-day prediction\n",
    "target_returns = returns.shift(-1).loc[features.index].dropna()\n",
    "features = features.loc[target_returns.index]\n",
    "\n",
    "# Define rolling window parameters\n",
    "train_years = 12          # Length of training data in years\n",
    "valid_years = 1           # Length of validation data in years\n",
    "test_years = 1            # Length of testing/prediction data in years (configurable)\n",
    "retrain_frequency = 1     # Retrain model every N years (configurable)\n",
    "start_year = 2009\n",
    "end_year = 2024\n",
    "\n",
    "# Generic feature names\n",
    "# all_generic_features = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA',\n",
    "#                         'SMA_5', 'SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', 'EMA_50',\n",
    "#                         'RSI_7', 'RSI_14', 'MACD', 'ATR', 'Volatility_5', 'Volatility_20', 'Volatility_50',\n",
    "#                         'Momentum_3', 'Momentum_10']\n",
    "\n",
    "all_generic_features = [\n",
    "    'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA',\n",
    "    'Mkt-RF_lag_1', 'Mkt-RF_lag_2', 'Mkt-RF_lag_3',\n",
    "    'SMB_lag_1', 'SMB_lag_2', 'SMB_lag_3',\n",
    "    'HML_lag_1', 'HML_lag_2', 'HML_lag_3',\n",
    "    'RMW_lag_1', 'RMW_lag_2', 'RMW_lag_3',\n",
    "    'CMA_lag_1', 'CMA_lag_2', 'CMA_lag_3',\n",
    "    'SMA_5', 'EMA_12', 'RSI_7', 'MACD',\n",
    "    'Volatility_5', 'Momentum_3',\n",
    "    'LagReturn_1', 'LagReturn_2', 'LagReturn_3','VIX'\n",
    "]\n",
    "\n",
    "# Step 1: Determine top N generic important features using aggregated SHAP across all ETFs\n",
    "shap_importances = pd.DataFrame(0.0, index=all_generic_features, columns=['SHAP_Value'])\n",
    "\n",
    "for etf in returns.columns:\n",
    "    print(f\"Computing SHAP for ETF: {etf}\")\n",
    "    train_start = pd.Timestamp(2009 - train_years, 1, 1)\n",
    "    train_end = pd.Timestamp(2009 - valid_years - 1, 12, 31)\n",
    "    \n",
    "    # etf_features = [col for col in features.columns if etf in col or col in factors.columns]\n",
    "    etf_features = [\n",
    "        col for col in features.columns \n",
    "        if (etf in col and any(gen_feat in col for gen_feat in ['SMA_5', 'EMA_12', 'RSI_7', 'MACD',\n",
    "                                                                'Volatility_5', 'Momentum_3',\n",
    "                                                                'LagReturn_1', 'LagReturn_2', 'LagReturn_3', 'VIX']))\n",
    "        or col in ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA',\n",
    "                   'Mkt-RF_lag_1', 'Mkt-RF_lag_2', 'Mkt-RF_lag_3',\n",
    "                   'SMB_lag_1', 'SMB_lag_2', 'SMB_lag_3',\n",
    "                   'HML_lag_1', 'HML_lag_2', 'HML_lag_3',\n",
    "                   'RMW_lag_1', 'RMW_lag_2', 'RMW_lag_3',\n",
    "                   'CMA_lag_1', 'CMA_lag_2', 'CMA_lag_3']\n",
    "    ]\n",
    "    X_train = features.loc[train_start:train_end, etf_features]\n",
    "    y_train = target_returns[etf].loc[train_start:train_end]\n",
    "\n",
    "    model = xgb.XGBRegressor(tree_method='hist', device='cuda').fit(X_train, y_train)\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_train)\n",
    "\n",
    "    for generic in all_generic_features:\n",
    "        cols = [col for col in X_train.columns if generic in col]\n",
    "        if cols:\n",
    "            idx = [X_train.columns.get_loc(c) for c in cols]\n",
    "            shap_importances.loc[generic] += np.mean(np.abs(shap_values.values[:, idx]))\n",
    "\n",
    "shap_importances /= len(returns.columns)\n",
    "common_generic_features = shap_importances.sort_values('SHAP_Value', ascending=False).head(10).index.tolist()\n",
    "\n",
    "# Step 2: Retrain models using selected top generic important features\n",
    "all_predictions = []\n",
    "for etf in returns.columns:  # Adjust this slice for all ETFs\n",
    "    \n",
    "    print(f\"Processing ETF: {etf}\")\n",
    "    selected_features = [f for f in features.columns if any(generic in f for generic in common_generic_features) or f in factors.columns]\n",
    "\n",
    "    # for year in range(2009, 2010):  # Adjust range for all years\n",
    "    year = start_year\n",
    "    while year <= end_year - test_years + 1:\n",
    "        print(f\"\\nTraining window starting year: {year}\")\n",
    "        start_time = time.time()\n",
    "        train_start = pd.Timestamp(year - train_years, 1, 1)\n",
    "        train_end = pd.Timestamp(year - valid_years - 1, 12, 31)\n",
    "        valid_start = pd.Timestamp(year - valid_years, 1, 1)\n",
    "        valid_end = pd.Timestamp(year - 1, 12, 31)\n",
    "        test_start = pd.Timestamp(year, 1, 1)\n",
    "        test_end = pd.Timestamp(year + test_years - 1, 12, 31)\n",
    "\n",
    "        X_train = features.loc[train_start:train_end, selected_features]\n",
    "        y_train = target_returns[etf].loc[train_start:train_end]\n",
    "\n",
    "        X_valid = features.loc[valid_start:valid_end, selected_features]\n",
    "        y_valid = target_returns[etf].loc[valid_start:valid_end]\n",
    "\n",
    "        X_test = features.loc[test_start:test_end, selected_features]\n",
    "        y_test = target_returns[etf].loc[test_start:test_end]\n",
    "\n",
    "        model = xgb.XGBRegressor(\n",
    "            tree_method='hist',\n",
    "            device='cuda',\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42,\n",
    "            n_jobs=4\n",
    "        )\n",
    "\n",
    "        params = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'learning_rate': [0.01, 0.03, 0.05],\n",
    "            'subsample': [0.7, 0.8],\n",
    "            'colsample_bytree': [0.7, 0.8]\n",
    "        }\n",
    "\n",
    "        grid_search = GridSearchCV(model, params, cv=TimeSeriesSplit(3), scoring='neg_mean_squared_error', verbose=1, n_jobs=4)\n",
    "        grid_search.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "       \n",
    "        # Convert test data explicitly to DMatrix for GPU\n",
    "        dtest = xgb.DMatrix(X_test, enable_categorical=False)\n",
    "        \n",
    "        # Make predictions using the best_model explicitly\n",
    "        preds = best_model.get_booster().predict(dtest)\n",
    "\n",
    "\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "        directional_accuracy = np.mean((np.sign(y_test) == np.sign(preds)).astype(int))\n",
    "\n",
    "        print(f\"MSE: {mse:.6f}, RMSE: {rmse:.6f}, MAE: {mae:.6f}, R2: {r2:.6f}, Directional Accuracy: {directional_accuracy:.2%}\")\n",
    "        joblib.dump(best_model, f\"best_model_{etf}_{year}.joblib\")\n",
    "\n",
    "        # Save daily predictions\n",
    "        predictions_df = pd.DataFrame({'Date': X_test.index, 'ETF': etf, 'Year': year, \n",
    "                                       'Actual_Return': y_test, 'Predicted_Return': preds})\n",
    "\n",
    "        # SHAP values explicitly aligned\n",
    "        explainer_test = shap.Explainer(best_model)\n",
    "        shap_values_test = explainer_test(X_test)\n",
    "        \n",
    "        # Create SHAP DataFrame explicitly with 'Date' to ensure correct merging\n",
    "        shap_df = pd.DataFrame(\n",
    "            shap_values_test.values,\n",
    "            columns=[f'SHAP_{col}' for col in X_test.columns],\n",
    "            index=X_test.index\n",
    "        ).reset_index().rename(columns={'index': 'Date'})\n",
    "        \n",
    "        # Ensure predictions_df has 'Date' column explicitly for merging\n",
    "        predictions_df = predictions_df.reset_index(drop=True)\n",
    "        \n",
    "        # Merge explicitly by 'Date' to align SHAP values correctly\n",
    "        predictions_df = pd.merge(predictions_df, shap_df, on='Date', how='left')\n",
    "        \n",
    "        # Append explicitly for each ETF-year combination\n",
    "        all_predictions.append(predictions_df)\n",
    "        year += retrain_frequency\n",
    "        end_time = time.time()\n",
    "        print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "final_predictions_df = pd.concat(all_predictions).reset_index(drop=True)\n",
    "final_predictions_df.to_csv(\"stage1_predictions_with_shap_10ETFs.csv\", index=False)\n",
    "\n",
    "print(\"Stage 1 completed and data saved for Stage 2.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "110622934baf18b4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final_predictions_df = pd.concat(all_predictions).reset_index(drop=True)\n",
    "final_predictions_df.to_csv(\"stage1_predictions_with_shap_10ETFs.csv\", index=False)\n",
    "\n",
    "print(\"Stage 1 completed and data saved for Stage 2.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6efb3e1a4faf0b68",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load stage 1 data explicitly\n",
    "stage1_df = pd.read_csv(\"stage1_predictions_with_shap_10ETFs.csv\", parse_dates=['Date'])\n",
    "# stage1_df = pd.read_csv(\"stage1_predictions_with_shap.csv\", parse_dates=['Date'])\n",
    "etfs = stage1_df['ETF'].unique()\n",
    "\n",
    "# Initialize DataFrame for daily aggregated data explicitly\n",
    "dates = sorted(stage1_df['Date'].unique())\n",
    "aggregated_data = pd.DataFrame({'Date': dates})\n",
    "\n",
    "# ETF-specific predicted returns, actual returns, and volatility\n",
    "for etf in etfs:\n",
    "    etf_data = stage1_df[stage1_df['ETF'] == etf].set_index('Date').sort_index()\n",
    "\n",
    "    aggregated_data[f'Predicted_Return_{etf}'] = aggregated_data['Date'].map(etf_data['Predicted_Return'])\n",
    "    aggregated_data[f'Actual_Return_{etf}'] = aggregated_data['Date'].map(etf_data['Actual_Return'])\n",
    "\n",
    "    # Explicit rolling 5-day volatility calculation\n",
    "    volatility = etf_data['Actual_Return'].rolling(window=5).std()\n",
    "    aggregated_data[f'Volatility_{etf}'] = aggregated_data['Date'].map(volatility)\n",
    "\n",
    "# Define explicitly generic SHAP features to aggregate across ETFs\n",
    "# generic_shap_features = [\n",
    "#     'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', \n",
    "#     'SMA_5', 'SMA_20', 'SMA_50',\n",
    "#     'EMA_12', 'EMA_26', 'EMA_50',\n",
    "#     'RSI_7', 'RSI_14', 'MACD', 'ATR',\n",
    "#     'Volatility_5', 'Volatility_20', 'Volatility_50',\n",
    "#     'Momentum_3', 'Momentum_10'\n",
    "# ]\n",
    "# generic_shap_features = ['Mkt-RF',\n",
    "#  'Volatility_50',\n",
    "#  'HML',\n",
    "#  'Momentum_3',\n",
    "#  'Volatility_5',\n",
    "#  'Volatility_20',\n",
    "#  'RMW',\n",
    "#  'SMB',\n",
    "#  'CMA',\n",
    "#  'RSI_7']\n",
    "generic_shap_features = ['Volatility_5',\n",
    " 'SMB_lag_2',\n",
    " 'Momentum_3',\n",
    " 'HML_lag_2',\n",
    " 'SMA_5',\n",
    " 'LagReturn_1',\n",
    " 'EMA_12',\n",
    " 'SMB',\n",
    " 'RSI_7',\n",
    " 'CMA_lag_1']\n",
    "\n",
    "# Aggregate SHAP values explicitly across ETFs by generic feature\n",
    "shap_aggregated = {'Date': dates}\n",
    "shap_df_list = []\n",
    "\n",
    "for feature in generic_shap_features:\n",
    "    # Adjust matching explicitly for 'SHAP_{ETF}_{feature}' format\n",
    "    matching_shap_cols = [col for col in stage1_df.columns if col.startswith('SHAP_') and col.endswith(f'_{feature}')]\n",
    "    \n",
    "    if matching_shap_cols:\n",
    "        # Compute daily mean explicitly across selected SHAP columns\n",
    "        daily_shap_mean = stage1_df.groupby('Date')[matching_shap_cols].mean().mean(axis=1)\n",
    "        shap_df_list.append(daily_shap_mean.rename(f'Avg_SHAP_{feature}'))\n",
    "    else:\n",
    "        print(f\"Warning: No matches found explicitly for feature: {feature}\")\n",
    "\n",
    "# Concatenate aggregated SHAP features explicitly, ensuring alignment\n",
    "shap_aggregated_df = pd.concat(shap_df_list, axis=1).reset_index()\n",
    "\n",
    "# Explicit merge with ETF-specific metrics on Date to ensure alignment\n",
    "aggregated_data = pd.merge(aggregated_data, shap_aggregated_df, on='Date', how='left')\n",
    "\n",
    "# Explicit handling of missing values\n",
    "aggregated_data.sort_values('Date', inplace=True)\n",
    "aggregated_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Remove rows explicitly if initial volatility calculations have NaNs\n",
    "vol_cols = [f'Volatility_{etf}' for etf in etfs]\n",
    "aggregated_data.dropna(subset=vol_cols, inplace=True)\n",
    "\n",
    "# Check if aggregated_data is empty before saving explicitly\n",
    "if aggregated_data.empty:\n",
    "    print(\"Warning: aggregated_data is empty after processing. Please verify input data and alignment explicitly.\")\n",
    "else:\n",
    "    aggregated_data.to_csv(\"stage2_rl_observations_optimized_10ETFs.csv\", index=False)\n",
    "    print(f\"Optimized Stage 2 RL dataset created with shape: {aggregated_data.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "626373434f64d0c4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d8f3f0813e659ead",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# start of stage 2 training\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "SEED = 42\n",
    "def set_global_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    set_random_seed(seed)\n",
    "\n",
    "set_global_seed(SEED)\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar', risk_coefficient=0.5, rebalance_period=21, lookback_period=21):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(etf_list),), dtype=np.float32)\n",
    "\n",
    "        # Explicitly select feature columns (excluding Date and returns used only for calculating reward)\n",
    "        self.feature_cols = [col for col in data.columns if col not in ['Date'] and not col.startswith('Actual_Return')]\n",
    "        self.num_features_per_day = len(self.feature_cols)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.num_features_per_day * self.lookback_period,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(etf_list)] * len(etf_list))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(self.etf_list)] * len(self.etf_list))\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        next_step = self.current_step + 1\n",
    "\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            # v2 long short\n",
    "            desired_long = 1.20  # 120% long exposure explicitly\n",
    "            desired_short = 0.20  # 20% short exposure explicitly\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                # Normal 120/20 explicitly0\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                # Only long explicitly: default realistically to 100% long\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                # Only short explicitly (unrealistic), fallback clearly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                # All zeros explicitly: fallback explicitly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            # Apply explicit clipping\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate explicitly after clipping\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            # Final explicit normalization after clipping\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)  # exactly 100% long\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                # Realistic fallback explicitly: equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "            self.current_weights = final_weights\n",
    "            \n",
    "            # v1 softmax normalization\n",
    "            \n",
    "            # temperature = 0.5  # Explicitly lower for higher concentration (try 0.2 to 0.8)\n",
    "            # scaled_action = action / temperature\n",
    "            # self.current_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "\n",
    "        else:\n",
    "            returns_today = np.array([self.data.loc[self.current_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            self.current_weights *= (1 + returns_today)\n",
    "            self.current_weights /= np.sum(self.current_weights)\n",
    "\n",
    "        if next_step >= len(self.data):\n",
    "            terminated = True\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            returns = np.array([self.data.loc[next_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            portfolio_return = np.dot(self.current_weights, returns)\n",
    "            self.cumulative_wealth *= (1 + portfolio_return)\n",
    "            reward = self.calculate_reward(portfolio_return, returns)\n",
    "            terminated = next_step >= len(self.data) - 1\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "        # def _get_obs(self):\n",
    "        #     obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        #     obs_window = obs_window.drop(columns=['Date']).values.flatten().astype(np.float32)\n",
    "        #     return obs_window\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        obs_window = obs_window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "        return obs_window\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns):\n",
    "        if self.reward_type == 'cumulative_return':\n",
    "            return self.cumulative_wealth - 1.0\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            return np.log(self.cumulative_wealth)\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100 * alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            return portfolio_return - self.risk_coefficient * cvar\n",
    "        else:\n",
    "            raise ValueError('Invalid reward type')\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_stable_features(df, etf_list):\n",
    "    data = df.copy()\n",
    "\n",
    "    for etf in etf_list:\n",
    "        price_col = f'Price_{etf}'\n",
    "\n",
    "        # Volatility (20-day)\n",
    "        data[f'Volatility_{etf}'] = data[price_col].pct_change().rolling(20).std()\n",
    "\n",
    "        # Momentum indicators (returns over 5, 10, 20 days)\n",
    "        data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
    "        data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
    "        data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
    "\n",
    "        # Moving averages (5-day and 20-day)\n",
    "        data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
    "        data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
    "\n",
    "        # Moving average crossover (5-day MA - 20-day MA)\n",
    "        data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
    "\n",
    "    # Drop NaN values due to rolling calculations\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # Explicit patterns to identify columns\n",
    "    predicted_return_pattern = 'Predicted_Return'\n",
    "    shap_metric_pattern = 'SHAP'\n",
    "\n",
    "    # Exclude Predicted Returns explicitly if requested\n",
    "    if not include_predicted_returns:\n",
    "        predicted_cols = [col for col in df_filtered.columns if predicted_return_pattern in col]\n",
    "        df_filtered.drop(columns=predicted_cols, inplace=True)\n",
    "        print(f\"Excluded predicted return columns: {predicted_cols}\")\n",
    "\n",
    "    # Exclude SHAP-related metrics explicitly if requested\n",
    "    if not include_shap_metrics:\n",
    "        shap_cols = [col for col in df_filtered.columns if shap_metric_pattern in col]\n",
    "        df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "        print(f\"Excluded SHAP-related columns: {shap_cols}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# ETFs\n",
    "etf_list = ['XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLY', 'XLV', 'XLU']\n",
    "\n",
    "# etf_list = ['BA',\n",
    "# 'AMGN',\n",
    "# 'DIS',\n",
    "# 'NKE',\n",
    "# 'HON',\n",
    "# 'MMM',\n",
    "# 'CAT',\n",
    "# 'KO',\n",
    "# 'PG',\n",
    "# 'AXP',\n",
    "# 'JPM',\n",
    "# 'MCD',\n",
    "# 'HD',\n",
    "# 'AAPL',\n",
    "# 'CSCO',\n",
    "# 'IBM',\n",
    "# 'MSFT',\n",
    "# 'TRV',\n",
    "# 'UNH',\n",
    "# 'CVX',\n",
    "# 'JNJ',\n",
    "# 'MRK',\n",
    "# 'AMZN',\n",
    "# 'WMT',\n",
    "# 'INTC',\n",
    "# 'VZ']\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 5e-5],\n",
    "    'n_steps': [20, 40],\n",
    "    'batch_size': [5, 10],\n",
    "    'gamma': [0.98, 0.99]\n",
    "}\n",
    "consolidated_file = 'stage2_rl_observations_optimized_10ETFs.csv'\n",
    "reward_type = 'mean_cvar'\n",
    "# data = pd.read_csv(consolidated_file, parse_dates=['Date'])\n",
    "# data = data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "data = pd.read_csv('stage2_rl_observations_optimized_10ETFs.csv', parse_dates=['Date'])\n",
    "price_data = pd.read_csv('stock_prices_10ETFs.csv')\n",
    "# price_data = pd.read_csv('stock_prices_10ETFs.csv')\n",
    "# Convert the Date column in price data, handling the timezone correctly\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'], utc=True)\n",
    "price_data['Date'] = price_data['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Rename price columns explicitly to 'price_{ticker}'\n",
    "price_cols = {col: f'Price_{col}' for col in price_data.columns if col != 'Date'}\n",
    "price_data.rename(columns=price_cols, inplace=True)\n",
    "\n",
    "# Merge datasets on Date\n",
    "merged_data = pd.merge(data, price_data, on='Date', how='inner')\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "# Check if merge was successful\n",
    "if len(merged_data) != len(data):\n",
    "    print(f\"Warning: Data length mismatch after merging (Original: {len(data)}, Merged: {len(merged_data)}).\")\n",
    "else:\n",
    "    print(\"Merged successfully with aligned dates.\")\n",
    "\n",
    "data_with_features_raw = add_stable_features(merged_data, etf_list)\n",
    "data_with_features_raw.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Usage Example clearly for benchmark (only price metrics, no predicted return or SHAP):\n",
    "data_with_features = filter_features(data_with_features_raw, \n",
    "                                 include_predicted_returns=True, \n",
    "                                 include_shap_metrics=True)\n",
    "################### override data to use SHAP only\n",
    "# data_with_features = data\n",
    "################### END override \n",
    "\n",
    "# Define your rolling window lengths clearly:\n",
    "train_window_days = 252 * 10\n",
    "validation_window_days = 126\n",
    "prediction_window_days = 126\n",
    "lookback_period = 10\n",
    "rebalance_period = 10\n",
    "\n",
    "start_indices = range(0, len(data) - (train_window_days + validation_window_days + prediction_window_days), prediction_window_days)\n",
    "all_weights = []\n",
    "model_path = 'ppo_single_train_best_model_10ETFs.zip'\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "def validate_and_tune(train_data, val_data, reward_type, rebalance_period=10, lookback_period=10, n_iter=8, timesteps=5000):\n",
    "    best_reward, best_params = -np.inf, None\n",
    "\n",
    "    # Narrow and meaningful parameter distribution\n",
    "    param_dist = {\n",
    "        'learning_rate': [3e-4, 1e-4],\n",
    "        'n_steps': [20, 40],\n",
    "        'batch_size': [10, 20],\n",
    "        'gamma': [0.95, 0.98],\n",
    "        'risk_coefficient': [0.1, 0.5, 1.0] if reward_type in ['mean_var', 'mean_cvar'] else [0.5],\n",
    "        'seed': [42, 100, 2024, 12345, 579]\n",
    "    }\n",
    "\n",
    "    sampled_params = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=SEED))\n",
    "\n",
    "    for params in sampled_params:\n",
    "        seed = params.pop('seed')\n",
    "        risk_coeff = params.pop('risk_coefficient', 0.5)\n",
    "        set_global_seed(seed)\n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1, seed=seed)\n",
    "        model = PPO('MlpPolicy', env,\n",
    "                    ent_coef=0.01,    # explicitly encourages exploration\n",
    "                    clip_range=0.2,\n",
    "                    seed=seed,\n",
    "                    **params, verbose=0)\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        val_env = PortfolioEnv(val_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        obs, _ = val_env.reset(seed=seed)\n",
    "        done, total_reward = False, 0\n",
    "\n",
    "        while not done:\n",
    "            # num_samples = 100  # Recommended starting point\n",
    "            # action_samples = []\n",
    "            # \n",
    "            # for _ in range(num_samples):\n",
    "            #     sampled_action, _ = model.predict(obs, deterministic=False)  # obs directly\n",
    "            #     action_samples.append(sampled_action)\n",
    "            # \n",
    "            # action = np.mean(action_samples, axis=0)\n",
    "            \n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_params = {**params, 'risk_coefficient': risk_coeff, 'seed': seed}\n",
    "    with open('best_params.json', 'w') as f:\n",
    "        json.dump(best_params, f)\n",
    "    return best_params\n",
    "\n",
    "def scale_data(df, feature_cols, scaler):\n",
    "    scaled_features = scaler.transform(df[feature_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=feature_cols, index=df.index)\n",
    "\n",
    "    # Re-add columns that were not scaled (e.g., Date, Actual_Return_*)\n",
    "    for col in df.columns:\n",
    "        if col not in feature_cols:\n",
    "            scaled_df[col] = df[col].values\n",
    "\n",
    "    # Keep original column order\n",
    "    scaled_df = scaled_df[df.columns]\n",
    "    return scaled_df\n",
    "\n",
    "# Main execution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for idx, start_idx in enumerate(start_indices):\n",
    "    # for start_idx in range(0, 252*2, 252):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Explicit indices for training, validation, and prediction datasets\n",
    "    train_start_idx = start_idx\n",
    "    train_end_idx = train_start_idx + train_window_days\n",
    "\n",
    "    val_start_idx = train_end_idx\n",
    "    val_end_idx = val_start_idx + validation_window_days\n",
    "\n",
    "    pred_start_idx = val_end_idx\n",
    "    pred_end_idx = pred_start_idx + prediction_window_days\n",
    "\n",
    "    # Corresponding dates explicitly\n",
    "    train_start_date = data_with_features.loc[train_start_idx, 'Date']\n",
    "    train_end_date = data_with_features.loc[train_end_idx - 1, 'Date']\n",
    "\n",
    "    val_start_date = data_with_features.loc[val_start_idx, 'Date']\n",
    "    val_end_date = data_with_features.loc[val_end_idx - 1, 'Date']\n",
    "\n",
    "    pred_start_date = data_with_features.loc[pred_start_idx, 'Date']\n",
    "    pred_end_date = data_with_features.loc[pred_end_idx - 1, 'Date']\n",
    "\n",
    "    # Clearly print ranges for clarity\n",
    "    print(f\"Training period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "    print(f\"Validation period: {val_start_date.date()} to {val_end_date.date()}\")\n",
    "    print(f\"Prediction period: {pred_start_date.date()} to {pred_end_date.date()}\")\n",
    "\n",
    "    # Explicitly subset data accordingly\n",
    "    train_data = data_with_features.iloc[train_start_idx:train_end_idx].reset_index(drop=True)\n",
    "    val_data = data_with_features.iloc[val_start_idx:val_end_idx].reset_index(drop=True)\n",
    "    pred_data = data_with_features.iloc[pred_start_idx:pred_end_idx].reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [col for col in train_data.columns if col != 'Date' and not col.startswith('Actual_Return')]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data[feature_cols])\n",
    "\n",
    "    train_data_scaled = scale_data(train_data, feature_cols, scaler)\n",
    "    val_data_scaled = scale_data(val_data, feature_cols, scaler)\n",
    "    pred_data_scaled = scale_data(pred_data, feature_cols, scaler)\n",
    "\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    best_params = validate_and_tune(train_data_scaled, val_data_scaled, reward_type)\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "    incremental_timesteps = 3000    \n",
    "    max_timesteps = 30000\n",
    "    patience = 3\n",
    "    \n",
    "    best_val_reward = -np.inf\n",
    "    no_improve_steps = 0\n",
    "\n",
    "    # risk_coeff = best_params.pop('risk_coefficient',0.5)\n",
    "    policy_kwargs = dict(net_arch=[256, 256])\n",
    "\n",
    "    with open('best_params.json', 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "    \n",
    "    risk_coeff = best_params.pop('risk_coefficient')\n",
    "    seed = best_params.pop('seed')\n",
    "    \n",
    "    set_global_seed(seed)\n",
    "    env = make_vec_env(lambda: PortfolioEnv(train_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1, seed=seed)\n",
    "    \n",
    "    # Load previous model if exists\n",
    "    # if idx > 0 and os.path.exists(model_path):\n",
    "    #     print(f\"Loading previous model from {model_path}...\")\n",
    "    #     model = PPO.load(model_path, env=env)\n",
    "    #     model.set_env(env)\n",
    "    # else:\n",
    "    #     print(\"Initializing new PPO model...\")\n",
    "    #     model = PPO('MlpPolicy', env,\n",
    "    #                 policy_kwargs=policy_kwargs,\n",
    "    #                 ent_coef=0.01,\n",
    "    #                 clip_range=0.2,\n",
    "    #                 seed=seed, \n",
    "    #                 **best_params, verbose=0)\n",
    "     # always retrain\n",
    "    model = PPO('MlpPolicy', env,\n",
    "                    policy_kwargs=policy_kwargs,\n",
    "                    ent_coef=0.01,\n",
    "                    clip_range=0.2,\n",
    "                    seed=seed, \n",
    "                    **best_params, verbose=0)\n",
    "    # model.learn(total_timesteps=20000)\n",
    "    print(\"Starting model training with early stopping...\")\n",
    "\n",
    "    for step in range(0, max_timesteps, incremental_timesteps):\n",
    "        model.learn(total_timesteps=incremental_timesteps)\n",
    "    \n",
    "        # Evaluate on validation environment\n",
    "        val_env = PortfolioEnv(val_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        val_obs, _ = val_env.reset()\n",
    "        val_done = False\n",
    "        val_total_reward = 0.0\n",
    "    \n",
    "        while not val_done:\n",
    "            val_action, _ = model.predict(val_obs, deterministic=True)\n",
    "            # num_samples = 100  # Recommended\n",
    "            # value_action_samples = []\n",
    "            # \n",
    "            # for _ in range(num_samples):\n",
    "            #     value_sampled_action, _ = model.predict(val_obs, deterministic=False)\n",
    "            #     value_action_samples.append(value_sampled_action)\n",
    "            # \n",
    "            # val_action = np.mean(value_action_samples, axis=0)    \n",
    "            \n",
    "            val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "            val_total_reward += val_reward\n",
    "    \n",
    "        print(f\"Step: {step + incremental_timesteps}, Validation Total Reward: {val_total_reward:.4f}\")\n",
    "    \n",
    "        # Early stopping check\n",
    "        if val_total_reward > best_val_reward:\n",
    "            best_val_reward = val_total_reward\n",
    "            no_improve_steps = 0\n",
    "            # model.save(\"best_ppo_model.zip\")\n",
    "            model.save(model_path)\n",
    "            print(f\"Improved validation reward; model saved at step {step + incremental_timesteps}\")\n",
    "        else:\n",
    "            no_improve_steps += 1\n",
    "            print(f\"No improvement ({no_improve_steps}/{patience})\")\n",
    "    \n",
    "            if no_improve_steps >= patience:\n",
    "                print(\"Early stopping explicitly triggered.\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model explicitly\n",
    "    model = PPO.load(model_path)\n",
    "    print(\"Loaded the best PPO model explicitly for prediction.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Ensure historical context explicitly available in prediction\n",
    "    full_data = pd.concat([train_data_scaled, val_data_scaled, pred_data_scaled])\n",
    "    pred_data_with_history = full_data[full_data['Date'] >= (pred_start_date - pd.Timedelta(days=lookback_period))].reset_index(drop=True)\n",
    "\n",
    "    pred_env = PortfolioEnv(pred_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "    # pred_env = PortfolioEnv(pred_data_with_history, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "\n",
    "    obs, info = pred_env.reset()\n",
    "    done = False\n",
    "\n",
    "    action = np.zeros(len(etf_list), dtype=np.float32)\n",
    "\n",
    "    while not done:\n",
    "        if pred_env.current_step >= lookback_period and pred_env.current_step % pred_env.rebalance_period == 0:\n",
    "            # obs_for_agent = pred_data_with_history.drop(columns=['Date']).iloc[pred_env.current_step - lookback_period:pred_env.current_step].values.flatten().astype(np.float32)\n",
    "            # action, _ = model.predict(obs_for_agent, deterministic=True)\n",
    "\n",
    "            # v1 normalize weight\n",
    "            # action, _ = model.predict(obs, deterministic=True)\n",
    "            # use determinstic = FALSE       \n",
    "            # num_samples = 100  # Recommended\n",
    "            # action_samples = []\n",
    "            # for _ in range(num_samples):\n",
    "            #     sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "            #     action_samples.append(sampled_action)\n",
    "            # action = np.mean(action_samples, axis=0)    \n",
    "            # \n",
    "            # temperature = 0.5\n",
    "            # scaled_action = action / temperature\n",
    "            # weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "            # rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "            # all_weights.append([rebalance_date] + weights.tolist())\n",
    "\n",
    "\n",
    "            # v2 long short normalization\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # uncomment this for predictopm\n",
    "            # num_samples = 100  # Recommended\n",
    "            # action_samples = []\n",
    "            # \n",
    "            # for _ in range(num_samples):\n",
    "            #     sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "            #     action_samples.append(sampled_action)\n",
    "            # \n",
    "            # action = np.mean(action_samples, axis=0)    \n",
    "\n",
    "            # Explicitly apply your new 120/20 normalization logic (to match environment step)\n",
    "            desired_long = 1.20  # Explicitly 120% long exposure\n",
    "            desired_short = 0.20  # Explicitly 20% short exposure\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate after clipping explicitly\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "\n",
    "            rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "            all_weights.append([rebalance_date] + final_weights.tolist())\n",
    "\n",
    "        obs, _, done, _, _ = pred_env.step(action)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "columns = ['Date'] + etf_list\n",
    "weights_df = pd.DataFrame(all_weights, columns=columns)\n",
    "weights_df.to_csv('ppo_multi_year_weights_10ETFs.csv', index=False)\n",
    "print(\"Saved predictions to ppo_multi_year_weights_10ETFs.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c770edf2acf6b5a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "############################## This is start to run 25 iterations ##############################\n",
    "########################################################################################################################"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8acf6abff959b252",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ITERATION - final variable: 128/20 - retrain - 50kx30k sample - mean cvar - determinstic false with 50 - 7 yr train by 21 day test\n",
    "# start of stage 2 training\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar', risk_coefficient=0.5, rebalance_period=21, lookback_period=21):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(etf_list),), dtype=np.float32)\n",
    "\n",
    "        # Explicitly select feature columns (excluding Date and returns used only for calculating reward)\n",
    "        self.feature_cols = [col for col in data.columns if col not in ['Date'] and not col.startswith('Actual_Return')]\n",
    "        self.num_features_per_day = len(self.feature_cols)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.num_features_per_day * self.lookback_period,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(etf_list)] * len(etf_list))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(self.etf_list)] * len(self.etf_list))\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        next_step = self.current_step + 1\n",
    "\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            # v2 long short\n",
    "            desired_long = 1.20  # 120% long exposure explicitly\n",
    "            desired_short = 0.20  # 20% short exposure explicitly\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                # Normal 120/20 explicitly0\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                # Only long explicitly: default realistically to 100% long\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                # Only short explicitly (unrealistic), fallback clearly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                # All zeros explicitly: fallback explicitly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            # Apply explicit clipping\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate explicitly after clipping\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            # Final explicit normalization after clipping\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)  # exactly 100% long\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                # Realistic fallback explicitly: equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "            self.current_weights = final_weights\n",
    "            \n",
    "            # v1 softmax normalization\n",
    "            # \n",
    "            # temperature = 0.5  # Explicitly lower for higher concentration (try 0.2 to 0.8)\n",
    "            # scaled_action = action / temperature\n",
    "            # self.current_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "\n",
    "        else:\n",
    "            returns_today = np.array([self.data.loc[self.current_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            self.current_weights *= (1 + returns_today)\n",
    "            self.current_weights /= np.sum(self.current_weights)\n",
    "\n",
    "        if next_step >= len(self.data):\n",
    "            terminated = True\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            returns = np.array([self.data.loc[next_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            portfolio_return = np.dot(self.current_weights, returns)\n",
    "            self.cumulative_wealth *= (1 + portfolio_return)\n",
    "            reward = self.calculate_reward(portfolio_return, returns)\n",
    "            terminated = next_step >= len(self.data) - 1\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "        # def _get_obs(self):\n",
    "        #     obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        #     obs_window = obs_window.drop(columns=['Date']).values.flatten().astype(np.float32)\n",
    "        #     return obs_window\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        obs_window = obs_window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "        return obs_window\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns):\n",
    "        if self.reward_type == 'cumulative_return':\n",
    "            return self.cumulative_wealth - 1.0\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            return np.log(self.cumulative_wealth)\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100 * alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            return portfolio_return - self.risk_coefficient * cvar\n",
    "        else:\n",
    "            raise ValueError('Invalid reward type')\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_stable_features(df, etf_list):\n",
    "    data = df.copy()\n",
    "\n",
    "    for etf in etf_list:\n",
    "        price_col = f'Price_{etf}'\n",
    "\n",
    "        # Volatility (20-day)\n",
    "        data[f'Volatility_{etf}'] = data[price_col].pct_change().rolling(20).std()\n",
    "\n",
    "        # Momentum indicators (returns over 5, 10, 20 days)\n",
    "        data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
    "        data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
    "        data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
    "\n",
    "        # Moving averages (5-day and 20-day)\n",
    "        data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
    "        data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
    "\n",
    "        # Moving average crossover (5-day MA - 20-day MA)\n",
    "        data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
    "\n",
    "    # Drop NaN values due to rolling calculations\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # Explicit patterns to identify columns\n",
    "    predicted_return_pattern = 'Predicted_Return'\n",
    "    shap_metric_pattern = 'SHAP'\n",
    "\n",
    "    # Exclude Predicted Returns explicitly if requested\n",
    "    if not include_predicted_returns:\n",
    "        predicted_cols = [col for col in df_filtered.columns if predicted_return_pattern in col]\n",
    "        df_filtered.drop(columns=predicted_cols, inplace=True)\n",
    "        print(f\"Excluded predicted return columns: {predicted_cols}\")\n",
    "\n",
    "    # Exclude SHAP-related metrics explicitly if requested\n",
    "    if not include_shap_metrics:\n",
    "        shap_cols = [col for col in df_filtered.columns if shap_metric_pattern in col]\n",
    "        df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "        print(f\"Excluded SHAP-related columns: {shap_cols}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# ETFs\n",
    "# etf_list = ['XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLY', 'XLV', 'XLU']\n",
    "etf_list = ['BA',\n",
    "'AMGN',\n",
    "'DIS',\n",
    "'NKE',\n",
    "'HON',\n",
    "'MMM',\n",
    "'CAT',\n",
    "'KO',\n",
    "'PG',\n",
    "'AXP',\n",
    "'JPM',\n",
    "'MCD',\n",
    "'HD',\n",
    "'AAPL',\n",
    "'CSCO',\n",
    "'IBM',\n",
    "'MSFT',\n",
    "'TRV',\n",
    "'UNH',\n",
    "'CVX',\n",
    "'JNJ',\n",
    "'MRK',\n",
    "'AMZN',\n",
    "'WMT',\n",
    "'INTC',\n",
    "'VZ']\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 5e-5],\n",
    "    'n_steps': [20, 40],\n",
    "    'batch_size': [5, 10],\n",
    "    'gamma': [0.98, 0.99]\n",
    "}\n",
    "consolidated_file = 'stage2_rl_observations_optimized_DIA_ETF.csv'\n",
    "reward_type = 'mean_cvar'\n",
    "# data = pd.read_csv(consolidated_file, parse_dates=['Date'])\n",
    "# data = data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "data = pd.read_csv('stage2_rl_observations_optimized_DIA_ETF.csv', parse_dates=['Date'])\n",
    "price_data = pd.read_csv('stock_prices_DIA_ETF.csv')\n",
    "\n",
    "# Convert the Date column in price data, handling the timezone correctly\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'], utc=True)\n",
    "price_data['Date'] = price_data['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Rename price columns explicitly to 'price_{ticker}'\n",
    "price_cols = {col: f'Price_{col}' for col in price_data.columns if col != 'Date'}\n",
    "price_data.rename(columns=price_cols, inplace=True)\n",
    "\n",
    "# Merge datasets on Date\n",
    "merged_data = pd.merge(data, price_data, on='Date', how='inner')\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "# Check if merge was successful\n",
    "if len(merged_data) != len(data):\n",
    "    print(f\"Warning: Data length mismatch after merging (Original: {len(data)}, Merged: {len(merged_data)}).\")\n",
    "else:\n",
    "    print(\"Merged successfully with aligned dates.\")\n",
    "\n",
    "data_with_features_raw = add_stable_features(merged_data, etf_list)\n",
    "data_with_features_raw.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Usage Example clearly for benchmark (only price metrics, no predicted return or SHAP):\n",
    "data_with_features = filter_features(data_with_features_raw, \n",
    "                                 include_predicted_returns=True, \n",
    "                                 include_shap_metrics=True)\n",
    "################### override data to use SHAP only\n",
    "# data_with_features = data\n",
    "################### END override \n",
    "\n",
    "# Define your rolling window lengths clearly:\n",
    "train_window_days = 252 * 7\n",
    "validation_window_days = 252\n",
    "prediction_window_days = 252\n",
    "lookback_period = 21\n",
    "rebalance_period = 21\n",
    "\n",
    "start_indices = range(0, len(data) - (train_window_days + validation_window_days + prediction_window_days), prediction_window_days)\n",
    "all_weights = []\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "def validate_and_tune(train_data, val_data, reward_type, rebalance_period=10, lookback_period=10, n_iter=8, timesteps=5000):\n",
    "    best_reward, best_params = -np.inf, None\n",
    "\n",
    "    # Narrow and meaningful parameter distribution\n",
    "    param_dist = {\n",
    "        'learning_rate': [3e-4, 1e-4],\n",
    "        'n_steps': [20, 40],\n",
    "        'batch_size': [10, 20],\n",
    "        'gamma': [0.95, 0.98],\n",
    "        'risk_coefficient': [0.1, 0.5, 1.0] if reward_type in ['mean_var', 'mean_cvar'] else [0.5],\n",
    "    }\n",
    "\n",
    "    sampled_params = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=42))\n",
    "\n",
    "    for params in sampled_params:\n",
    "        risk_coeff = params.pop('risk_coefficient', 0.5)\n",
    "\n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "        model = PPO('MlpPolicy', env,\n",
    "                    ent_coef=0.01,    # explicitly encourages exploration\n",
    "                    clip_range=0.2,\n",
    "                    **params, verbose=0)\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        val_env = PortfolioEnv(val_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        obs, _ = val_env.reset()\n",
    "        done, total_reward = False, 0\n",
    "        \n",
    "        # while not done:\n",
    "        #     action, _ = model.predict(obs, deterministic=True)\n",
    "        #     obs, reward, done, _, _ = val_env.step(action)\n",
    "        #     total_reward += reward\n",
    "        \n",
    "        while not done:\n",
    "            num_samples = 100  # Recommended starting point\n",
    "            action_samples = []\n",
    "        \n",
    "            for _ in range(num_samples):\n",
    "                sampled_action, _ = model.predict(obs, deterministic=False)  # obs directly\n",
    "                action_samples.append(sampled_action)\n",
    "        \n",
    "            action = np.mean(action_samples, axis=0)\n",
    "        \n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_params = {**params, 'risk_coefficient': risk_coeff}\n",
    "\n",
    "    return best_params\n",
    "\n",
    "def scale_data(df, feature_cols, scaler):\n",
    "    scaled_features = scaler.transform(df[feature_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=feature_cols, index=df.index)\n",
    "\n",
    "    # Re-add columns that were not scaled (e.g., Date, Actual_Return_*)\n",
    "    for col in df.columns:\n",
    "        if col not in feature_cols:\n",
    "            scaled_df[col] = df[col].values\n",
    "\n",
    "    # Keep original column order\n",
    "    scaled_df = scaled_df[df.columns]\n",
    "    return scaled_df\n",
    "\n",
    "# Main execution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "iterations = 10\n",
    "all_weights_iterations = []\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    print(f\"\\n==== Starting Iteration {iteration + 1}/{iterations} ====\")\n",
    "    model_path = f\"ppo_train_best_model_iteration_{iteration}.zip\"\n",
    "    for idx, start_idx in enumerate(start_indices):\n",
    "        # for start_idx in range(0, 252*2, 252):\n",
    "        start_time = time.time()\n",
    "    \n",
    "        # Explicit indices for training, validation, and prediction datasets\n",
    "        train_start_idx = start_idx\n",
    "        train_end_idx = train_start_idx + train_window_days\n",
    "    \n",
    "        val_start_idx = train_end_idx\n",
    "        val_end_idx = val_start_idx + validation_window_days\n",
    "    \n",
    "        pred_start_idx = val_end_idx\n",
    "        pred_end_idx = pred_start_idx + prediction_window_days\n",
    "    \n",
    "        # Corresponding dates explicitly\n",
    "        train_start_date = data_with_features.loc[train_start_idx, 'Date']\n",
    "        train_end_date = data_with_features.loc[train_end_idx - 1, 'Date']\n",
    "    \n",
    "        val_start_date = data_with_features.loc[val_start_idx, 'Date']\n",
    "        val_end_date = data_with_features.loc[val_end_idx - 1, 'Date']\n",
    "    \n",
    "        pred_start_date = data_with_features.loc[pred_start_idx, 'Date']\n",
    "        pred_end_date = data_with_features.loc[pred_end_idx - 1, 'Date']\n",
    "    \n",
    "        # Clearly print ranges for clarity\n",
    "        print(f\"Training period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "        print(f\"Validation period: {val_start_date.date()} to {val_end_date.date()}\")\n",
    "        print(f\"Prediction period: {pred_start_date.date()} to {pred_end_date.date()}\")\n",
    "    \n",
    "        # Explicitly subset data accordingly\n",
    "        train_data = data_with_features.iloc[train_start_idx:train_end_idx].reset_index(drop=True)\n",
    "        val_data = data_with_features.iloc[val_start_idx:val_end_idx].reset_index(drop=True)\n",
    "        pred_data = data_with_features.iloc[pred_start_idx:pred_end_idx].reset_index(drop=True)\n",
    "    \n",
    "        feature_cols = [col for col in train_data.columns if col != 'Date' and not col.startswith('Actual_Return')]\n",
    "    \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_data[feature_cols])\n",
    "    \n",
    "        train_data_scaled = scale_data(train_data, feature_cols, scaler)\n",
    "        val_data_scaled = scale_data(val_data, feature_cols, scaler)\n",
    "        pred_data_scaled = scale_data(pred_data, feature_cols, scaler)\n",
    "    \n",
    "        print(\"Starting hyperparameter tuning...\")\n",
    "        best_params = validate_and_tune(train_data_scaled, val_data_scaled, reward_type)\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "        incremental_timesteps = 5000\n",
    "        max_timesteps = 30000\n",
    "        patience = 3\n",
    "        \n",
    "        best_val_reward = -np.inf\n",
    "        no_improve_steps = 0\n",
    "    \n",
    "        risk_coeff = best_params.pop('risk_coefficient',0.5)\n",
    "        policy_kwargs = dict(net_arch=[256, 256])\n",
    "    \n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "        \n",
    "        # Load previous model if exists\n",
    "        if idx > 0 and os.path.exists(model_path):\n",
    "            print(f\"Loading previous model from {model_path}...\")\n",
    "            model = PPO.load(model_path, env=env)\n",
    "            model.set_env(env)\n",
    "        else:\n",
    "            print(\"Initializing new PPO model...\")\n",
    "            model = PPO('MlpPolicy', env,\n",
    "                        policy_kwargs=policy_kwargs,\n",
    "                        ent_coef=0.01,\n",
    "                        clip_range=0.2,\n",
    "                        **best_params, verbose=0)\n",
    "         # always retrain\n",
    "        # model = PPO('MlpPolicy', env,\n",
    "        #             policy_kwargs=policy_kwargs,\n",
    "        #             ent_coef=0.01,    # explicitly encourages exploration\n",
    "        #             clip_range=0.2,\n",
    "        #             **best_params, verbose=0)\n",
    "        # print(\"Starting model training...\")\n",
    "        # model.learn(total_timesteps=20000)\n",
    "        print(\"Starting model training with early stopping...\")\n",
    "        \n",
    "        # model = PPO('MlpPolicy', env,\n",
    "        #             policy_kwargs=policy_kwargs,\n",
    "        #             ent_coef=0.01,    # explicitly encourages exploration\n",
    "        #             clip_range=0.2,\n",
    "        #             **best_params, verbose=0)\n",
    "        # print(\"Starting model training...\")\n",
    "        # model.learn(total_timesteps=20000)\n",
    "    \n",
    "        for step in range(0, max_timesteps, incremental_timesteps):\n",
    "            model.learn(total_timesteps=incremental_timesteps)\n",
    "        \n",
    "            # Evaluate on validation environment\n",
    "            val_env = PortfolioEnv(val_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "            val_obs, _ = val_env.reset()\n",
    "            val_done = False\n",
    "            val_total_reward = 0.0\n",
    "        \n",
    "            while not val_done:\n",
    "                # val_action, _ = model.predict(val_obs, deterministic=True)\n",
    "                # val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "                # val_total_reward += val_reward\n",
    "                \n",
    "                num_samples = 100  # Recommended\n",
    "                value_action_samples = []\n",
    "        \n",
    "                for _ in range(num_samples):\n",
    "                    value_sampled_action, _ = model.predict(val_obs, deterministic=False)\n",
    "                    value_action_samples.append(value_sampled_action)\n",
    "            \n",
    "                val_action = np.mean(value_action_samples, axis=0)    \n",
    "                \n",
    "                val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "                val_total_reward += val_reward\n",
    "        \n",
    "            print(f\"Step: {step + incremental_timesteps}, Validation Total Reward: {val_total_reward:.4f}\")\n",
    "        \n",
    "            # Early stopping check\n",
    "            if val_total_reward > best_val_reward:\n",
    "                best_val_reward = val_total_reward\n",
    "                no_improve_steps = 0\n",
    "                # model.save(\"best_ppo_model.zip\")\n",
    "                model.save(model_path)\n",
    "                print(f\"Improved validation reward; model saved at step {step + incremental_timesteps}\")\n",
    "            else:\n",
    "                no_improve_steps += 1\n",
    "                print(f\"No improvement ({no_improve_steps}/{patience})\")\n",
    "        \n",
    "                if no_improve_steps >= patience:\n",
    "                    print(\"Early stopping explicitly triggered.\")\n",
    "                    break\n",
    "        \n",
    "        # Load the best model explicitly\n",
    "        # model = PPO.load(\"best_ppo_model.zip\")\n",
    "        model = PPO.load(model_path)\n",
    "        \n",
    "        print(\"Loaded the best PPO model explicitly for prediction.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "        # Ensure historical context explicitly available in prediction\n",
    "        full_data = pd.concat([train_data_scaled, val_data_scaled, pred_data_scaled])\n",
    "        pred_data_with_history = full_data[full_data['Date'] >= (pred_start_date - pd.Timedelta(days=lookback_period))].reset_index(drop=True)\n",
    "    \n",
    "        pred_env = PortfolioEnv(pred_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        # pred_env = PortfolioEnv(pred_data_with_history, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "    \n",
    "        obs, info = pred_env.reset()\n",
    "        done = False\n",
    "    \n",
    "        action = np.zeros(len(etf_list), dtype=np.float32)\n",
    "    \n",
    "        while not done:\n",
    "            if pred_env.current_step >= lookback_period and pred_env.current_step % pred_env.rebalance_period == 0:\n",
    "                # obs_for_agent = pred_data_with_history.drop(columns=['Date']).iloc[pred_env.current_step - lookback_period:pred_env.current_step].values.flatten().astype(np.float32)\n",
    "                # action, _ = model.predict(obs_for_agent, deterministic=True)\n",
    "    \n",
    "                # v1 normalize weight\n",
    "                # action, _ = model.predict(obs, deterministic=True)\n",
    "                \n",
    "                # num_samples = 100  # Recommended\n",
    "                # action_samples = []\n",
    "                # \n",
    "                # for _ in range(num_samples):\n",
    "                #     sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "                #     action_samples.append(sampled_action)\n",
    "                # \n",
    "                # action = np.mean(action_samples, axis=0)    \n",
    "                # \n",
    "                # temperature = 0.5\n",
    "                # scaled_action = action / temperature\n",
    "                # final_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "                # rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "                # # all_weights.append([rebalance_date] + weights.tolist())\n",
    "                # all_weights_iterations.append([iteration + 1, rebalance_date] + final_weights.tolist())\n",
    "    \n",
    "                # v2 long short normalization\n",
    "                # action, _ = model.predict(obs, deterministic=True)\n",
    "                \n",
    "                num_samples = 100  # Recommended\n",
    "                action_samples = []\n",
    "\n",
    "                for _ in range(num_samples):\n",
    "                    sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "                    action_samples.append(sampled_action)\n",
    "\n",
    "                action = np.mean(action_samples, axis=0)    \n",
    "\n",
    "                # Explicitly apply your new 120/20 normalization logic (to match environment step)\n",
    "                desired_long = 1.20  # Explicitly 120% long exposure\n",
    "                desired_short = 0.20  # Explicitly 20% short exposure\n",
    "                clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "                raw_weights = action.copy()\n",
    "\n",
    "                # Separate explicitly positive (long) and negative (short) actions\n",
    "                long_weights = np.maximum(raw_weights, 0)\n",
    "                short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "                has_longs = np.sum(long_weights) > 0\n",
    "                has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "                if has_longs and has_shorts:\n",
    "                    normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                    normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "                elif has_longs and not has_shorts:\n",
    "                    normalized_long = long_weights / np.sum(long_weights)\n",
    "                    normalized_short = np.zeros_like(short_weights)\n",
    "                elif not has_longs and has_shorts:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    normalized_long = np.ones(num_assets) / num_assets\n",
    "                    normalized_short = np.zeros(num_assets)\n",
    "                else:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    normalized_long = np.ones(num_assets) / num_assets\n",
    "                    normalized_short = np.zeros(num_assets)\n",
    "\n",
    "                combined_weights = normalized_long - normalized_short\n",
    "                clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "                # Re-separate after clipping explicitly\n",
    "                long_clipped = np.maximum(clipped_weights, 0)\n",
    "                short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "                has_long_clipped = np.sum(long_clipped) > 0\n",
    "                has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "                if has_long_clipped and has_short_clipped:\n",
    "                    final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                    final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "                elif has_long_clipped and not has_short_clipped:\n",
    "                    final_long = long_clipped / np.sum(long_clipped)\n",
    "                    final_short = np.zeros_like(short_clipped)\n",
    "                else:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    final_long = np.ones(num_assets) / num_assets\n",
    "                    final_short = np.zeros(num_assets)\n",
    "\n",
    "                final_weights = final_long - final_short\n",
    "\n",
    "                rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "                # all_weights.append([rebalance_date] + final_weights.tolist())\n",
    "                all_weights_iterations.append([iteration + 1, rebalance_date] + final_weights.tolist())\n",
    "                # \n",
    "            obs, _, done, _, _ = pred_env.step(action)\n",
    "    \n",
    "        end_time = time.time()\n",
    "        print(f\"Iteration {iteration + 1}, start index {start_idx} completed in {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "columns = ['Iteration', 'Date'] + etf_list\n",
    "weights_df = pd.DataFrame(all_weights_iterations, columns=columns)\n",
    "weights_df.to_csv('ppo_allocations_multiple_iterations_DIA_ETF.csv', index=False)\n",
    "print(\"Saved all iterations' allocations to ppo_allocations_multiple_iterations_DIA_ETF.csv\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86d1a98b60121652",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Stage2 PPO training with recommended enhancements\n",
    "# ==================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utility functions and seeding\n",
    "# -------------------------------------------------------------------\n",
    "SEED = 42\n",
    "\n",
    "def set_global_seed(seed):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_global_seed(SEED)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Environment definition with softmax normalisation and MeanCVaR reward\n",
    "# -------------------------------------------------------------------\n",
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gym environment for portfolio allocation.\n",
    "    Observations are flattened windows of features; actions are unconstrained\n",
    "    real numbers that are converted to portfolio weights via softmax.\n",
    "    Reward is computed at each rebalance period as mean minus   CVaR.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar',\n",
    "                 risk_coefficient=1.0, rebalance_period=21,\n",
    "                 lookback_period=60):\n",
    "        super().__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "\n",
    "        # Action space: one unbounded action per asset\n",
    "        self.action_space = spaces.Box(low=-10, high=10, shape=(len(etf_list),), dtype=np.float32)\n",
    "\n",
    "        # Observation space: flatten last lookback_period days of features\n",
    "        self.feature_cols = [c for c in data.columns\n",
    "                             if c not in ['Date'] and not c.startswith('Actual_Return')]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(len(self.feature_cols)*lookback_period,),\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "        self.current_step = self.lookback_period\n",
    "        self.current_weights = np.array([1/len(etf_list)]*len(etf_list), dtype=float)\n",
    "        self.cumulative_wealth = 1.0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.current_weights = np.array([1/len(self.etf_list)]*len(self.etf_list),\n",
    "                                        dtype=float)\n",
    "        self.cumulative_wealth = 1.0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Return a flattened window of recent features.\"\"\"\n",
    "        window = self.data.iloc[\n",
    "            self.current_step - self.lookback_period : self.current_step\n",
    "        ]\n",
    "        return window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "\n",
    "    def _action_to_weights(self, action):\n",
    "        \"\"\"\n",
    "        Convert raw action outputs into a valid longonly weight vector via softmax.\n",
    "        This implements the 'continuous 10dimensional weights with softmax normalisation'\n",
    "        specification from your methodology (Step4).\n",
    "        \"\"\"\n",
    "        # temperature scaling  adjust if you want more/less concentration\n",
    "        temperature = 1.0\n",
    "        scaled = action / temperature\n",
    "        exp_vals = np.exp(scaled - np.max(scaled))\n",
    "        return exp_vals / exp_vals.sum()\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns):\n",
    "        \"\"\"Compute reward according to the chosen risk measure.\"\"\"\n",
    "        if self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100*alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            return portfolio_return - self.risk_coefficient * cvar\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            return np.log(self.cumulative_wealth)\n",
    "        elif self.reward_type == 'cumulative_return':\n",
    "            return self.cumulative_wealth - 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown reward_type {self.reward_type}\")\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Update portfolio and compute reward.\"\"\"\n",
    "        next_step = self.current_step + 1\n",
    "\n",
    "        # rebalance portfolio at rebalance dates\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            self.current_weights = self._action_to_weights(action)\n",
    "        else:\n",
    "            # drift weights using actual returns\n",
    "            daily_rets = np.array([\n",
    "                self.data.loc[self.current_step, f'Actual_Return_{t}']\n",
    "                for t in self.etf_list\n",
    "            ])\n",
    "            self.current_weights *= (1 + daily_rets)\n",
    "            self.current_weights /= self.current_weights.sum()\n",
    "\n",
    "        # compute reward on the next day\n",
    "        if next_step >= len(self.data):\n",
    "            done = True\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            asset_returns = np.array([\n",
    "                self.data.loc[next_step, f'Actual_Return_{t}']\n",
    "                for t in self.etf_list\n",
    "            ])\n",
    "            portfolio_ret = float(np.dot(self.current_weights, asset_returns))\n",
    "            self.cumulative_wealth *= (1 + portfolio_ret)\n",
    "            reward = self.calculate_reward(portfolio_ret, asset_returns)\n",
    "            done = (next_step >= len(self.data) - 1)\n",
    "\n",
    "        self.current_step += 1\n",
    "        return self._get_obs(), reward, done, False, {}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Data preparation (Step1)\n",
    "# -------------------------------------------------------------------\n",
    "# Load your Stage2 RL observations (predicted returns, SHAP, etc.)\n",
    "stage2_file = 'stage2_rl_observations_optimized_10ETFs.csv'\n",
    "price_file = 'stock_prices_10ETFs.csv'\n",
    "\n",
    "stage2 = pd.read_csv(stage2_file, parse_dates=['Date'])\n",
    "prices = pd.read_csv(price_file)\n",
    "prices['Date'] = pd.to_datetime(prices['Date'], utc=True).dt.tz_localize(None)\n",
    "\n",
    "# Align data on Date\n",
    "prices.rename(columns={c: f'Price_{c}' for c in prices.columns if c != 'Date'},\n",
    "              inplace=True)\n",
    "data_merged = pd.merge(stage2, prices, on='Date', how='inner')\n",
    "\n",
    "# Compute technical indicators (volatility, momentum, moving averages)\n",
    "# as outlined in the methodology (20day volatility, 5/10/20day momentum,\n",
    "# 5 and 20day moving averages and crossover).\n",
    "\n",
    "\n",
    "def add_features(df, etfs):\n",
    "    df2 = df.copy()\n",
    "    for etf in etfs:\n",
    "        price_col = f'Price_{etf}'\n",
    "        returns = df2[price_col].pct_change()\n",
    "        df2[f'Volatility_{etf}'] = returns.rolling(20).std()\n",
    "        df2[f'Momentum_5d_{etf}'] = returns.rolling(5).sum()\n",
    "        df2[f'Momentum_10d_{etf}'] = returns.rolling(10).sum()\n",
    "        df2[f'Momentum_20d_{etf}'] = returns.rolling(20).sum()\n",
    "        df2[f'MA_5d_{etf}'] = df2[price_col].rolling(5).mean()\n",
    "        df2[f'MA_20d_{etf}'] = df2[price_col].rolling(20).mean()\n",
    "        df2[f'MA_Crossover_{etf}'] = df2[f'MA_5d_{etf}'] - df2[f'MA_20d_{etf}']\n",
    "    return df2.dropna()\n",
    "\n",
    "data_with_features = add_features(data_merged, ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'])\n",
    "\n",
    "# Optionally, filter out predicted returns or SHAP metrics; here we include both\n",
    "# because they are key inputs in Step4s observation space.\n",
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df2 = df.copy()\n",
    "    if not include_predicted_returns:\n",
    "        cols = [c for c in df2.columns if 'Predicted_Return' in c]\n",
    "        df2.drop(columns=cols, inplace=True)\n",
    "    if not include_shap_metrics:\n",
    "        cols = [c for c in df2.columns if 'SHAP' in c]\n",
    "        df2.drop(columns=cols, inplace=True)\n",
    "    return df2\n",
    "\n",
    "data_with_features = filter_features(data_with_features, True, True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Rolling window splits (Stage2 Initial Training, Validation, Test)\n",
    "# -------------------------------------------------------------------\n",
    "# Use 10 years for training, 2 years for validation, 3 years for test\n",
    "train_days = 252*10\n",
    "val_days   = 252*2\n",
    "test_days  = 252*3\n",
    "\n",
    "lookback  = 60         # 60day lookback (recommended)\n",
    "rebalance = 21         # monthly rebalance (21 trading days)\n",
    "\n",
    "# In a real implementation you would loop over many start dates; here we take the first one\n",
    "start_idx = 0\n",
    "train_data = data_with_features.iloc[start_idx:start_idx+train_days].reset_index(drop=True)\n",
    "val_data   = data_with_features.iloc[start_idx+train_days:\n",
    "                                     start_idx+train_days+val_days].reset_index(drop=True)\n",
    "test_data  = data_with_features.iloc[start_idx+train_days+val_days:\n",
    "                                     start_idx+train_days+val_days+test_days].reset_index(drop=True)\n",
    "\n",
    "# Standardise features\n",
    "feature_cols = [c for c in train_data.columns if c not in ['Date']\n",
    "                and not c.startswith('Actual_Return')]\n",
    "scaler = StandardScaler().fit(train_data[feature_cols])\n",
    "\n",
    "def scale(df):\n",
    "    x = scaler.transform(df[feature_cols])\n",
    "    df_scaled = pd.DataFrame(x, columns=feature_cols, index=df.index)\n",
    "    for col in df.columns:\n",
    "        if col not in feature_cols:\n",
    "            df_scaled[col] = df[col]\n",
    "    return df_scaled[df.columns]\n",
    "\n",
    "train_scaled = scale(train_data)\n",
    "val_scaled   = scale(val_data)\n",
    "test_scaled  = scale(test_data)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# PPO Training with improved hyperparameters (Step5)\n",
    "# -------------------------------------------------------------------\n",
    "def linear_schedule(initial_value, final_value):\n",
    "    def schedule(progress_remaining):\n",
    "        return final_value + progress_remaining * (initial_value - final_value)\n",
    "    return schedule\n",
    "\n",
    "# Use a vectorised environment with 10 parallel instances for faster training\n",
    "n_envs = 10\n",
    "def make_env():\n",
    "    return PortfolioEnv(train_scaled, \n",
    "                        ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                        reward_type='mean_cvar',\n",
    "                        risk_coefficient=1.0,\n",
    "                        rebalance_period=rebalance,\n",
    "                        lookback_period=lookback)\n",
    "\n",
    "vec_env = SubprocVecEnv([make_env for _ in range(n_envs)], start_method='spawn')\n",
    "\n",
    "# PPO hyperparameters inspired by recent research:contentReference[oaicite:0]{index=0}\n",
    "# n_steps collects 3 months of daily data per environment: 252 * 3  756\n",
    "n_steps = 252 * 3\n",
    "ppo_model = PPO(\n",
    "    policy='MlpPolicy',\n",
    "    env=vec_env,\n",
    "    learning_rate=linear_schedule(3e-4, 1e-5),\n",
    "    n_steps=n_steps,\n",
    "    batch_size=1260,           # 252 * 5\n",
    "    n_epochs=16,\n",
    "    gamma=0.9,                 # lower discount to focus on nearterm returns\n",
    "    gae_lambda=0.9,\n",
    "    clip_range=0.25,\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=[64, 64],\n",
    "        activation_fn=torch.nn.Tanh,\n",
    "        log_std_init=-1.0\n",
    "    ),\n",
    "    ent_coef=0.01,\n",
    "    seed=SEED,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train for 7.5 million timesteps (600 episodes  10 envs  2525 steps)\n",
    "total_timesteps = int(7.5e6)\n",
    "ppo_model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "# Optionally save the model\n",
    "ppo_model.save('ppo_stage2_best_model.zip')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Validation and early stopping (Step6)\n",
    "# -------------------------------------------------------------------\n",
    "# After training, evaluate on the validation set without updating parameters\n",
    "val_env = PortfolioEnv(val_scaled,\n",
    "                       ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                       reward_type='mean_cvar',\n",
    "                       risk_coefficient=1.0,\n",
    "                       rebalance_period=rebalance,\n",
    "                       lookback_period=lookback)\n",
    "obs, _ = val_env.reset(seed=SEED)\n",
    "done = False\n",
    "val_reward = 0.0\n",
    "while not done:\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, _ = val_env.step(action)\n",
    "    val_reward += reward\n",
    "print(f\"Total validation reward: {val_reward:.4f}\")\n",
    "\n",
    "# If necessary, you can adjust hyperparameters and retrain based on validation performance.\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Outofsample testing (20222024) and performance metrics (Step6)\n",
    "# -------------------------------------------------------------------\n",
    "test_env = PortfolioEnv(test_scaled,\n",
    "                        ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                        reward_type='mean_cvar',\n",
    "                        risk_coefficient=1.0,\n",
    "                        rebalance_period=rebalance,\n",
    "                        lookback_period=lookback)\n",
    "\n",
    "obs, _ = test_env.reset()\n",
    "done = False\n",
    "rebalance_dates = []\n",
    "weights_history = []\n",
    "while not done:\n",
    "    # produce an action every step; env will apply it only on rebalance dates\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    obs, _, done, _, _ = test_env.step(action)\n",
    "    # record weights at rebalance points\n",
    "    if test_env.current_step % rebalance == 0:\n",
    "        date = test_scaled.loc[test_env.current_step-1, 'Date']\n",
    "        weights_history.append([date] + test_env.current_weights.tolist())\n",
    "\n",
    "# Save monthly weights to CSV\n",
    "weights_df = pd.DataFrame(weights_history, columns=['Date'] + ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'])\n",
    "weights_df.to_csv('ppo_stage2_weights.csv', index=False)\n",
    "\n",
    "# Compute drifted daily returns and compare to equal weights\n",
    "# (similar to your existing evaluation code)\n",
    "def compute_returns(weights, price_df):\n",
    "    # Explicitly define price columns by removing \"Price_\" prefix\n",
    "    price_df.columns = [c.replace('Price_', '') for c in price_df.columns]\n",
    "\n",
    "    common = [c for c in weights.columns if c in price_df.columns]\n",
    "    if len(common) == 0:\n",
    "        raise ValueError(\"No common ETFs found between weights and prices DataFrames.\")\n",
    "\n",
    "    price_df = price_df[common]\n",
    "    daily_returns = price_df.pct_change().dropna()\n",
    "    weights = weights.set_index('Date')\n",
    "    start = weights.index.min()\n",
    "    end = weights.index.max() + timedelta(days=rebalance)\n",
    "    daily_returns = daily_returns.loc[start:end]\n",
    "\n",
    "    eq_weight = np.array([1/len(common)]*len(common))\n",
    "    drifted = pd.DataFrame(index=daily_returns.index, columns=common)\n",
    "    eq_drift = pd.DataFrame(index=daily_returns.index, columns=common)\n",
    "    cur_w = weights.iloc[0].values\n",
    "    cur_eq = eq_weight\n",
    "\n",
    "    returns_df = pd.DataFrame(index=daily_returns.index, columns=['RL', 'Equal'])\n",
    "\n",
    "    for d in daily_returns.index:\n",
    "        rets = daily_returns.loc[d]  # <-- Define this here explicitly every loop iteration\n",
    "\n",
    "        if d in weights.index:\n",
    "            cur_w = weights.loc[d].values\n",
    "            cur_eq = eq_weight\n",
    "        else:\n",
    "            cur_w = (cur_w * (1 + rets.values))\n",
    "            cur_w /= cur_w.sum()\n",
    "            cur_eq = (cur_eq * (1 + rets.values))\n",
    "            cur_eq /= cur_eq.sum()\n",
    "\n",
    "        drifted.loc[d] = cur_w\n",
    "        eq_drift.loc[d] = cur_eq\n",
    "\n",
    "        shifted_rl = drifted.shift(1).loc[d]\n",
    "        shifted_eq = eq_drift.shift(1).loc[d]\n",
    "\n",
    "        if d == daily_returns.index[0]:\n",
    "            returns_df.loc[d, 'RL'] = np.dot(cur_w, rets)\n",
    "            returns_df.loc[d, 'Equal'] = np.dot(cur_eq, rets)\n",
    "        else:\n",
    "            returns_df.loc[d, 'RL'] = np.dot(shifted_rl, rets)\n",
    "            returns_df.loc[d, 'Equal'] = np.dot(shifted_eq, rets)\n",
    "\n",
    "    return returns_df.dropna()\n",
    "\n",
    "# Compute test returns\n",
    "test_returns = compute_returns(weights_df, prices.set_index('Date'))\n",
    "cum_rl    = (1 + test_returns['RL']).prod() - 1\n",
    "cum_equal = (1 + test_returns['Equal']).prod() - 1\n",
    "print(f\"Outofsample cumulative return (RL):    {cum_rl:.4%}\")\n",
    "print(f\"Outofsample cumulative return (Equal): {cum_equal:.4%}\")\n",
    "\n",
    "# You can also compute annualised return, volatility, Sharpe ratio and max drawdown\n",
    "def performance_metrics(returns, freq=252):\n",
    "    ann_return = (1 + returns).prod()**(freq/len(returns)) - 1\n",
    "    ann_vol    = returns.std() * np.sqrt(freq)\n",
    "    sharpe     = ann_return / ann_vol if ann_vol != 0 else np.nan\n",
    "    cum_pnl    = (1+returns).cumprod()\n",
    "    max_dd     = (cum_pnl / cum_pnl.cummax() - 1).min()\n",
    "    return ann_return, ann_vol, sharpe, max_dd\n",
    "\n",
    "rl_ann, rl_vol, rl_sharpe, rl_dd = performance_metrics(test_returns['RL'])\n",
    "eq_ann, eq_vol, eq_sharpe, eq_dd = performance_metrics(test_returns['Equal'])\n",
    "print(f\"RL annualised return:    {rl_ann:.4%}, Sharpe: {rl_sharpe:.3f}, Max Drawdown: {rl_dd:.4%}\")\n",
    "print(f\"Equal annualised return: {eq_ann:.4%}, Sharpe: {eq_sharpe:.3f}, Max Drawdown: {eq_dd:.4%}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6947d247eaecd896",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Validation and early stopping (Step6)\n",
    "# -------------------------------------------------------------------\n",
    "# After training, evaluate on the validation set without updating parameters\n",
    "val_env = PortfolioEnv(val_scaled,\n",
    "                       ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                       reward_type='mean_cvar',\n",
    "                       risk_coefficient=1.0,\n",
    "                       rebalance_period=rebalance,\n",
    "                       lookback_period=lookback)\n",
    "obs, _ = val_env.reset(seed=SEED)\n",
    "done = False\n",
    "val_reward = 0.0\n",
    "while not done:\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, _ = val_env.step(action)\n",
    "    val_reward += reward\n",
    "print(f\"Total validation reward: {val_reward:.4f}\")\n",
    "\n",
    "# If necessary, you can adjust hyperparameters and retrain based on validation performance.\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Outofsample testing (20222024) and performance metrics (Step6)\n",
    "# -------------------------------------------------------------------\n",
    "test_env = PortfolioEnv(test_scaled,\n",
    "                        ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                        reward_type='mean_cvar',\n",
    "                        risk_coefficient=1.0,\n",
    "                        rebalance_period=rebalance,\n",
    "                        lookback_period=lookback)\n",
    "\n",
    "obs, _ = test_env.reset()\n",
    "done = False\n",
    "rebalance_dates = []\n",
    "weights_history = []\n",
    "while not done:\n",
    "    # produce an action every step; env will apply it only on rebalance dates\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    obs, _, done, _, _ = test_env.step(action)\n",
    "    # record weights at rebalance points\n",
    "    if test_env.current_step % rebalance == 0:\n",
    "        date = test_scaled.loc[test_env.current_step-1, 'Date']\n",
    "        weights_history.append([date] + test_env.current_weights.tolist())\n",
    "\n",
    "# Save monthly weights to CSV\n",
    "weights_df = pd.DataFrame(weights_history, columns=['Date'] + ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'])\n",
    "weights_df.to_csv('ppo_stage2_weights.csv', index=False)\n",
    "\n",
    "# Compute drifted daily returns and compare to equal weights\n",
    "# (similar to your existing evaluation code)\n",
    "def compute_returns(weights, price_df):\n",
    "    # Explicitly define price columns by removing \"Price_\" prefix\n",
    "    price_df.columns = [c.replace('Price_', '') for c in price_df.columns]\n",
    "\n",
    "    common = [c for c in weights.columns if c in price_df.columns]\n",
    "    if len(common) == 0:\n",
    "        raise ValueError(\"No common ETFs found between weights and prices DataFrames.\")\n",
    "\n",
    "    price_df = price_df[common]\n",
    "    daily_returns = price_df.pct_change().dropna()\n",
    "    weights = weights.set_index('Date')\n",
    "    start = weights.index.min()\n",
    "    end = weights.index.max() + timedelta(days=rebalance)\n",
    "    daily_returns = daily_returns.loc[start:end]\n",
    "\n",
    "    eq_weight = np.array([1/len(common)]*len(common))\n",
    "    drifted = pd.DataFrame(index=daily_returns.index, columns=common)\n",
    "    eq_drift = pd.DataFrame(index=daily_returns.index, columns=common)\n",
    "    cur_w = weights.iloc[0].values\n",
    "    cur_eq = eq_weight\n",
    "\n",
    "    returns_df = pd.DataFrame(index=daily_returns.index, columns=['RL', 'Equal'])\n",
    "\n",
    "    for d in daily_returns.index:\n",
    "        rets = daily_returns.loc[d]  # <-- Define this here explicitly every loop iteration\n",
    "\n",
    "        if d in weights.index:\n",
    "            cur_w = weights.loc[d].values\n",
    "            cur_eq = eq_weight\n",
    "        else:\n",
    "            cur_w = (cur_w * (1 + rets.values))\n",
    "            cur_w /= cur_w.sum()\n",
    "            cur_eq = (cur_eq * (1 + rets.values))\n",
    "            cur_eq /= cur_eq.sum()\n",
    "\n",
    "        drifted.loc[d] = cur_w\n",
    "        eq_drift.loc[d] = cur_eq\n",
    "\n",
    "        shifted_rl = drifted.shift(1).loc[d]\n",
    "        shifted_eq = eq_drift.shift(1).loc[d]\n",
    "\n",
    "        if d == daily_returns.index[0]:\n",
    "            returns_df.loc[d, 'RL'] = np.dot(cur_w, rets)\n",
    "            returns_df.loc[d, 'Equal'] = np.dot(cur_eq, rets)\n",
    "        else:\n",
    "            returns_df.loc[d, 'RL'] = np.dot(shifted_rl, rets)\n",
    "            returns_df.loc[d, 'Equal'] = np.dot(shifted_eq, rets)\n",
    "\n",
    "    return returns_df.dropna()\n",
    "\n",
    "# Compute test returns\n",
    "test_returns = compute_returns(weights_df, prices.set_index('Date'))\n",
    "cum_rl    = (1 + test_returns['RL']).prod() - 1\n",
    "cum_equal = (1 + test_returns['Equal']).prod() - 1\n",
    "print(f\"Outofsample cumulative return (RL):    {cum_rl:.4%}\")\n",
    "print(f\"Outofsample cumulative return (Equal): {cum_equal:.4%}\")\n",
    "\n",
    "# You can also compute annualised return, volatility, Sharpe ratio and max drawdown\n",
    "def performance_metrics(returns, freq=252):\n",
    "    ann_return = (1 + returns).prod()**(freq/len(returns)) - 1\n",
    "    ann_vol    = returns.std() * np.sqrt(freq)\n",
    "    sharpe     = ann_return / ann_vol if ann_vol != 0 else np.nan\n",
    "    cum_pnl    = (1+returns).cumprod()\n",
    "    max_dd     = (cum_pnl / cum_pnl.cummax() - 1).min()\n",
    "    return ann_return, ann_vol, sharpe, max_dd\n",
    "\n",
    "rl_ann, rl_vol, rl_sharpe, rl_dd = performance_metrics(test_returns['RL'])\n",
    "eq_ann, eq_vol, eq_sharpe, eq_dd = performance_metrics(test_returns['Equal'])\n",
    "print(f\"RL annualised return:    {rl_ann:.4%}, Sharpe: {rl_sharpe:.3f}, Max Drawdown: {rl_dd:.4%}\")\n",
    "print(f\"Equal annualised return: {eq_ann:.4%}, Sharpe: {eq_sharpe:.3f}, Max Drawdown: {eq_dd:.4%}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3462ab258866c09e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data explicitly\n",
    "port_wts = pd.read_csv('ppo_allocations_multiple_iterations_DIA_ETF.csv', parse_dates=['Date'], index_col='Date')\n",
    "daily_returns = pd.read_csv('daily_returns_10ETFs.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "common_tickers = [col for col in port_wts.columns if col in daily_returns.columns]\n",
    "daily_returns = daily_returns[common_tickers]\n",
    "\n",
    "# Explicitly filter daily returns to the date range covered by portfolio weights\n",
    "start_date, end_date = port_wts.index.min(), port_wts.index.max() + pd.Timedelta(days=21)\n",
    "daily_returns = daily_returns.loc[start_date:end_date]\n",
    "\n",
    "# Initialize drifted weights with the first available rebalance weights\n",
    "initial_weights = port_wts.loc[start_date].values\n",
    "\n",
    "equal_weight = np.array([1.0 / len(common_tickers)] * len(common_tickers))\n",
    "\n",
    "# Create drifted weights DataFrame explicitly initialized\n",
    "drifted_weights = pd.DataFrame(index=daily_returns.index, columns=common_tickers)\n",
    "equal_weights = pd.DataFrame(index=daily_returns.index, columns=common_tickers)\n",
    "\n",
    "current_weights = initial_weights\n",
    "current_equal_weights = equal_weight\n",
    "\n",
    "# Initialize returns DataFrame explicitly\n",
    "returns_df = pd.DataFrame(index=daily_returns.index, columns=['Optimal_Portfolio_Return', 'Equal_Weight_Return'])\n",
    "\n",
    "for current_date in daily_returns.index:\n",
    "    if current_date in port_wts.index:\n",
    "        # Explicit rebalance date: assign new weights\n",
    "        current_weights = port_wts.loc[current_date].values\n",
    "        current_equal_weights = equal_weight\n",
    "    else:\n",
    "        # Explicitly drift weights using previous day's return\n",
    "        prev_day_return = daily_returns.loc[current_date]\n",
    "\n",
    "        drifted_wts_numerator = current_weights * (1 + prev_day_return.values)\n",
    "        current_weights = drifted_wts_numerator / np.sum(drifted_wts_numerator)\n",
    "\n",
    "        equal_drifted_numerator = current_equal_weights * (1 + prev_day_return.values)\n",
    "        current_equal_weights = equal_drifted_numerator / np.sum(equal_drifted_numerator)\n",
    "\n",
    "    drifted_weights.loc[current_date] = current_weights\n",
    "    equal_weights.loc[current_date] = current_equal_weights\n",
    "    shifted_drifted_weights = drifted_weights.shift(1)\n",
    "    shifted_equal_weights = equal_weights.shift(1)\n",
    "    if current_date == daily_returns.index[0]:\n",
    "        # On the first day, use initial weights directly\n",
    "        returns_df.loc[current_date, 'Optimal_Portfolio_Return'] = np.dot(\n",
    "            drifted_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "        returns_df.loc[current_date, 'Equal_Weight_Return'] = np.dot(\n",
    "            equal_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "    else:\n",
    "        # Explicitly use previous day's weights\n",
    "        returns_df.loc[current_date, 'Optimal_Portfolio_Return'] = np.dot(\n",
    "            shifted_drifted_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "        returns_df.loc[current_date, 'Equal_Weight_Return'] = np.dot(\n",
    "            shifted_equal_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "\n",
    "# Check explicitly\n",
    "print(\"Drifted weights (head):\\n\", drifted_weights.head())\n",
    "print(\"\\nPortfolio returns (head):\\n\", returns_df.head())\n",
    "\n",
    "# Save explicitly\n",
    "drifted_weights.to_csv('drifted_weights_corrected.csv')\n",
    "equal_weights.to_csv('equal_weights.csv')\n",
    "returns_df.to_csv('portfolio_returns_combined.csv')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9e17792fa6118e4",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
