{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data (as before)\n",
    "shap_metrics = pd.read_csv('shap_value_metrics_export.csv', parse_dates=['End_Date'])\n",
    "factor_returns = pd.read_csv('aligned_factors.csv', parse_dates=['Date'], index_col='Date')\n",
    "stock_returns = pd.read_csv('daily_returns.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# Select relevant SHAP features for regime clustering\n",
    "shap_features = shap_metrics[[\n",
    "    'mean_abs_shap_Mkt-RF', 'mean_abs_shap_SMB', 'mean_abs_shap_HML',\n",
    "    'mean_abs_shap_RMW', 'mean_abs_shap_CMA',\n",
    "    'shap_std_Mkt-RF', 'shap_std_SMB', 'shap_std_HML',\n",
    "    'shap_std_RMW', 'shap_std_CMA'\n",
    "]]\n",
    "\n",
    "# Normalize SHAP metrics using StandardScaler (mean=0, std=1)\n",
    "scaler_shap = StandardScaler()\n",
    "shap_features_normalized = scaler_shap.fit_transform(shap_features)\n",
    "\n",
    "# Save normalized SHAP metrics back to a DataFrame\n",
    "shap_features_norm_df = pd.DataFrame(\n",
    "    shap_features_normalized,\n",
    "    columns=shap_features.columns,\n",
    "    index=shap_metrics.index\n",
    ")\n",
    "\n",
    "# Include 'End_Date' and 'Stock' columns for reference\n",
    "shap_features_norm_df['End_Date'] = shap_metrics['End_Date']\n",
    "shap_features_norm_df['Stock'] = shap_metrics['Stock']\n",
    "\n",
    "# Drop non-numeric columns explicitly before aggregation\n",
    "numeric_columns = shap_features_norm_df.columns.drop(['End_Date', 'Stock'])\n",
    "\n",
    "# Aggregated SHAP regime-level features (average over DJ30 stocks per date)\n",
    "regime_features_norm = shap_features_norm_df.groupby('End_Date')[numeric_columns].mean().dropna()\n",
    "\n",
    "# Inspect aggregated and normalized regime-level SHAP features\n",
    "print(regime_features_norm.head())\n",
    "\n",
    "# Normalize factor returns similarly for PPO input\n",
    "factor_scaler = StandardScaler()\n",
    "factor_returns_norm = pd.DataFrame(\n",
    "    factor_scaler.fit_transform(factor_returns),\n",
    "    index=factor_returns.index,\n",
    "    columns=factor_returns.columns\n",
    ")\n",
    "\n",
    "# Normalize stock returns for PPO environment\n",
    "stock_scaler = StandardScaler()\n",
    "stock_returns_norm = pd.DataFrame(\n",
    "    stock_scaler.fit_transform(stock_returns),\n",
    "    index=stock_returns.index,\n",
    "    columns=stock_returns.columns\n",
    ")\n",
    "\n",
    "# Inspect normalized data\n",
    "print(\"Normalized SHAP features:\", regime_features_norm.head())\n",
    "print(\"Normalized factor returns:\", factor_returns_norm.head())\n",
    "print(\"Normalized stock returns:\", stock_returns_norm.head())\n",
    "regime_features_norm.to_csv(\"regime_features_norm.csv\")\n",
    "factor_returns_norm.to_csv(\"factor_returns_norm.csv\")\n",
    "stock_returns_norm.to_csv(\"stock_returns_norm.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c807fd6acd5211a2"
  },
  {
   "cell_type": "raw",
   "source": [
    "# Implement t-SNE and clustering to detect market regimes:\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# t-SNE Dimension reduction\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_result = tsne.fit_transform(regime_features_norm)\n",
    "\n",
    "# Gaussian Mixture clustering (or KMeans as simpler alternative)\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "clusters = gmm.fit_predict(tsne_result)\n",
    "\n",
    "regime_features_norm['Regime'] = clusters\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(tsne_result[:,0], tsne_result[:,1], c=clusters, cmap='viridis')\n",
    "plt.title(\"t-SNE SHAP Regime Clustering\")\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.colorbar(label='Regime')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acee54455ebb1102"
  },
  {
   "cell_type": "raw",
   "source": [
    "1. Single-Step PPO: \"SinglePPOEnv\"\n",
    "\n",
    "Environment definition (rename explicitly):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81abba305046567"
  },
  {
   "cell_type": "raw",
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "class SinglePPOEnv(gym.Env):\n",
    "    def __init__(self, stock_returns_norm, regime_features_norm):\n",
    "        super().__init__()\n",
    "        self.stock_returns = stock_returns_norm.values\n",
    "        self.regime_features = regime_features_norm['Regime'].values\n",
    "        self.current_step = 0\n",
    "        self.n_assets = self.stock_returns.shape[1]\n",
    "\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_assets + 1,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        returns = self.stock_returns[self.current_step]\n",
    "        regime = self.regime_features[self.current_step]\n",
    "        obs = np.concatenate([returns, [regime]])\n",
    "        obs = np.nan_to_num(obs, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        action_sum = action.sum()\n",
    "        if action_sum == 0:\n",
    "            action = np.ones_like(action) / len(action)\n",
    "        else:\n",
    "            action /= action_sum\n",
    "\n",
    "        reward = np.dot(action, self.stock_returns[self.current_step + 1])\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Explicit fix to ensure exactly 63 observations\n",
    "        done = self.current_step >= len(self.stock_returns) - 1\n",
    "\n",
    "        # Adjust observation to always return valid step explicitly\n",
    "        if not done:\n",
    "            obs = self._get_obs()\n",
    "        else:\n",
    "            obs = np.zeros(self.observation_space.shape)\n",
    "\n",
    "        if np.isnan(reward) or np.isinf(reward):\n",
    "            reward = 0.0\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b684f58e04c996c2"
  },
  {
   "cell_type": "raw",
   "source": [
    "Rolling Training and Out-of-Sample Testing (Single-Step PPO):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b88d84e25cf2eeea"
  },
  {
   "cell_type": "raw",
   "source": [
    "single_step_records = []\n",
    "\n",
    "train_window, test_window = 252, 63\n",
    "tickers = stock_returns_norm.columns.tolist()\n",
    "total_windows = len(range(0, len(stock_returns_norm) - train_window - test_window, test_window))\n",
    "\n",
    "for i, start in enumerate(range(0, len(stock_returns_norm) - train_window - test_window, test_window)):\n",
    "    \n",
    "    print(f\"\\nStarting training window {i+1}/{total_windows}...\")\n",
    "    \n",
    "    train_returns_df = stock_returns_norm.iloc[start : start + train_window].ffill()\n",
    "    test_returns_df = stock_returns_norm.iloc[start + train_window : start + train_window + test_window + 1].ffill()\n",
    "\n",
    "    train_regimes_df = regime_features_norm.iloc[start : start + train_window].ffill()\n",
    "    test_regimes_df = regime_features_norm.iloc[start + train_window : start + train_window + test_window + 1].ffill()\n",
    "\n",
    "    env = DummyVecEnv([lambda: SinglePPOEnv(train_returns_df, train_regimes_df)])\n",
    "    model = PPO('MlpPolicy', env, verbose=0)\n",
    "\n",
    "    print(\"Training PPO model...\")\n",
    "    model.learn(total_timesteps=10000)\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    env_test = SinglePPOEnv(test_returns_df, test_regimes_df)\n",
    "    obs = env_test.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step_idx = 0\n",
    "\n",
    "    test_dates = test_returns_df.index[:-1].tolist()  # explicitly matching 63 steps\n",
    "\n",
    "    print(\"Starting out-of-sample evaluation...\")\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = env_test.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        single_step_records.append({\n",
    "            'Date': test_dates[step_idx],\n",
    "            'Window': i,\n",
    "            'Step': step_idx,\n",
    "            'Reward': reward,\n",
    "            **{ticker: weight for ticker, weight in zip(tickers, action)}\n",
    "        })\n",
    "\n",
    "        step_idx += 1\n",
    "\n",
    "    print(f\"Window {i+1}/{total_windows} completed. Total Steps: {step_idx}, Total Out-of-Sample Reward: {total_reward:.4f}\")\n",
    "\n",
    "single_df = pd.DataFrame(single_step_records)\n",
    "single_df['Cumulative_Return'] = single_df['Reward'].cumsum()\n",
    "\n",
    "single_df.to_csv('single_step_allocations_with_dates.csv', index=False)\n",
    "\n",
    "single_pnl_df = single_df[['Date', 'Cumulative_Return']]\n",
    "single_pnl_df.to_csv('single_step_pnl_with_dates.csv', index=False)\n",
    "\n",
    "print(\"\\nAll training windows completed successfully!\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e6c5c167b729846"
  },
  {
   "cell_type": "raw",
   "source": [
    "New: 1. Single-Step PPO: \"SinglePPOEnv\"\n",
    "\n",
    "Environment definition (rename explicitly):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6597bad74c5aa6c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data explicitly\n",
    "shap_metrics = pd.read_csv('shap_value_metrics_export.csv', parse_dates=['Start_Date', 'End_Date'])\n",
    "factor_returns = pd.read_csv('aligned_factors.csv', parse_dates=['Date'], index_col='Date')\n",
    "stock_returns = pd.read_csv('daily_returns.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# Explicitly filter only 'Test' phase SHAP metrics\n",
    "shap_metrics_test = shap_metrics[shap_metrics['Phase'] == 'Test'].copy()\n",
    "\n",
    "# Verify explicitly\n",
    "print(f\"Total SHAP records (Test Phase): {len(shap_metrics_test)}\")\n",
    "\n",
    "# Select SHAP metrics explicitly\n",
    "selected_columns = [\n",
    "    'mean_abs_shap_Mkt-RF', 'mean_abs_shap_SMB', 'mean_abs_shap_HML',\n",
    "    'mean_abs_shap_RMW', 'mean_abs_shap_CMA',\n",
    "    'shap_std_Mkt-RF', 'shap_std_SMB', 'shap_std_HML',\n",
    "    'shap_std_RMW', 'shap_std_CMA',\n",
    "    'mean_abs_over_std_Mkt-RF', 'mean_abs_over_std_SMB', 'mean_abs_over_std_HML',\n",
    "    'mean_abs_over_std_RMW', 'mean_abs_over_std_CMA'\n",
    "]\n",
    "\n",
    "# Aggregate SHAP metrics explicitly by Start_Date and End_Date across stocks\n",
    "shap_agg = shap_metrics_test.groupby(['Start_Date', 'End_Date'])[selected_columns].mean().reset_index()\n",
    "\n",
    "# Explicitly scale SHAP metrics\n",
    "scaler_shap = StandardScaler()\n",
    "shap_agg[selected_columns] = scaler_shap.fit_transform(shap_agg[selected_columns])\n",
    "\n",
    "# Verify explicitly scaled SHAP metrics\n",
    "print(\"Scaled SHAP metrics (Test phase):\", shap_agg.head())\n",
    "\n",
    "# Explicitly map SHAP intervals to daily returns\n",
    "daily_regimes = pd.DataFrame(index=stock_returns.index)\n",
    "\n",
    "for _, row in shap_agg.iterrows():\n",
    "    mask = (daily_regimes.index >= row['Start_Date']) & (daily_regimes.index <= row['End_Date'])\n",
    "    daily_regimes.loc[mask, selected_columns] = row[selected_columns].values\n",
    "\n",
    "# Explicit forward-fill for continuity\n",
    "daily_regimes.ffill(inplace=True)\n",
    "daily_regimes.dropna(inplace=True)\n",
    "\n",
    "# Verify explicitly aligned daily regimes\n",
    "print(\"Daily regimes (aligned):\", daily_regimes.head())\n",
    "\n",
    "# Normalize explicitly factor returns\n",
    "factor_scaler = StandardScaler()\n",
    "factor_returns_norm = pd.DataFrame(\n",
    "    factor_scaler.fit_transform(factor_returns),\n",
    "    index=factor_returns.index,\n",
    "    columns=factor_returns.columns\n",
    ")\n",
    "\n",
    "# Normalize explicitly stock returns\n",
    "stock_scaler = StandardScaler()\n",
    "stock_returns_norm = pd.DataFrame(\n",
    "    stock_scaler.fit_transform(stock_returns),\n",
    "    index=stock_returns.index,\n",
    "    columns=stock_returns.columns\n",
    ")\n",
    "\n",
    "# Merge explicitly into final dataset\n",
    "merged_data = stock_returns_norm.join(daily_regimes, how='inner').join(factor_returns_norm, rsuffix='_factor').dropna()\n",
    "\n",
    "# Explicit final dataset verification\n",
    "print(\"Merged normalized data preview:\", merged_data.head())\n",
    "\n",
    "# Save explicitly final aligned dataset\n",
    "merged_data.to_csv(\"final_merged_data.csv\")\n",
    "\n",
    "start_date = merged_data.index.min()\n",
    "end_date = merged_data.index.max()\n",
    "\n",
    "# Explicitly filter raw stock returns to match the merged_data dates\n",
    "raw_stock_returns = stock_returns.loc[start_date:end_date].copy()\n",
    "raw_stock_returns.to_csv(\"aligned_raw_stock_returns.csv\")\n",
    "\n",
    "stock_returns_norm_filter = stock_returns_norm.loc[start_date:end_date].copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50052cf6eee1b086",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class SinglePPOEnv(gym.Env):\n",
    "    def __init__(self, returns_df, regime_df, raw_returns_df):\n",
    "        super().__init__()\n",
    "        self.stock_returns = returns_df.values\n",
    "        self.regime_features = regime_df.values\n",
    "        self.raw_stock_returns = raw_returns_df.values\n",
    "        self.current_step = 0\n",
    "        self.n_assets = self.stock_returns.shape[1]\n",
    "\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "\n",
    "        # Adjust observation space shape accordingly\n",
    "        obs_dim = self.n_assets + self.regime_features.shape[1]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(obs_dim,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        stock_returns = self.stock_returns[self.current_step]\n",
    "        regime = self.regime_features[self.current_step]\n",
    "\n",
    "        obs = np.concatenate([stock_returns, regime])\n",
    "        return np.nan_to_num(obs)\n",
    "\n",
    "    def step(self, action):\n",
    "        action_sum = action.sum()\n",
    "        action = action / action_sum if action_sum != 0 else np.ones_like(action) / len(action)\n",
    "\n",
    "        # Reward calculated explicitly using raw returns\n",
    "        reward = np.dot(action, self.raw_stock_returns[self.current_step + 1])\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.stock_returns) - 1\n",
    "        obs = self._get_obs() if not done else np.zeros(self.observation_space.shape)\n",
    "        reward = 0.0 if np.isnan(reward) else reward\n",
    "        return obs, reward, done, {}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16986ba29a0650f7",
   "execution_count": null
  },
  {
   "cell_type": "raw",
   "source": [
    "# reb every day\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "train_window, test_window = 252, 63\n",
    "tickers = stock_returns_norm.columns.tolist()\n",
    "records = []\n",
    "\n",
    "total_windows = len(range(0, len(merged_data) - train_window - test_window, test_window))\n",
    "\n",
    "for i, start in enumerate(range(0, len(merged_data) - train_window - test_window, test_window)):\n",
    "    print(f\"\\nStarting Window {i+1}/{total_windows}\")\n",
    "\n",
    "    train_df = merged_data.iloc[start:start+train_window]\n",
    "    test_df = merged_data.iloc[start+train_window:start+train_window+test_window+1]\n",
    "\n",
    "    env = DummyVecEnv([\n",
    "        lambda: SinglePPOEnv(train_df[tickers], train_df[selected_columns])\n",
    "    ])\n",
    "\n",
    "    model = PPO('MlpPolicy', env, verbose=0)\n",
    "    print(\"Training PPO model...\")\n",
    "    model.learn(total_timesteps=10000)\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    print(\"Starting out-of-sample evaluation...\")\n",
    "    env_test = SinglePPOEnv(test_df[tickers], test_df[selected_columns])\n",
    "    obs = env_test.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step_idx = 0\n",
    "    test_dates = test_df.index[:-1]\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = env_test.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        records.append({\n",
    "            'Date': test_dates[step_idx],\n",
    "            'Window': i,\n",
    "            'Step': step_idx,\n",
    "            'Reward': reward,\n",
    "            **{ticker: weight for ticker, weight in zip(tickers, action)}\n",
    "        })\n",
    "\n",
    "        step_idx += 1\n",
    "\n",
    "    # print(f\"Window {i+1} completed, Steps: {step_idx}\")\n",
    "    print(f\"Window {i+1}/{total_windows} completed. Total Steps: {step_idx}, Total Out-of-Sample Reward: {total_reward:.4f}\")\n",
    "\n",
    "# Save explicitly final results\n",
    "result_df = pd.DataFrame(records)\n",
    "result_df['Cumulative_Return'] = result_df['Reward'].cumsum()\n",
    "result_df.to_csv('single_step_allocations_dates.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"All windows completed successfully.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "456e4f509a40f0be"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# reb every 21 days\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model_save_dir = 'E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/trained_single_ppo/'\n",
    "\n",
    "train_window, test_window = 252, 63\n",
    "rebalance_freq = 21  # rebalance every 21 days\n",
    "tickers = stock_returns_norm.columns.tolist()\n",
    "selected_columns = daily_regimes.columns.tolist()\n",
    "factor_columns = factor_returns_norm.columns.tolist()\n",
    "use_factor_returns = False\n",
    "records = []\n",
    "raw_stock_returns = stock_returns_norm_filter\n",
    "load_existing_model = False\n",
    "\n",
    "total_windows = len(range(0, len(merged_data) - train_window - test_window, test_window))\n",
    "\n",
    "for i, start in enumerate(range(0, len(merged_data) - train_window - test_window, test_window)):\n",
    "    print(f\"\\n{'='*30}\\nStarting Training Window {i+1}/{total_windows}\")\n",
    "    print(f\"Training Period: {merged_data.index[start].date()} to {merged_data.index[start + train_window - 1].date()}\")\n",
    "\n",
    "    # Explicit training and testing dataset slicing\n",
    "    train_df = merged_data.iloc[start:start + train_window]\n",
    "    test_df = merged_data.iloc[start + train_window:start + train_window + test_window + 1]\n",
    "    raw_returns_train = raw_stock_returns.loc[train_df.index]\n",
    "    raw_returns_test = raw_stock_returns.loc[test_df.index]\n",
    "\n",
    "    if train_df.isna().values.any() or test_df.isna().values.any():\n",
    "        print(f\"Skipping window {i+1} due to NaNs.\")\n",
    "        continue\n",
    "\n",
    "    # Train PPO model explicitly\n",
    "    model_filename = os.path.join(model_save_dir, f\"ppo_model_window_{i}.zip\")\n",
    "\n",
    "    # Setup environment\n",
    "    env = DummyVecEnv([\n",
    "        lambda: SinglePPOEnv(\n",
    "            train_df[tickers],\n",
    "            train_df[selected_columns + factor_columns] if use_factor_returns else train_df[selected_columns],\n",
    "            raw_returns_train\n",
    "        )\n",
    "    ])\n",
    "\n",
    "\n",
    "     # Load existing model if indicated and file exists\n",
    "    if load_existing_model and i > 0 and os.path.exists(os.path.join(model_save_dir, f\"ppo_model_window_{i-1}.zip\")):\n",
    "        previous_model_filename = os.path.join(model_save_dir, f\"ppo_model_window_{i-1}.zip\")\n",
    "        print(f\"Loading and retraining previous PPO model from {previous_model_filename}\")\n",
    "        model = PPO.load(previous_model_filename, env=env)\n",
    "        model.learn(total_timesteps=10000)\n",
    "    else:\n",
    "        print(\"Training new PPO model from scratch...\")\n",
    "        model = PPO('MlpPolicy', env, verbose=0)\n",
    "        model.learn(total_timesteps=10000)\n",
    "\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "    # Testing phase setup\n",
    "    test_dates = test_df.index\n",
    "    test_returns = test_df[tickers].values\n",
    "    test_regimes = test_df[selected_columns + factor_columns].values if use_factor_returns else test_df[selected_columns].values\n",
    "\n",
    "    print(f\"\\nTesting Period: {test_dates[0].date()} to {test_dates[-1].date()} (63 days total)\")\n",
    "\n",
    "    # Evaluate every 21 days (3 rebalances per testing window)\n",
    "    for rebalance_num, rebalance_start in enumerate(range(0, test_window, rebalance_freq)):\n",
    "        rebalance_end = min(rebalance_start + rebalance_freq, test_window)\n",
    "\n",
    "        print(f\"\\n--- Rebalance {rebalance_num + 1} ---\")\n",
    "        print(f\"Rebalance date: {test_dates[rebalance_start].date()}\")\n",
    "        print(f\"Holding period: {test_dates[rebalance_start].date()} to {test_dates[rebalance_end - 1].date()}\")\n",
    "\n",
    "        obs_returns = test_returns[rebalance_start:rebalance_end]\n",
    "        obs_regimes = test_regimes[rebalance_start:rebalance_end]\n",
    "        obs_raw_returns = raw_returns_test.iloc[rebalance_start:rebalance_end]\n",
    "\n",
    "        env_test = SinglePPOEnv(\n",
    "            returns_df=pd.DataFrame(obs_returns, columns=tickers),\n",
    "            regime_df=pd.DataFrame(obs_regimes, columns=(selected_columns + factor_columns) if use_factor_returns else selected_columns),\n",
    "            raw_returns_df=obs_raw_returns\n",
    "        )\n",
    "\n",
    "        obs = env_test.reset()\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "        # Explicit normalization right here:\n",
    "        action_sum = action.sum()\n",
    "        if action_sum != 0:\n",
    "            action /= action_sum\n",
    "        else:\n",
    "            action = np.ones_like(action) / len(action)\n",
    "\n",
    "        weights_dict = {ticker: round(weight, 4) for ticker, weight in zip(tickers, action)}\n",
    "        print(f\"Normalized Portfolio weights:\\n{weights_dict}\")\n",
    "\n",
    "        portfolio_value = 1.0\n",
    "        for step in range(rebalance_end - rebalance_start - 1):\n",
    "            reward = np.dot(action, obs_raw_returns.iloc[step + 1])\n",
    "            portfolio_value *= (1 + reward)\n",
    "        cumulative_reward = portfolio_value - 1.0\n",
    "\n",
    "        records.append({\n",
    "            'Rebalance_Date': test_dates[rebalance_start],\n",
    "            'Window': i,\n",
    "            'Rebalance_Number': rebalance_num,\n",
    "            'Holding_Period_Start': test_dates[rebalance_start],\n",
    "            'Holding_Period_End': test_dates[rebalance_end - 1],\n",
    "            'Cumulative_Reward': cumulative_reward,\n",
    "            **weights_dict\n",
    "        })\n",
    "\n",
    "        print(f\"Cumulative reward for holding period: {cumulative_reward:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\nAll training/testing windows completed successfully.\\n\" + \"=\"*30)\n",
    "\n",
    "# Save final results explicitly\n",
    "result_df = pd.DataFrame(records)\n",
    "result_df['Cumulative_Return'] = result_df['Cumulative_Reward'].cumsum()\n",
    "result_df.to_csv('single_step_allocations_dates_rebalanced.csv', index=False)\n",
    "\n",
    "print(\"Final results saved to 'single_step_allocations_dates_rebalanced.csv'\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f741ad7b986bb6e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*30 + \"\\nAll training/testing windows completed successfully.\\n\" + \"=\"*30)\n",
    "\n",
    "# Save final results explicitly\n",
    "result_df = pd.DataFrame(records)\n",
    "result_df['Cumulative_Return'] = result_df['Cumulative_Reward'].cumsum()\n",
    "result_df.to_csv('single_step_allocations_dates_rebalanced.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff36318ecdaef1db",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ea46a06e07568b2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2f1dd301757f4969"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "# Load data explicitly\n",
    "shap_metrics = pd.read_csv('shap_value_metrics_export.csv', parse_dates=['Start_Date', 'End_Date'])\n",
    "factor_returns = pd.read_csv('aligned_factors.csv', parse_dates=['Date'], index_col='Date')\n",
    "stock_returns = pd.read_csv('daily_returns.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# Explicitly filter only 'Test' phase SHAP metrics\n",
    "shap_metrics_test = shap_metrics[shap_metrics['Phase'] == 'Test'].copy()\n",
    "\n",
    "# Verify explicitly\n",
    "print(f\"Total SHAP records (Test Phase): {len(shap_metrics_test)}\")\n",
    "\n",
    "# Select SHAP metrics explicitly\n",
    "selected_columns = [\n",
    "    'mean_abs_shap_Mkt-RF', 'mean_abs_shap_SMB', 'mean_abs_shap_HML',\n",
    "    'mean_abs_shap_RMW', 'mean_abs_shap_CMA',\n",
    "    'shap_std_Mkt-RF', 'shap_std_SMB', 'shap_std_HML',\n",
    "    'shap_std_RMW', 'shap_std_CMA',\n",
    "    'mean_abs_over_std_Mkt-RF', 'mean_abs_over_std_SMB', 'mean_abs_over_std_HML',\n",
    "    'mean_abs_over_std_RMW', 'mean_abs_over_std_CMA'\n",
    "]\n",
    "\n",
    "# Aggregate SHAP metrics explicitly by Start_Date and End_Date across stocks\n",
    "shap_agg = shap_metrics_test.groupby(['Start_Date', 'End_Date'])[selected_columns].mean().reset_index()\n",
    "\n",
    "# Explicitly scale SHAP metrics\n",
    "scaler_shap = StandardScaler()\n",
    "shap_agg[selected_columns] = scaler_shap.fit_transform(shap_agg[selected_columns])\n",
    "\n",
    "# Verify explicitly scaled SHAP metrics\n",
    "# print(\"Scaled SHAP metrics (Test phase):\", shap_agg.head())\n",
    "\n",
    "# Explicitly map SHAP intervals to daily returns\n",
    "daily_regimes = pd.DataFrame(index=stock_returns.index)\n",
    "\n",
    "for _, row in shap_agg.iterrows():\n",
    "    mask = (daily_regimes.index >= row['Start_Date']) & (daily_regimes.index <= row['End_Date'])\n",
    "    daily_regimes.loc[mask, selected_columns] = row[selected_columns].values\n",
    "\n",
    "# Explicit forward-fill for continuity\n",
    "daily_regimes.ffill(inplace=True)\n",
    "daily_regimes.dropna(inplace=True)\n",
    "\n",
    "# Perform KMeans clustering explicitly on daily regimes\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "daily_regimes['Cluster'] = kmeans.fit_predict(daily_regimes[selected_columns])\n",
    "# daily_regimes['Cluster'] = 1 - daily_regimes['Cluster']\n",
    "# cluster_mapping = {0: 1, 1: 0}  # Good explicitly set as 1\n",
    "# daily_regimes['Cluster_Mapped'] = daily_regimes['Cluster'].map(cluster_mapping)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Example: Clustering raw returns into regimes\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "# regime_labels = kmeans.fit_predict(raw_future_returns_df)\n",
    "# regime_labels_series = pd.Series(regime_labels, index=raw_future_returns_df.index)\n",
    "\n",
    "# Verify explicitly aligned daily regimes\n",
    "# print(\"Daily regimes (aligned):\", daily_regimes.head())\n",
    "\n",
    "# Normalize explicitly factor returns\n",
    "factor_scaler = StandardScaler()\n",
    "factor_returns_norm = pd.DataFrame(\n",
    "    factor_scaler.fit_transform(factor_returns),\n",
    "    index=factor_returns.index,\n",
    "    columns=factor_returns.columns\n",
    ")\n",
    "\n",
    "# Normalize explicitly stock returns\n",
    "stock_scaler = StandardScaler()\n",
    "stock_returns_norm = pd.DataFrame(\n",
    "    stock_scaler.fit_transform(stock_returns),\n",
    "    index=stock_returns.index,\n",
    "    columns=stock_returns.columns\n",
    ")\n",
    "\n",
    "# Merge explicitly into final dataset\n",
    "merged_data = stock_returns_norm.join(daily_regimes[['Cluster'] + selected_columns], how='inner')\\\n",
    "                                .join(factor_returns_norm, rsuffix='_factor')\\\n",
    "                                .dropna()\n",
    "\n",
    "# Explicit final dataset verification\n",
    "# print(\"Merged normalized data preview:\", merged_data.head())\n",
    "\n",
    "# Save explicitly final aligned dataset\n",
    "merged_data.to_csv(\"final_merged_data.csv\")\n",
    "# Explicitly filter raw stock returns to match the merged_data dates\n",
    "start_date = merged_data.index.min()\n",
    "end_date = merged_data.index.max()\n",
    "\n",
    "# Explicitly filter raw stock returns to match the merged_data dates\n",
    "raw_stock_returns = stock_returns.loc[start_date:end_date].copy()\n",
    "raw_stock_returns.to_csv(\"aligned_raw_stock_returns.csv\")\n",
    "\n",
    "stock_returns_norm_filter = stock_returns_norm.loc[start_date:end_date].copy()\n",
    "cluster_avg_returns = raw_stock_returns.join(daily_regimes['Cluster']).groupby('Cluster').mean().mean(axis=1)\n",
    "print(\"Explicit Cluster Mean Returns Check:\\n\", cluster_avg_returns)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c0027f46cdbabce",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use existing explicitly scaled SHAP metrics from daily_regimes\n",
    "regime_data = daily_regimes.values\n",
    "cluster_range = range(2, 11)  # Evaluate 2 to 10 clusters explicitly\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "calinski_scores = []\n",
    "\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(regime_data)\n",
    "\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(regime_data, labels))\n",
    "    calinski_scores.append(calinski_harabasz_score(regime_data, labels))\n",
    "\n",
    "    print(f\"Clusters: {k} | Inertia: {kmeans.inertia_:.2f} | \"\n",
    "          f\"Silhouette: {silhouette_scores[-1]:.4f} | \"\n",
    "          f\"Calinski-Harabasz: {calinski_scores[-1]:.2f}\")\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "ax[0].plot(cluster_range, inertia, 'bo-', linewidth=2)\n",
    "ax[0].set_title('Elbow Method (Inertia)', fontsize=14)\n",
    "ax[0].set_xlabel('Number of Clusters', fontsize=12)\n",
    "ax[0].set_ylabel('Inertia', fontsize=12)\n",
    "\n",
    "ax[1].plot(cluster_range, silhouette_scores, 'go-', linewidth=2)\n",
    "ax[1].set_title('Silhouette Score', fontsize=14)\n",
    "ax[1].set_xlabel('Number of Clusters', fontsize=12)\n",
    "ax[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "\n",
    "ax[2].plot(cluster_range, calinski_scores, 'ro-', linewidth=2)\n",
    "ax[2].set_title('Calinski-Harabasz Index', fontsize=14)\n",
    "ax[2].set_xlabel('Number of Clusters', fontsize=12)\n",
    "ax[2].set_ylabel('Calinski-Harabasz Score', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5e18248d57def17",
   "execution_count": null
  },
  {
   "cell_type": "raw",
   "source": [
    "# daily_regimes['Cluster'] = labels\n",
    "# returns_by_cluster = stock_returns_norm.join(daily_regimes['Cluster'])\n",
    "# # print(returns_by_cluster.groupby('Cluster').mean())\n",
    "# \n",
    "# print(returns_by_cluster.groupby('Cluster').std())\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "daily_regimes['Cluster'] = kmeans.fit_predict(daily_regimes[selected_columns])\n",
    "# Do NOT include 'Cluster' in regime_columns explicitly, because that's a label not a feature:\n",
    "regime_columns = selected_columns.copy()\n",
    "# Explicit join:\n",
    "merged_data = stock_returns_norm.join(\n",
    "    daily_regimes[['Cluster'] + regime_columns], \n",
    "    how='inner'\n",
    ").dropna()\n",
    "\n",
    "# Verify explicitly:\n",
    "print(merged_data.head())\n",
    "# High-Level environment explicitly using only regime_columns (SHAP metrics):\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50eec2710513c348"
  },
  {
   "cell_type": "raw",
   "source": [
    "Step 1: High-Level PPO (Regime Selector)\n",
    "\n",
    "High-Level Regime Selector Environment:\n",
    "Step 2: Low-Level PPO (Portfolio Allocator)\n",
    "\n",
    "Regime-Conditioned Portfolio Environment:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5792785ff137338f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class RegimeSelectorEnv(gym.Env):\n",
    "    def __init__(self, regime_features_df, raw_future_returns_df):\n",
    "        super().__init__()\n",
    "        self.regime_features = regime_features_df.values\n",
    "        # self.raw_future_returns = raw_future_returns_df.values\n",
    "        self.raw_future_returns = raw_future_returns_df\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.n_regimes = 2  # Regimes: 0 = bad, 1 = good\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.n_regimes)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, \n",
    "            shape=(self.regime_features.shape[1],),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.regime_features[self.current_step]\n",
    "\n",
    "    def step(self, action):\n",
    "        avg_future_return = np.mean(self.raw_future_returns[self.current_step])\n",
    "\n",
    "        # Reward logic explicitly explained:\n",
    "        # - action = 1 (predict good regime): reward = avg_future_return\n",
    "        # - action = 0 (predict bad regime): reward = -avg_future_return\n",
    "        # Thus, correct predictions yield positive rewards, incorrect yield negative rewards.\n",
    "        reward = avg_future_return if action == 1 else -avg_future_return\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.regime_features) - 1\n",
    "\n",
    "        obs = (\n",
    "            self.regime_features[self.current_step]\n",
    "            if not done else np.zeros(self.observation_space.shape)\n",
    "        )\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class RegimeConditionedPortfolioEnv(gym.Env):\n",
    "    def __init__(self, returns_df, chosen_regime_vector, raw_returns_df, factor_returns_df=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stock_returns = returns_df.values\n",
    "        self.chosen_regime_vector = chosen_regime_vector\n",
    "        self.raw_stock_returns = raw_returns_df.values\n",
    "        self.factor_returns = factor_returns_df.values if factor_returns_df is not None else None\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.n_assets = self.stock_returns.shape[1]\n",
    "\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "        \n",
    "        obs_dim = self.n_assets + 1\n",
    "        if self.factor_returns is not None:\n",
    "            obs_dim += self.factor_returns.shape[1]\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(obs_dim,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        returns = self.stock_returns[self.current_step]\n",
    "        regime = np.array([self.chosen_regime_vector[self.current_step]])\n",
    "        obs = np.concatenate([returns, regime])\n",
    "\n",
    "        if self.factor_returns is not None:\n",
    "            factor_obs = self.factor_returns[self.current_step]\n",
    "            obs = np.concatenate([obs, factor_obs])\n",
    "\n",
    "        return np.nan_to_num(obs)\n",
    "\n",
    "    def step(self, action):\n",
    "        action_sum = action.sum()\n",
    "        if action_sum == 0:\n",
    "            action = np.ones_like(action) / len(action)\n",
    "        else:\n",
    "            action = action / action_sum\n",
    "\n",
    "        # Intuitive reward: explicitly realized portfolio return\n",
    "        reward = np.dot(action, self.raw_stock_returns[self.current_step + 1])\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.stock_returns) - 1\n",
    "\n",
    "        obs = self._get_obs() if not done else np.zeros(self.observation_space.shape)\n",
    "\n",
    "        reward = 0.0 if np.isnan(reward) else reward\n",
    "\n",
    "        return obs, reward, done, {}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fa6000d68203e53",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model_save_dir = 'E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/trained_hierarchical_model/'\n",
    "\n",
    "train_window, test_window = 252, 63\n",
    "rebalance_freq = 21\n",
    "use_factor_returns = False  # Set this flag as needed\n",
    "tickers = stock_returns_norm.columns.tolist()\n",
    "# regime_columns = daily_regimes.columns.tolist() #### remove cluster in the feature set!!\n",
    "regime_columns = daily_regimes.columns.drop('Cluster').tolist()\n",
    "# regime_columns = daily_regimes[['Cluster']].columns.tolist()\n",
    "factor_columns = factor_returns_norm.columns.tolist() if use_factor_returns else []\n",
    "records = []\n",
    "raw_stock_returns = stock_returns_norm_filter\n",
    "load_existing_model = False\n",
    "total_windows = len(range(0, len(merged_data) - train_window - test_window, test_window))\n",
    "\n",
    "for i, start in enumerate(range(0, len(merged_data) - train_window - test_window, test_window)):\n",
    "    print(f\"\\n{'='*30}\\nStarting Hierarchical Training Window {i+1}/{total_windows}\")\n",
    "    high_model_path = os.path.join(model_save_dir, f\"high_model_window_{i}.zip\")\n",
    "\n",
    "    train_df = merged_data.iloc[start:start + train_window]\n",
    "    test_df = merged_data.iloc[start + train_window:start + train_window + test_window + 1]\n",
    "    raw_returns_train = raw_stock_returns.loc[train_df.index]\n",
    "    raw_returns_test = raw_stock_returns.loc[test_df.index]\n",
    "\n",
    "    if train_df.isna().values.any() or test_df.isna().values.any():\n",
    "        print(f\"Skipping window {i+1} due to NaNs.\")\n",
    "        continue\n",
    "\n",
    "    # High-Level training data preparation (no look-ahead bias)\n",
    "    regime_features_high = train_df[regime_columns + factor_columns][:-rebalance_freq]\n",
    "    future_returns_high = np.array([\n",
    "        raw_returns_train.iloc[idx + 1: idx + rebalance_freq + 1].mean().mean()\n",
    "        for idx in range(len(regime_features_high))\n",
    "    ])\n",
    "\n",
    "    high_env = DummyVecEnv([\n",
    "        lambda: RegimeSelectorEnv(regime_features_high, future_returns_high)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Load existing high-level model or train a new one\n",
    "    if load_existing_model and os.path.exists(high_model_path):\n",
    "        print(\"Loading existing High-Level Regime Selector model...\")\n",
    "        high_model = PPO.load(high_model_path, env=high_env)\n",
    "        high_model.learn(total_timesteps=3000)  # continue training briefly\n",
    "        print(f\"Continued training on High-Level model. Saved again to {high_model_path}.\")\n",
    "    else:\n",
    "        print(\"Training new High-Level Regime Selector model...\")\n",
    "        # policy_kwargs = dict(net_arch=[128, 64], activation_fn=nn.Tanh)\n",
    "        # \n",
    "        # high_model = PPO(\n",
    "        #     'MlpPolicy', high_env, verbose=0,\n",
    "        #     learning_rate=1e-4,\n",
    "        #     n_steps=2048,\n",
    "        #     batch_size=128,\n",
    "        #     gamma=0.95,\n",
    "        #     ent_coef=0.005,  # Slightly lower entropy encourages differentiation\n",
    "        #     clip_range=0.2,\n",
    "        #     n_epochs=10,\n",
    "        #     policy_kwargs=policy_kwargs\n",
    "        # )\n",
    "        # high_model.learn(total_timesteps=10000)\n",
    "    \n",
    "        print(\"Training High-Level Regime Selector...\")\n",
    "        high_model = PPO('MlpPolicy', high_env, verbose=0)\n",
    "        high_model.learn(total_timesteps=10000)\n",
    "    # high_model_path = os.path.join(model_save_dir, f\"high_model_window_{i}.zip\")\n",
    "    # high_model.save(high_model_path)\n",
    "    # print(f\"High-level model saved at: {high_model_path}\")\n",
    "\n",
    "    # Generate historical regimes for low-level model training (no look-ahead)\n",
    "    historical_regimes = []\n",
    "    for idx in range(len(regime_features_high)):\n",
    "        obs = regime_features_high.iloc[idx].values\n",
    "        regime, _ = high_model.predict(obs, deterministic=True)\n",
    "        historical_regimes.append(int(regime))\n",
    "\n",
    "    historical_regimes = pd.Series(historical_regimes, index=regime_features_high.index)\n",
    "\n",
    "    # Train low-level models per historical regime\n",
    "    low_level_models = {}\n",
    "    for regime in historical_regimes.unique():\n",
    "        low_model_path = os.path.join(model_save_dir, f\"low_model_window_{i}_regime_{regime}.zip\")\n",
    "        regime_indices = historical_regimes[historical_regimes == regime].index\n",
    "        regime_df = train_df.loc[regime_indices]\n",
    "        chosen_regime_vector_train = np.repeat(regime, len(regime_df))\n",
    "\n",
    "        low_env_train = DummyVecEnv([\n",
    "            lambda: RegimeConditionedPortfolioEnv(\n",
    "                returns_df=regime_df[tickers],\n",
    "                chosen_regime_vector=chosen_regime_vector_train,\n",
    "                raw_returns_df=raw_returns_train.loc[regime_df.index]\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        if load_existing_model and os.path.exists(low_model_path):\n",
    "            print(f\"Loading existing Low-Level model for regime {regime}...\")\n",
    "            low_model = PPO.load(low_model_path, env=low_env_train)\n",
    "            low_model.learn(total_timesteps=3000)  # brief continued training\n",
    "            print(f\"Continued training on Low-Level model regime {regime}. Saved again to {low_model_path}.\")\n",
    "        else:\n",
    "            print(f\"Training new Low-Level model for regime {regime}...\")\n",
    "            # low_model = PPO(\n",
    "            #     'MlpPolicy', low_env_train, verbose=0,\n",
    "            #     learning_rate=5e-5,\n",
    "            #     n_steps=2048,\n",
    "            #     batch_size=128,\n",
    "            #     gamma=0.97,\n",
    "            #     ent_coef=0.003,  # Reduced entropy to discourage uniform policy\n",
    "            #     clip_range=0.2,\n",
    "            #     n_epochs=10,\n",
    "            #     policy_kwargs=policy_kwargs\n",
    "            # )\n",
    "            # low_model.learn(total_timesteps=10000)\n",
    "\n",
    "            low_model = PPO('MlpPolicy', low_env_train, verbose=0)\n",
    "            low_model.learn(total_timesteps=10000)\n",
    "        # low_model_path = os.path.join(model_save_dir, f\"low_model_window_{i}_regime_{regime}.zip\")\n",
    "        # low_model.save(low_model_path)\n",
    "        low_level_models[regime] = low_model\n",
    "        # print(f\"Low-level model for regime {regime} saved at: {low_model_path}\")\n",
    "\n",
    "    # Testing\n",
    "    test_dates = test_df.index\n",
    "    test_returns = test_df[tickers].values\n",
    "    test_regimes = test_df[regime_columns + factor_columns].values\n",
    "\n",
    "    for rebalance_num, rebalance_start in enumerate(range(0, test_window, rebalance_freq)):\n",
    "        rebalance_end = min(rebalance_start + rebalance_freq, test_window)\n",
    "\n",
    "        high_obs = test_regimes[rebalance_start]\n",
    "        chosen_regime, _ = high_model.predict(high_obs, deterministic=True)\n",
    "        chosen_regime = int(chosen_regime.item())\n",
    "        # high_predictions = [high_model.predict(high_obs, deterministic=False)[0].item() for _ in range(200)]\n",
    "        # chosen_regime = int(pd.Series(high_predictions).mode()[0])  # take most common predicted regime\n",
    "        \n",
    "        chosen_regime_vector_test = np.repeat(chosen_regime, rebalance_end - rebalance_start)\n",
    "        print(f'chosen_regime = {chosen_regime} and check is {chosen_regime in low_level_models}')\n",
    "        if chosen_regime in low_level_models:\n",
    "            low_model = low_level_models[chosen_regime]\n",
    "            low_env_test = RegimeConditionedPortfolioEnv(\n",
    "                returns_df=pd.DataFrame(test_returns[rebalance_start:rebalance_end], columns=tickers),\n",
    "                chosen_regime_vector=chosen_regime_vector_test,\n",
    "                raw_returns_df=raw_returns_test.iloc[rebalance_start:rebalance_end]\n",
    "            )\n",
    "\n",
    "            low_obs = low_env_test.reset()\n",
    "            action, _ = low_model.predict(low_obs, deterministic=True)\n",
    "            action /= action.sum()\n",
    "            # action_samples = np.array([\n",
    "            #     low_model.predict(low_obs, deterministic=False)[0]\n",
    "            #     for _ in range(200)\n",
    "            # ])\n",
    "            # action_mean = action_samples.mean(axis=0)\n",
    "            # action_mean /= action_mean.sum()  # normalization\n",
    "            # action = action_mean\n",
    "            \n",
    "        else:\n",
    "            action = np.ones(len(tickers)) / len(tickers)\n",
    "\n",
    "        portfolio_value = 1.0\n",
    "        for step in range(rebalance_end - rebalance_start - 1):\n",
    "            reward = np.dot(action, raw_returns_test.iloc[rebalance_start + step + 1])\n",
    "            portfolio_value *= (1 + reward)\n",
    "        cumulative_reward = portfolio_value - 1.0\n",
    "\n",
    "        weights_dict = {ticker: round(weight, 4) for ticker, weight in zip(tickers, action)}\n",
    "        records.append({\n",
    "            'Rebalance_Date': test_dates[rebalance_start],\n",
    "            'Window': i,\n",
    "            'Rebalance_Number': rebalance_num,\n",
    "            'Chosen_Regime': chosen_regime,\n",
    "            'Holding_Period_Start': test_dates[rebalance_start],\n",
    "            'Holding_Period_End': test_dates[rebalance_end - 1],\n",
    "            'Cumulative_Reward': cumulative_reward,\n",
    "            **weights_dict\n",
    "        })\n",
    "\n",
    "        print(f\"Cumulative reward: {cumulative_reward:.4f}\")\n",
    "\n",
    "result_df = pd.DataFrame(records)\n",
    "result_df['Cumulative_Return'] = result_df['Cumulative_Reward'].cumsum()\n",
    "result_df.to_csv('hierarchical_allocations_dates_rebalanced.csv', index=False)\n",
    "\n",
    "print(\"\\nHierarchical training/testing completed successfully.\")\n",
    "print(\"Final hierarchical results saved to 'hierarchical_allocations_dates_rebalanced.csv'\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a3b88d647f58602",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "############################\n",
    "# retrain model using the previous trained agents for high and low:\n",
    "############################\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model_save_dir = 'E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/trained_hierarchical_model/'\n",
    "\n",
    "train_window, test_window = 252, 42\n",
    "rebalance_freq = 42\n",
    "use_factor_returns = False  # Set this flag as needed\n",
    "tickers = stock_returns_norm.columns.tolist()\n",
    "regime_columns = daily_regimes.columns.tolist() \n",
    "# regime_columns = daily_regimes.columns.drop('Cluster').tolist() #### remove cluster in the feature set!!\n",
    "# regime_columns = daily_regimes[['Cluster']].columns.tolist()\n",
    "factor_columns = factor_returns_norm.columns.tolist() if use_factor_returns else []\n",
    "records = []\n",
    "raw_stock_returns = stock_returns_norm_filter\n",
    "load_existing_model = False\n",
    "total_windows = len(range(0, len(merged_data) - train_window - test_window, test_window))\n",
    "\n",
    "for i, start in enumerate(range(0, len(merged_data) - train_window - test_window, test_window)):\n",
    "    print(f\"\\n{'='*30}\\nStarting Hierarchical Training Window {i+1}/{total_windows}\")\n",
    "    \n",
    "    # Model paths explicitly defined for high and low-level models\n",
    "    high_model_path = os.path.join(model_save_dir, f\"high_model_window_{i-1}.zip\")  # Load previous model\n",
    "    new_high_model_path = os.path.join(model_save_dir, f\"high_model_window_{i}.zip\")  # Save current model\n",
    "    \n",
    "    # Data splitting\n",
    "    train_df = merged_data.iloc[start:start + train_window]\n",
    "    test_df = merged_data.iloc[start + train_window:start + train_window + test_window + 1]\n",
    "    raw_returns_train = raw_stock_returns.loc[train_df.index]\n",
    "    raw_returns_test = raw_stock_returns.loc[test_df.index]\n",
    "\n",
    "    # High-Level training data preparation (no look-ahead bias)\n",
    "    regime_features_high = train_df[regime_columns + factor_columns][:-rebalance_freq]\n",
    "    future_returns_high = np.array([\n",
    "        raw_returns_train.iloc[idx + 1: idx + rebalance_freq + 1].mean().mean()\n",
    "        for idx in range(len(regime_features_high))\n",
    "    ])\n",
    "\n",
    "    high_env = DummyVecEnv([\n",
    "        lambda: RegimeSelectorEnv(regime_features_high, future_returns_high)\n",
    "    ])\n",
    "    \n",
    "    # Load and continue training from previous window's model\n",
    "    if load_existing_model and i > 0 and os.path.exists(high_model_path):\n",
    "        print(\"Loading existing High-Level model from previous window...\")\n",
    "        high_model = PPO.load(high_model_path, env=high_env)\n",
    "        high_model.learn(total_timesteps=20000)  # shorter fine-tuning\n",
    "    else:\n",
    "        print(\"Training new High-Level Regime Selector from scratch...\")\n",
    "        high_model = PPO('MlpPolicy', high_env, verbose=0)\n",
    "        high_model.learn(total_timesteps=50000)  # full training only for first window\n",
    "    \n",
    "    # Save updated high-level model explicitly for next window's reuse\n",
    "    high_model.save(new_high_model_path)\n",
    "    print(f\"High-level model saved at: {new_high_model_path}\")\n",
    "\n",
    "    # Generate historical regimes using the trained high-level model\n",
    "    historical_regimes = []\n",
    "    for idx in range(len(regime_features_high)):\n",
    "        obs = regime_features_high.iloc[idx].values\n",
    "        regime, _ = high_model.predict(obs, deterministic=True)\n",
    "        historical_regimes.append(int(regime))\n",
    "\n",
    "    historical_regimes = pd.Series(historical_regimes, index=regime_features_high.index)\n",
    "\n",
    "    # Low-level model training with incremental training\n",
    "    low_level_models = {}\n",
    "    for regime in historical_regimes.unique():\n",
    "        prev_low_model_path = os.path.join(model_save_dir, f\"low_model_window_{i-1}_regime_{regime}.zip\")\n",
    "        current_low_model_path = os.path.join(model_save_dir, f\"low_model_window_{i}_regime_{regime}.zip\")\n",
    "        \n",
    "        regime_indices = historical_regimes[historical_regimes == regime].index\n",
    "        regime_df = train_df.loc[regime_indices]\n",
    "        chosen_regime_vector_train = np.repeat(regime, len(regime_df))\n",
    "\n",
    "        low_env_train = DummyVecEnv([\n",
    "            lambda: RegimeConditionedPortfolioEnv(\n",
    "                returns_df=regime_df[tickers],\n",
    "                chosen_regime_vector=chosen_regime_vector_train,\n",
    "                raw_returns_df=raw_returns_train.loc[regime_df.index]\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        if load_existing_model and i > 0 and os.path.exists(prev_low_model_path):\n",
    "            print(f\"Loading existing Low-Level model for regime {regime} from previous window...\")\n",
    "            low_model = PPO.load(prev_low_model_path, env=low_env_train)\n",
    "            low_model.learn(total_timesteps=20000)  # shorter fine-tuning\n",
    "        else:\n",
    "            print(f\"Training new Low-Level model for regime {regime} from scratch...\")\n",
    "            low_model = PPO('MlpPolicy', low_env_train, verbose=0)\n",
    "            low_model.learn(total_timesteps=10000)  # full training for first window or new regime\n",
    "\n",
    "        low_model.save(current_low_model_path)\n",
    "        low_level_models[regime] = low_model\n",
    "        print(f\"Low-level model for regime {regime} saved at: {current_low_model_path}\")\n",
    "\n",
    "\n",
    "    # Testing\n",
    "    test_dates = test_df.index\n",
    "    test_returns = test_df[tickers].values\n",
    "    test_regimes = test_df[regime_columns + factor_columns].values\n",
    "\n",
    "    for rebalance_num, rebalance_start in enumerate(range(0, test_window, rebalance_freq)):\n",
    "        rebalance_end = min(rebalance_start + rebalance_freq, test_window)\n",
    "\n",
    "        high_obs = test_regimes[rebalance_start]\n",
    "        chosen_regime, _ = high_model.predict(high_obs, deterministic=True)\n",
    "        chosen_regime = int(chosen_regime.item())\n",
    "        # high_predictions = [high_model.predict(high_obs, deterministic=False)[0].item() for _ in range(200)]\n",
    "        # chosen_regime = int(pd.Series(high_predictions).mode()[0])  # take most common predicted regime\n",
    "        \n",
    "        chosen_regime_vector_test = np.repeat(chosen_regime, rebalance_end - rebalance_start)\n",
    "        print(f'chosen_regime = {chosen_regime} and check is {chosen_regime in low_level_models}')\n",
    "        if chosen_regime in low_level_models:\n",
    "            low_model = low_level_models[chosen_regime]\n",
    "            low_env_test = RegimeConditionedPortfolioEnv(\n",
    "                returns_df=pd.DataFrame(test_returns[rebalance_start:rebalance_end], columns=tickers),\n",
    "                chosen_regime_vector=chosen_regime_vector_test,\n",
    "                raw_returns_df=raw_returns_test.iloc[rebalance_start:rebalance_end]\n",
    "            )\n",
    "\n",
    "            low_obs = low_env_test.reset()\n",
    "            action, _ = low_model.predict(low_obs, deterministic=True)\n",
    "            action /= action.sum()\n",
    "            # action_samples = np.array([\n",
    "            #     low_model.predict(low_obs, deterministic=False)[0]\n",
    "            #     for _ in range(200)\n",
    "            # ])\n",
    "            # action_mean = action_samples.mean(axis=0)\n",
    "            # action_mean /= action_mean.sum()  # normalization\n",
    "            # action = action_mean\n",
    "            \n",
    "        else:\n",
    "            action = np.ones(len(tickers)) / len(tickers)\n",
    "\n",
    "        portfolio_value = 1.0\n",
    "        for step in range(rebalance_end - rebalance_start - 1):\n",
    "            reward = np.dot(action, raw_returns_test.iloc[rebalance_start + step + 1])\n",
    "            portfolio_value *= (1 + reward)\n",
    "        cumulative_reward = portfolio_value - 1.0\n",
    "\n",
    "        weights_dict = {ticker: round(weight, 4) for ticker, weight in zip(tickers, action)}\n",
    "        records.append({\n",
    "            'Rebalance_Date': test_dates[rebalance_start],\n",
    "            'Window': i,\n",
    "            'Rebalance_Number': rebalance_num,\n",
    "            'Chosen_Regime': chosen_regime,\n",
    "            'Holding_Period_Start': test_dates[rebalance_start],\n",
    "            'Holding_Period_End': test_dates[rebalance_end - 1],\n",
    "            'Cumulative_Reward': cumulative_reward,\n",
    "            **weights_dict\n",
    "        })\n",
    "\n",
    "        print(f\"Cumulative reward: {cumulative_reward:.4f}\")\n",
    "\n",
    "result_df = pd.DataFrame(records)\n",
    "result_df['Cumulative_Return'] = result_df['Cumulative_Reward'].cumsum()\n",
    "result_df.to_csv('hierarchical_allocations_dates_rebalanced.csv', index=False)\n",
    "\n",
    "print(\"\\nHierarchical training/testing completed successfully.\")\n",
    "print(\"Final hierarchical results saved to 'hierarchical_allocations_dates_rebalanced.csv'\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce36553869f31a64",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(records)\n",
    "result_df['Cumulative_Return'] = result_df['Cumulative_Reward'].cumsum()\n",
    "result_df.to_csv('hierarchical_allocations_dates_rebalanced.csv', index=False)\n",
    "\n",
    "print(\"\\nHierarchical training/testing completed successfully.\")\n",
    "print(\"Final hierarchical results saved to 'hierarchical_allocations_dates_rebalanced.csv'\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e349ded169b8f2e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "##### parallel running\n",
    "##### Parallel Running with Progress Indicators\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Set your directories and parameters explicitly\n",
    "model_save_dir = 'E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/trained_hierarchical_model/'\n",
    "results_save_dir = 'E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/results/'\n",
    "\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "os.makedirs(results_save_dir, exist_ok=True)\n",
    "\n",
    "# Your configurations\n",
    "train_window, test_window = 252, 63\n",
    "rebalance_freq = 21\n",
    "use_factor_returns = False\n",
    "\n",
    "tickers = stock_returns_norm.columns.tolist()\n",
    "regime_columns = daily_regimes.columns.tolist()\n",
    "factor_columns = factor_returns_norm.columns.tolist() if use_factor_returns else []\n",
    "\n",
    "total_runs = 25\n",
    "load_existing_model = False\n",
    "\n",
    "final_records = []  # To collect all results across 100 runs\n",
    "\n",
    "# Main sequential loop\n",
    "for run_number in range(total_runs):\n",
    "    print(f\"\\n{'='*60}\\n Starting Run {run_number + 1}/{total_runs}\\n{'='*60}\")\n",
    "    run_records = []\n",
    "\n",
    "    total_windows = len(range(0, len(merged_data) - train_window - test_window, test_window))\n",
    "\n",
    "    for i, start in enumerate(range(0, len(merged_data) - train_window - test_window, test_window)):\n",
    "        # print(f\"\\n Run {run_number + 1}, Training Window {i + 1}/{total_windows}\")\n",
    "\n",
    "        # Model path definitions\n",
    "        high_model_path_prev = os.path.join(model_save_dir, f\"run{run_number}_high_model_window_{i-1}.zip\")\n",
    "        high_model_path_curr = os.path.join(model_save_dir, f\"run{run_number}_high_model_window_{i}.zip\")\n",
    "\n",
    "        # Data preparation\n",
    "        train_df = merged_data.iloc[start:start + train_window]\n",
    "        test_df = merged_data.iloc[start + train_window:start + train_window + test_window + 1]\n",
    "        raw_returns_train = stock_returns_norm_filter.loc[train_df.index]\n",
    "        raw_returns_test = stock_returns_norm_filter.loc[test_df.index]\n",
    "\n",
    "        # High-level regime features\n",
    "        regime_features_high = train_df[regime_columns + factor_columns][:-rebalance_freq]\n",
    "        future_returns_high = np.array([\n",
    "            raw_returns_train.iloc[idx + 1: idx + rebalance_freq + 1].mean().mean()\n",
    "            for idx in range(len(regime_features_high))\n",
    "        ])\n",
    "\n",
    "        high_env = DummyVecEnv([\n",
    "            lambda: RegimeSelectorEnv(regime_features_high, future_returns_high)\n",
    "        ])\n",
    "\n",
    "        # High-level model training\n",
    "        if load_existing_model and i > 0 and os.path.exists(high_model_path_prev):\n",
    "            # print(f\" Loading and retraining High-Level model from previous window {i-1}...\")\n",
    "            high_model = PPO.load(high_model_path_prev, env=high_env)\n",
    "            high_model.learn(total_timesteps=20000)\n",
    "        else:\n",
    "            # print(\" Training new High-Level Regime Selector model from scratch...\")\n",
    "            high_model = PPO('MlpPolicy', high_env, verbose=0)\n",
    "            high_model.learn(total_timesteps=10000)\n",
    "\n",
    "        high_model.save(high_model_path_curr)\n",
    "\n",
    "        # Generate historical regimes\n",
    "        historical_regimes = [\n",
    "            int(high_model.predict(regime_features_high.iloc[idx].values, deterministic=True)[0])\n",
    "            for idx in range(len(regime_features_high))\n",
    "        ]\n",
    "        historical_regimes_series = pd.Series(historical_regimes, index=regime_features_high.index)\n",
    "\n",
    "        # Low-level model training per regime\n",
    "        low_level_models = {}\n",
    "        for regime in historical_regimes_series.unique():\n",
    "            low_model_path_prev = os.path.join(model_save_dir, f\"run{run_number}_low_model_window_{i-1}_regime_{regime}.zip\")\n",
    "            low_model_path_curr = os.path.join(model_save_dir, f\"run{run_number}_low_model_window_{i}_regime_{regime}.zip\")\n",
    "\n",
    "            regime_indices = historical_regimes_series[historical_regimes_series == regime].index\n",
    "            regime_df = train_df.loc[regime_indices]\n",
    "            chosen_regime_vector_train = np.repeat(regime, len(regime_df))\n",
    "\n",
    "            low_env_train = DummyVecEnv([\n",
    "                lambda: RegimeConditionedPortfolioEnv(\n",
    "                    returns_df=regime_df[tickers],\n",
    "                    chosen_regime_vector=chosen_regime_vector_train,\n",
    "                    raw_returns_df=raw_returns_train.loc[regime_df.index]\n",
    "                )\n",
    "            ])\n",
    "\n",
    "            if load_existing_model and i > 0 and os.path.exists(low_model_path_prev):\n",
    "                # print(f\" Loading and retraining Low-Level model for regime {regime} from previous window...\")\n",
    "                low_model = PPO.load(low_model_path_prev, env=low_env_train)\n",
    "                low_model.learn(total_timesteps=20000)\n",
    "            else:\n",
    "                # print(f\" Training new Low-Level model for regime {regime} from scratch...\")\n",
    "                low_model = PPO('MlpPolicy', low_env_train, verbose=0)\n",
    "                low_model.learn(total_timesteps=10000)\n",
    "\n",
    "            low_model.save(low_model_path_curr)\n",
    "            low_level_models[regime] = low_model\n",
    "\n",
    "        # Testing\n",
    "        for rebalance_num, rebalance_start in enumerate(range(0, test_window, rebalance_freq)):\n",
    "            rebalance_end = min(rebalance_start + rebalance_freq, test_window)\n",
    "            high_obs = test_df[regime_columns + factor_columns].iloc[rebalance_start].values\n",
    "            chosen_regime = int(high_model.predict(high_obs, deterministic=True)[0])\n",
    "\n",
    "            chosen_regime_vector_test = np.repeat(chosen_regime, rebalance_end - rebalance_start)\n",
    "\n",
    "            if chosen_regime in low_level_models:\n",
    "                low_model = low_level_models[chosen_regime]\n",
    "                low_env_test = RegimeConditionedPortfolioEnv(\n",
    "                    returns_df=test_df[tickers].iloc[rebalance_start:rebalance_end],\n",
    "                    chosen_regime_vector=chosen_regime_vector_test,\n",
    "                    raw_returns_df=raw_returns_test.iloc[rebalance_start:rebalance_end]\n",
    "                )\n",
    "                low_obs = low_env_test.reset()\n",
    "                action, _ = low_model.predict(low_obs, deterministic=True)\n",
    "                action /= action.sum()\n",
    "            else:\n",
    "                action = np.ones(len(tickers)) / len(tickers)\n",
    "\n",
    "            portfolio_value = 1.0\n",
    "            for step in range(rebalance_end - rebalance_start - 1):\n",
    "                reward = np.dot(action, raw_returns_test.iloc[rebalance_start + step + 1])\n",
    "                portfolio_value *= (1 + reward)\n",
    "            cumulative_reward = portfolio_value - 1.0\n",
    "\n",
    "            weights_dict = {ticker: round(weight, 4) for ticker, weight in zip(tickers, action)}\n",
    "            run_records.append({\n",
    "                'Run': run_number,\n",
    "                'Rebalance_Date': test_df.index[rebalance_start],\n",
    "                'Window': i,\n",
    "                'Rebalance_Number': rebalance_num,\n",
    "                'Chosen_Regime': chosen_regime,\n",
    "                'Cumulative_Reward': cumulative_reward,\n",
    "                **weights_dict\n",
    "            })\n",
    "\n",
    "            print(f\" Run {run_number + 1}, Window {i + 1}, Rebalance {rebalance_num + 1}: Cumulative reward: {cumulative_reward:.4f}\")\n",
    "\n",
    "    # Save individual run result\n",
    "    run_df = pd.DataFrame(run_records)\n",
    "    run_df.to_csv(f'{results_save_dir}hierarchical_run_{run_number}.csv', index=False)\n",
    "    final_records.append(run_df)\n",
    "\n",
    "# Combine all results after all runs\n",
    "final_df = pd.concat(final_records, ignore_index=True)\n",
    "final_df.to_csv(f'{results_save_dir}consolidated_hierarchical_results.csv', index=False)\n",
    "\n",
    "print(\"\\n All 25 runs completed successfully!\")\n",
    "print(f\" Consolidated results saved to '{results_save_dir}consolidated_hierarchical_results.csv'\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c48f503bb73d2ffe",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "LOAD and CALC Portfolio Return"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3c111d75a6a1756"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drifted weights (head):\n",
      "                  XLB  XLE       XLF       XLI  XLK  XLP       XLY       XLV  \\\n",
      "Date                                                                          \n",
      "2017-03-14  0.277734  0.0  0.005614  0.279801  0.0  0.0  0.239181  0.039133   \n",
      "2017-03-15   0.27886  0.0  0.005539  0.279645  0.0  0.0  0.237612  0.039158   \n",
      "2017-03-16  0.278435  0.0  0.005578  0.279862  0.0  0.0  0.238857  0.038987   \n",
      "2017-03-17  0.278897  0.0  0.005507  0.280261  0.0  0.0  0.238569  0.038652   \n",
      "2017-03-20  0.280636  0.0  0.005465  0.279942  0.0  0.0  0.238161  0.038622   \n",
      "\n",
      "                 XLU  \n",
      "Date                  \n",
      "2017-03-14  0.158538  \n",
      "2017-03-15  0.159186  \n",
      "2017-03-16   0.15828  \n",
      "2017-03-17  0.158114  \n",
      "2017-03-20  0.157173  \n",
      "\n",
      "Portfolio returns (head):\n",
      "            Optimal_Portfolio_Return Equal_Weight_Return\n",
      "Date                                                   \n",
      "2017-03-14                -0.005398           -0.004688\n",
      "2017-03-15                 0.011918            0.010684\n",
      "2017-03-16                -0.005325            -0.00398\n",
      "2017-03-17                 -0.00051           -0.004431\n",
      "2017-03-20                -0.000691           -0.001479\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data explicitly\n",
    "port_wts = pd.read_csv('port_wt_test_old.csv', parse_dates=['Date'], index_col='Date')\n",
    "daily_returns = pd.read_csv('daily_returns_10ETFs.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "common_tickers = [col for col in port_wts.columns if col in daily_returns.columns]\n",
    "daily_returns = daily_returns[common_tickers]\n",
    "\n",
    "# Explicitly filter daily returns to the date range covered by portfolio weights\n",
    "start_date, end_date = port_wts.index.min(), port_wts.index.max() + pd.Timedelta(days=21)\n",
    "daily_returns = daily_returns.loc[start_date:end_date]\n",
    "\n",
    "# Initialize drifted weights with the first available rebalance weights\n",
    "initial_weights = port_wts.loc[start_date].values\n",
    "\n",
    "equal_weight = np.array([1.0 / len(common_tickers)] * len(common_tickers))\n",
    "\n",
    "# Create drifted weights DataFrame explicitly initialized\n",
    "drifted_weights = pd.DataFrame(index=daily_returns.index, columns=common_tickers)\n",
    "equal_weights = pd.DataFrame(index=daily_returns.index, columns=common_tickers)\n",
    "\n",
    "current_weights = initial_weights\n",
    "current_equal_weights = equal_weight\n",
    "\n",
    "# Initialize returns DataFrame explicitly\n",
    "returns_df = pd.DataFrame(index=daily_returns.index, columns=['Optimal_Portfolio_Return', 'Equal_Weight_Return'])\n",
    "\n",
    "for current_date in daily_returns.index:\n",
    "    if current_date in port_wts.index:\n",
    "        # Explicit rebalance date: assign new weights\n",
    "        current_weights = port_wts.loc[current_date].values\n",
    "        current_equal_weights = equal_weight\n",
    "    else:\n",
    "        # Explicitly drift weights using previous day's return\n",
    "        prev_day_return = daily_returns.loc[current_date]\n",
    "\n",
    "        drifted_wts_numerator = current_weights * (1 + prev_day_return.values)\n",
    "        current_weights = drifted_wts_numerator / np.sum(drifted_wts_numerator)\n",
    "\n",
    "        equal_drifted_numerator = current_equal_weights * (1 + prev_day_return.values)\n",
    "        current_equal_weights = equal_drifted_numerator / np.sum(equal_drifted_numerator)\n",
    "\n",
    "    drifted_weights.loc[current_date] = current_weights\n",
    "    equal_weights.loc[current_date] = current_equal_weights\n",
    "    shifted_drifted_weights = drifted_weights.shift(1)\n",
    "    shifted_equal_weights = equal_weights.shift(1)\n",
    "    if current_date == daily_returns.index[0]:\n",
    "        # On the first day, use initial weights directly\n",
    "        returns_df.loc[current_date, 'Optimal_Portfolio_Return'] = np.dot(\n",
    "            drifted_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "        returns_df.loc[current_date, 'Equal_Weight_Return'] = np.dot(\n",
    "            equal_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "    else:\n",
    "        # Explicitly use previous day's weights\n",
    "        returns_df.loc[current_date, 'Optimal_Portfolio_Return'] = np.dot(\n",
    "            shifted_drifted_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "        returns_df.loc[current_date, 'Equal_Weight_Return'] = np.dot(\n",
    "            shifted_equal_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "\n",
    "# Check explicitly\n",
    "print(\"Drifted weights (head):\\n\", drifted_weights.head())\n",
    "print(\"\\nPortfolio returns (head):\\n\", returns_df.head())\n",
    "\n",
    "# Save explicitly\n",
    "drifted_weights.to_csv('drifted_weights_corrected.csv')\n",
    "equal_weights.to_csv('equal_weights.csv')\n",
    "returns_df.to_csv('portfolio_returns_combined.csv')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-30T23:22:54.873776Z",
     "start_time": "2025-07-30T23:22:53.038318Z"
    }
   },
   "id": "7a61afcc56dc1982",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined results saved successfully to stage 2 result\\multi-seed one time predict\\10 iters\\combined_weights.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to your CSV files\n",
    "csv_folder = r\"stage 2 result\\multi-seed one time predict\\10 iters\"\n",
    "output_csv = os.path.join(csv_folder, 'combined_weights.csv')\n",
    "\n",
    "# Initialize an empty DataFrame to store combined results\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each seed file\n",
    "for seed in range(1, 2):  # Seed from 1 to 10\n",
    "    file_path = os.path.join(csv_folder, f'ppo_stage2_weights_seed{seed}.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add a column to indicate the seed number\n",
    "        df['Seed'] = seed\n",
    "        \n",
    "        # Append to the combined DataFrame\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"Warning: File {file_path} not found.\")\n",
    "\n",
    "# Save the combined DataFrame to a new CSV\n",
    "combined_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Combined results saved successfully to {output_csv}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-26T23:55:07.611318Z",
     "start_time": "2025-07-26T23:55:07.528299Z"
    }
   },
   "id": "d335528054706512",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 1\n",
      "process 2\n",
      "process 3\n",
      "process 4\n",
      "process 5\n",
      "process 6\n",
      "process 7\n",
      "process 8\n",
      "process 9\n",
      "process 10\n",
      "process 11\n",
      "process 12\n",
      "process 13\n",
      "process 14\n",
      "process 15\n",
      "process 16\n",
      "process 17\n",
      "process 18\n",
      "process 19\n",
      "process 20\n",
      "process 21\n",
      "process 22\n",
      "process 23\n",
      "process 24\n",
      "process 25\n",
      "process 26\n",
      "process 27\n",
      "process 28\n",
      "process 29\n",
      "            Cumulative_Return_Run_1  Cumulative_Return_Run_2  \\\n",
      "Date                                                           \n",
      "2017-03-14                -0.002668                -0.005907   \n",
      "2017-03-15                 0.003042                 0.006372   \n",
      "2017-03-16                 0.002931                 0.000685   \n",
      "2017-03-17                -0.000860                -0.002089   \n",
      "2017-03-20                -0.002660                -0.003773   \n",
      "\n",
      "            Cumulative_Return_Run_3  Cumulative_Return_Run_4  \\\n",
      "Date                                                           \n",
      "2017-03-14                -0.004572                -0.005295   \n",
      "2017-03-15                 0.008736                 0.005120   \n",
      "2017-03-16                 0.003740                 0.002161   \n",
      "2017-03-17                 0.001348                 0.001472   \n",
      "2017-03-20                 0.000026                 0.002906   \n",
      "\n",
      "            Cumulative_Return_Run_5  Cumulative_Return_Run_6  \\\n",
      "Date                                                           \n",
      "2017-03-14                -0.002602                -0.003596   \n",
      "2017-03-15                 0.006584                 0.007730   \n",
      "2017-03-16                -0.000057                 0.002256   \n",
      "2017-03-17                -0.008188                -0.003949   \n",
      "2017-03-20                -0.013250                -0.004985   \n",
      "\n",
      "            Cumulative_Return_Run_7  Cumulative_Return_Run_8  \\\n",
      "Date                                                           \n",
      "2017-03-14                -0.002632                -0.002054   \n",
      "2017-03-15                 0.005556                 0.006402   \n",
      "2017-03-16                 0.003698                 0.002576   \n",
      "2017-03-17                -0.000230                -0.002500   \n",
      "2017-03-20                -0.000173                -0.006364   \n",
      "\n",
      "            Cumulative_Return_Run_9  Cumulative_Return_Run_10  ...  \\\n",
      "Date                                                           ...   \n",
      "2017-03-14                -0.004777                 -0.002602  ...   \n",
      "2017-03-15                 0.005426                  0.006909  ...   \n",
      "2017-03-16                 0.003091                  0.003428  ...   \n",
      "2017-03-17                 0.000058                 -0.001135  ...   \n",
      "2017-03-20                 0.001991                 -0.001885  ...   \n",
      "\n",
      "            Cumulative_Return_Run_20  Cumulative_Return_Run_21  \\\n",
      "Date                                                             \n",
      "2017-03-14                 -0.005861                 -0.004482   \n",
      "2017-03-15                  0.003967                  0.006360   \n",
      "2017-03-16                  0.001782                  0.002843   \n",
      "2017-03-17                 -0.002014                 -0.000585   \n",
      "2017-03-20                 -0.002975                 -0.002276   \n",
      "\n",
      "            Cumulative_Return_Run_22  Cumulative_Return_Run_23  \\\n",
      "Date                                                             \n",
      "2017-03-14                 -0.003449                 -0.004540   \n",
      "2017-03-15                  0.004964                  0.005764   \n",
      "2017-03-16                  0.001862                  0.001593   \n",
      "2017-03-17                 -0.003727                 -0.002714   \n",
      "2017-03-20                 -0.005427                 -0.003219   \n",
      "\n",
      "            Cumulative_Return_Run_24  Cumulative_Return_Run_25  \\\n",
      "Date                                                             \n",
      "2017-03-14                 -0.004616                 -0.004147   \n",
      "2017-03-15                  0.008595                  0.002987   \n",
      "2017-03-16                  0.002992                  0.002163   \n",
      "2017-03-17                 -0.001330                 -0.003281   \n",
      "2017-03-20                 -0.002055                 -0.003164   \n",
      "\n",
      "            Cumulative_Return_Run_26  Cumulative_Return_Run_27  \\\n",
      "Date                                                             \n",
      "2017-03-14                 -0.006403                 -0.005673   \n",
      "2017-03-15                  0.004811                  0.008847   \n",
      "2017-03-16                  0.000375                  0.001788   \n",
      "2017-03-17                 -0.004445                 -0.001003   \n",
      "2017-03-20                 -0.005511                 -0.003588   \n",
      "\n",
      "            Cumulative_Return_Run_28  Cumulative_Return_Run_29  \n",
      "Date                                                            \n",
      "2017-03-14                 -0.005570                 -0.004470  \n",
      "2017-03-15                  0.005243                  0.006390  \n",
      "2017-03-16                  0.001087                  0.002677  \n",
      "2017-03-17                 -0.003998                 -0.002341  \n",
      "2017-03-20                 -0.004432                 -0.004164  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load portfolio weights and daily returns\n",
    "port_wts = pd.read_csv('port_wt_test.csv', parse_dates=['Date'], index_col='Date')\n",
    "daily_returns = pd.read_csv('daily_returns_10ETFs.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "common_tickers = [col for col in port_wts.columns if col in daily_returns.columns]\n",
    "daily_returns = daily_returns[common_tickers]\n",
    "\n",
    "# Ensure daily_returns covers required dates\n",
    "start_date, end_date = port_wts.index.min(), port_wts.index.max() + pd.Timedelta(days=21)\n",
    "daily_returns = daily_returns.loc[start_date:end_date]\n",
    "\n",
    "# Prepare final results DataFrame\n",
    "cumulative_returns_df = pd.DataFrame(index=daily_returns.index)\n",
    "\n",
    "# Loop through each run explicitly\n",
    "for run in sorted(port_wts['Run'].unique()):\n",
    "    print(f'process {run}')\n",
    "    port_wts_run = port_wts[port_wts['Run'] == run].drop(columns=['Run'])\n",
    "\n",
    "    # Initialize drifted weights\n",
    "    drifted_weights = pd.DataFrame(index=daily_returns.index, columns=common_tickers)\n",
    "    \n",
    "    current_weights = port_wts_run.iloc[0].values\n",
    "\n",
    "    returns_series = []\n",
    "\n",
    "    for current_date in daily_returns.index:\n",
    "        if current_date in port_wts_run.index:\n",
    "            # On rebalance dates, explicitly assign new weights\n",
    "            current_weights = port_wts_run.loc[current_date].values\n",
    "        else:\n",
    "            # Drift weights based on previous day's returns\n",
    "            prev_returns = daily_returns.loc[current_date]\n",
    "            current_weights = (current_weights * (1 + prev_returns.values))\n",
    "            current_weights /= np.sum(current_weights)\n",
    "\n",
    "        drifted_weights.loc[current_date] = current_weights\n",
    "        \n",
    "        # Use previous day's weights to calculate today's return\n",
    "        if current_date == daily_returns.index[0]:\n",
    "            daily_return = np.dot(current_weights, daily_returns.loc[current_date])\n",
    "        else:\n",
    "            prev_day_weights = drifted_weights.shift(1).loc[current_date]\n",
    "            daily_return = np.dot(prev_day_weights, daily_returns.loc[current_date])\n",
    "\n",
    "        returns_series.append(daily_return)\n",
    "\n",
    "    cumulative_returns_df[f'Cumulative_Return_Run_{run}'] = (1 + pd.Series(returns_series, index=daily_returns.index)).cumprod() - 1\n",
    "\n",
    "# Save final cumulative returns explicitly\n",
    "cumulative_returns_df.to_csv('cumulative_returns_all_runs.csv')\n",
    "\n",
    "print(cumulative_returns_df.head())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-02T21:34:39.159236Z",
     "start_time": "2025-08-02T21:34:14.134560Z"
    }
   },
   "id": "34e9915a64b4d530",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data explicitly\n",
    "file_path = \"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/results/consolidated_hierarchical_results.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Identify stock columns (assuming columns BA to VZ inclusive)\n",
    "start_col = 'BA'\n",
    "end_col = 'VZ'\n",
    "stock_columns = df.loc[:, start_col:end_col].columns.tolist()\n",
    "\n",
    "# Perform grouping by 'Run' and 'Rebalance_Date' and calculate average\n",
    "grouped_df = df.groupby(['Rebalance_Date'])[stock_columns].mean().reset_index()\n",
    "\n",
    "# Export to a new CSV file\n",
    "output_file = \"averaged_stock_allocations.csv\"\n",
    "grouped_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\" Averaged data successfully saved to '{output_file}'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c210396b0873fdcd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Load your existing data ===\n",
    "shap_metrics = pd.read_csv('shap_value_metrics_export.csv', parse_dates=['Start_Date', 'End_Date'])\n",
    "daily_regimes = pd.read_csv('final_merged_data_with_forecast.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# Extract relevant SHAP features for plotting\n",
    "shap_features = [\n",
    "    'mean_abs_shap_Mkt-RF',  # Market factor absolute SHAP\n",
    "    'shap_std_Mkt-RF',        # Market factor SHAP dispersion\n",
    "    'mean_abs_shap_SMB',      # Size factor absolute SHAP (as an example)\n",
    "    'shap_std_SMB',           # Size factor SHAP dispersion (as an example)\n",
    "]\n",
    "\n",
    "# Aggregate daily SHAP features clearly\n",
    "daily_shap = daily_regimes[shap_features].copy()\n",
    "daily_shap.index = pd.to_datetime(daily_shap.index)\n",
    "\n",
    "# Resample SHAP metrics to weekly averages for smoothness (optional)\n",
    "weekly_shap = daily_shap.resample('W').mean()\n",
    "\n",
    "# Get regime decisions (high-level PPO decisions)\n",
    "weekly_regimes = daily_regimes['Cluster'].resample('W').last()\n",
    "\n",
    "# === Plotting ===\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot Market factor mean absolute SHAP (indicator of market influence)\n",
    "ax1.plot(\n",
    "    weekly_shap.index,\n",
    "    weekly_shap['mean_abs_shap_Mkt-RF'],\n",
    "    label='Market Factor Abs SHAP (Influence Strength)',\n",
    "    color='royalblue'\n",
    ")\n",
    "ax1.set_ylabel('Market Factor Abs SHAP', color='royalblue')\n",
    "ax1.tick_params(axis='y', labelcolor='royalblue')\n",
    "\n",
    "# Plot Market factor SHAP dispersion (indicator of volatility)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    weekly_shap.index,\n",
    "    weekly_shap['shap_std_Mkt-RF'],\n",
    "    label='Market Factor SHAP Std (Volatility)',\n",
    "    color='gray'\n",
    ")\n",
    "ax2.set_ylabel('Market Factor SHAP Std', color='gray')\n",
    "ax2.tick_params(axis='y', labelcolor='gray')\n",
    "\n",
    "# Add vertical regime decision lines explicitly\n",
    "for date, regime in weekly_regimes.items():\n",
    "    if regime == 1:  # \"Good\" or Bull regime (customize based on your regime label meaning)\n",
    "        ax1.axvline(x=date, color='green', linestyle='--', alpha=0.3, linewidth=2)\n",
    "    elif regime == 0:  # \"Bad\" or Bear regime\n",
    "        ax1.axvline(x=date, color='red', linestyle='--', alpha=0.3, linewidth=2)\n",
    "\n",
    "# Explicitly annotate notable periods (e.g., 2020 sell-off)\n",
    "ax1.axvspan('2020-02-01', '2020-04-30', color='red', alpha=0.1, label='2020 Market Selloff')\n",
    "ax1.axvspan('2020-05-01', '2020-12-31', color='green', alpha=0.1, label='Post-stimulus Recovery')\n",
    "\n",
    "# Titles and Legends\n",
    "fig.suptitle('High-Level PPO Regime Decisions Interpreted by SHAP Signals', fontsize=16)\n",
    "ax1.set_xlabel('Date')\n",
    "\n",
    "# Construct a single legend combining lines and vertical regime lines\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "# Tight layout for clarity\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"regime_shap_interpretation.png\", dpi=300)\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fe562f6a940d499",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b26da3b6ae907495"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "30bcb8c14e88ec27"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "96198cc9e47c45cb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8547ac5c980f9dda"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1dada654d4dfb0e7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "45958b710b89cbb6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5c4bb705a5ccda10"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4d64a4f2d2c8dd56"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9d7e978ae98160ee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3d8b6daeffcdea6f"
  },
  {
   "cell_type": "raw",
   "source": [
    "OLD LOGIC "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4fd4eed4dbc1aff"
  },
  {
   "cell_type": "raw",
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class RegimeSelectorEnv(gym.Env):\n",
    "    def __init__(self, regime_features_df, raw_future_returns_df):\n",
    "        super().__init__()\n",
    "        self.regime_features = regime_features_df.values\n",
    "        self.raw_future_returns = raw_future_returns_df.values  # Forward raw returns for evaluating regimes\n",
    "        self.current_step = 0\n",
    "        self.n_regimes = 2  # Adjust based on your clustering choice\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.n_regimes)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, \n",
    "            shape=(self.regime_features.shape[1],),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.regime_features[self.current_step]\n",
    "\n",
    "    def step(self, action):\n",
    "        # Reward calculated explicitly using raw future returns\n",
    "        reward = np.mean(self.raw_future_returns[self.current_step])\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.regime_features) - 1\n",
    "        \n",
    "        obs = self.regime_features[self.current_step] if not done else np.zeros(self.observation_space.shape)\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "\n",
    "class RegimeConditionedPortfolioEnv(gym.Env):\n",
    "    def __init__(self, returns_df, chosen_regime_vector, raw_returns_df):\n",
    "        super().__init__()\n",
    "        self.stock_returns = returns_df.values\n",
    "        self.chosen_regime_vector = chosen_regime_vector  # Regime indicator (chosen by high-level model)\n",
    "        self.raw_stock_returns = raw_returns_df.values\n",
    "        self.current_step = 0\n",
    "        self.n_assets = self.stock_returns.shape[1]\n",
    "\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.n_assets + 1,),  # returns + chosen regime\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        returns = self.stock_returns[self.current_step]\n",
    "        regime = np.array([self.chosen_regime_vector[self.current_step]])\n",
    "        obs = np.concatenate([returns, regime])\n",
    "        return np.nan_to_num(obs)\n",
    "\n",
    "    def step(self, action):\n",
    "        action_sum = action.sum()\n",
    "        action = action / action_sum if action_sum != 0 else np.ones_like(action) / len(action)\n",
    "\n",
    "        # Explicitly use raw returns for reward calculation\n",
    "        reward = np.dot(action, self.raw_stock_returns[self.current_step + 1])\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.stock_returns) - 1\n",
    "\n",
    "        obs = self._get_obs() if not done else np.zeros(self.observation_space.shape)\n",
    "        reward = 0.0 if np.isnan(reward) else reward\n",
    "        return obs, reward, done, {}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cee97d703e01592"
  },
  {
   "cell_type": "raw",
   "source": [
    "Step 3: Rolling Training and Out-of-Sample Testing (Hierarchical PPO)\n",
    "\n",
    "Here's how you train and test your hierarchical PPO model explicitly using rolling windows:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c62e3ceb4c404943"
  },
  {
   "cell_type": "raw",
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "model_save_dir = 'E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/trained_hierarchical_model/'\n",
    "\n",
    "train_window, test_window = 252, 63\n",
    "rebalance_freq = 21\n",
    "# use_factor_returns = False  # Set this flag as needed\n",
    "tickers = stock_returns_norm.columns.tolist()\n",
    "regime_columns = daily_regimes.columns.tolist()\n",
    "factor_columns = factor_returns_norm.columns.tolist() if use_factor_returns else []\n",
    "records = []\n",
    "\n",
    "total_windows = len(range(0, len(merged_data) - train_window - test_window, test_window))\n",
    "\n",
    "for i, start in enumerate(range(0, len(merged_data) - train_window - test_window, test_window)):\n",
    "    print(f\"\\n{'='*30}\\nStarting Hierarchical Training Window {i+1}/{total_windows}\")\n",
    "\n",
    "    train_df = merged_data.iloc[start:start + train_window]\n",
    "    test_df = merged_data.iloc[start + train_window:start + train_window + test_window + 1]\n",
    "    raw_returns_train = raw_stock_returns.loc[train_df.index]\n",
    "    raw_returns_test = raw_stock_returns.loc[test_df.index]\n",
    "\n",
    "    if train_df.isna().values.any() or test_df.isna().values.any():\n",
    "        print(f\"Skipping window {i+1} due to NaNs.\")\n",
    "        continue\n",
    "\n",
    "    # ----- High-Level Regime Selector Training -----\n",
    "    high_env = DummyVecEnv([\n",
    "        lambda: RegimeSelectorEnv(train_df[regime_columns], raw_returns_train)\n",
    "    ])\n",
    "    print(\"Training High-Level Regime Selector...\")\n",
    "    high_model = PPO('MlpPolicy', high_env, verbose=0)\n",
    "    high_model.learn(total_timesteps=10000)\n",
    "    high_model_path = os.path.join(model_save_dir, f\"high_model_window_{i}.zip\")\n",
    "    high_model.save(high_model_path)\n",
    "    print(f\"High-level model saved at: {high_model_path}\")\n",
    "\n",
    "    # ----- Train Low-Level Models per Historical Regime -----\n",
    "    low_level_models = {}\n",
    "    unique_train_regimes = train_df['Cluster'].unique()\n",
    "\n",
    "    for regime in unique_train_regimes:\n",
    "        regime_df = train_df[train_df['Cluster'] == regime]\n",
    "        chosen_regime_vector_train = np.repeat(regime, len(regime_df))\n",
    "\n",
    "        low_env_train = DummyVecEnv([\n",
    "            lambda: RegimeConditionedPortfolioEnv(\n",
    "                returns_df=regime_df[tickers],\n",
    "                chosen_regime_vector=chosen_regime_vector_train,\n",
    "                raw_returns_df=raw_returns_train.loc[regime_df.index]\n",
    "            )\n",
    "        ])\n",
    "        low_model = PPO('MlpPolicy', low_env_train, verbose=0)\n",
    "        low_model.learn(total_timesteps=10000)\n",
    "        low_model_path = os.path.join(model_save_dir, f\"low_model_window_{i}_regime_{regime}.zip\")\n",
    "        low_model.save(low_model_path)\n",
    "        low_level_models[regime] = low_model\n",
    "        print(f\"Low-level model for regime {regime} saved at: {low_model_path}\")\n",
    "\n",
    "    # ----- Testing -----\n",
    "    test_dates = test_df.index\n",
    "    test_returns = test_df[tickers].values\n",
    "    test_regimes = test_df[regime_columns].values\n",
    "\n",
    "    for rebalance_num, rebalance_start in enumerate(range(0, test_window, rebalance_freq)):\n",
    "        rebalance_end = min(rebalance_start + rebalance_freq, test_window)\n",
    "\n",
    "        high_obs = test_regimes[rebalance_start]\n",
    "        ## choose random and average over 200 samples\n",
    "        chosen_regime, _ = high_model.predict(high_obs, deterministic=True)\n",
    "        chosen_regime = int(chosen_regime.item())\n",
    "        # high_predictions = [high_model.predict(high_obs, deterministic=False)[0].item() for _ in range(200)]\n",
    "        # chosen_regime = int(pd.Series(high_predictions).mode()[0])  # take most common predicted regime\n",
    "\n",
    "        \n",
    "        chosen_regime_vector_test = np.repeat(chosen_regime, rebalance_end - rebalance_start)\n",
    "\n",
    "        if chosen_regime in low_level_models:\n",
    "            low_model = low_level_models[chosen_regime]\n",
    "            low_env_test = RegimeConditionedPortfolioEnv(\n",
    "                returns_df=pd.DataFrame(test_returns[rebalance_start:rebalance_end], columns=tickers),\n",
    "                chosen_regime_vector=chosen_regime_vector_test,\n",
    "                raw_returns_df=raw_returns_test.iloc[rebalance_start:rebalance_end]\n",
    "            )\n",
    "\n",
    "            low_obs = low_env_test.reset()\n",
    "             ## choose random and average over 200 samples\n",
    "            action, _ = low_model.predict(low_obs, deterministic=True)\n",
    "            action /= action.sum()\n",
    "            # action_samples = np.array([\n",
    "            #     low_model.predict(low_obs, deterministic=False)[0]\n",
    "            #     for _ in range(200)\n",
    "            # ])\n",
    "            # action_mean = action_samples.mean(axis=0)\n",
    "            # action_mean /= action_mean.sum()  # normalization\n",
    "            # action = action_mean\n",
    "            \n",
    "        else:\n",
    "            action = np.ones(len(tickers)) / len(tickers)\n",
    "\n",
    "        portfolio_value = 1.0\n",
    "        for step in range(rebalance_end - rebalance_start - 1):\n",
    "            reward = np.dot(action, raw_returns_test.iloc[rebalance_start + step + 1])\n",
    "            portfolio_value *= (1 + reward)\n",
    "        cumulative_reward = portfolio_value - 1.0\n",
    "\n",
    "        weights_dict = {ticker: round(weight, 4) for ticker, weight in zip(tickers, action)}\n",
    "        records.append({\n",
    "            'Rebalance_Date': test_dates[rebalance_start],\n",
    "            'Window': i,\n",
    "            'Rebalance_Number': rebalance_num,\n",
    "            'Chosen_Regime': chosen_regime,\n",
    "            'Holding_Period_Start': test_dates[rebalance_start],\n",
    "            'Holding_Period_End': test_dates[rebalance_end - 1],\n",
    "            'Cumulative_Reward': cumulative_reward,\n",
    "            **weights_dict\n",
    "        })\n",
    "\n",
    "        print(f\"Cumulative reward: {cumulative_reward:.4f}\")\n",
    "\n",
    "result_df = pd.DataFrame(records)\n",
    "result_df['Cumulative_Return'] = result_df['Cumulative_Reward'].cumsum()\n",
    "result_df.to_csv('hierarchical_allocations_dates_rebalanced.csv', index=False)\n",
    "\n",
    "print(\"\\nHierarchical training/testing completed successfully.\")\n",
    "print(\"Final hierarchical results saved to 'hierarchical_allocations_dates_rebalanced.csv'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8125d5e9d9e88f0c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2044a2848f0119bf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a54d2668debf911b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "af248394e7e3c7f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c86be064a1e9705b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "271d7709439ea06c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "587d725b483c79fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a18ac95932480672"
  },
  {
   "cell_type": "raw",
   "source": [
    "# (1) Compute SHAP Regime Consistency (SRC)\n",
    "# \n",
    "# This metric measures how consistently SHAP values reflect the regime structure:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_src(shap_df, regime_labels):\n",
    "    \"\"\"\n",
    "    shap_df: DataFrame of SHAP values, shape [time, stock, factor]\n",
    "    regime_labels: Series/DataFrame with regime label per time period\n",
    "    \"\"\"\n",
    "    unique_dates = shap_df['End_Date'].unique()\n",
    "    src_values = []\n",
    "\n",
    "    for date in unique_dates:\n",
    "        # SHAP matrix S_t for current date\n",
    "        S_t_df = shap_df[shap_df['End_Date'] == date]\n",
    "        factors = ['mean_abs_shap_Mkt-RF', 'mean_abs_shap_SMB', 'mean_abs_shap_HML',\n",
    "                   'mean_abs_shap_RMW', 'mean_abs_shap_CMA']\n",
    "        S_t = S_t_df[factors].values\n",
    "\n",
    "        regime = regime_labels.loc[date]\n",
    "\n",
    "        # SHAP matrix S_t|r (average SHAP in regime)\n",
    "        regime_dates = regime_labels[regime_labels == regime].index\n",
    "        S_r_df = shap_df[shap_df['End_Date'].isin(regime_dates)]\n",
    "        S_r = S_r_df[factors].values.mean(axis=0)\n",
    "\n",
    "        # Compute SRC numerator and denominator\n",
    "        numerator = np.trace(S_t @ S_t.T)\n",
    "        denominator = np.linalg.norm(S_t, 'fro') * np.linalg.norm(S_r, 'fro')\n",
    "        src = numerator / denominator if denominator != 0 else 0\n",
    "        src_values.append(src)\n",
    "\n",
    "    return np.mean(src_values)\n",
    "\n",
    "# Example call:\n",
    "# regime_labels: pd.Series(index=dates, data=regime numbers)\n",
    "# shap_metrics: loaded previously\n",
    "# SRC_result = compute_src(shap_metrics, regime_labels)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9839f11da2a8f98f"
  },
  {
   "cell_type": "raw",
   "source": [
    "# (2) Compute Explanation Risk Premium (ERP)\n",
    "# \n",
    "# This metric assesses the returns premium tied to higher SHAP explanation stability:\n",
    "def compute_erp(shap_df, returns_df):\n",
    "    \"\"\"\n",
    "    shap_df: DataFrame containing Explanation Stability Index (psi) per stock per date\n",
    "    returns_df: DataFrame containing stock returns, same dates/stocks aligned\n",
    "    \"\"\"\n",
    "    erp_values = []\n",
    "    stocks = shap_df['Stock'].unique()\n",
    "\n",
    "    for stock in stocks:\n",
    "        # Filter data for the current stock\n",
    "        stock_shap = shap_df[shap_df['Stock'] == stock].copy()\n",
    "        stock_returns = returns_df[stock].loc[stock_shap['End_Date']].values\n",
    "        \n",
    "        # Stability index psi for the current stock\n",
    "        psi = stock_shap['mean_abs_over_std_Stock'].values\n",
    "\n",
    "        # Compute thresholds\n",
    "        psi_75 = np.percentile(psi, 75)\n",
    "        psi_25 = np.percentile(psi, 25)\n",
    "\n",
    "        # Returns conditioned on psi\n",
    "        high_psi_returns = stock_returns[psi > psi_75]\n",
    "        low_psi_returns = stock_returns[psi < psi_25]\n",
    "\n",
    "        # Calculate mean returns, handle division by zero\n",
    "        mean_high = np.mean(high_psi_returns) if len(high_psi_returns) > 0 else 0\n",
    "        mean_low = np.mean(low_psi_returns) if len(low_psi_returns) > 0 else 1e-8  # Avoid zero division\n",
    "\n",
    "        # ERP for current stock\n",
    "        erp_stock = (mean_high / mean_low) - 1\n",
    "        erp_values.append(erp_stock)\n",
    "\n",
    "    return np.mean(erp_values)\n",
    "\n",
    "# Example call:\n",
    "# returns_df: DataFrame [date  stock] returns loaded previously\n",
    "# shap_df: DataFrame loaded previously\n",
    "# ERP_result = compute_erp(shap_metrics, stock_returns)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "772ccc93fd01c00a"
  },
  {
   "cell_type": "raw",
   "source": [
    "# Use the normalized SHAP features explicitly for validation metrics calculation:\n",
    "# Ensure regime labels are correctly aligned after clustering\n",
    "regime_labels = regime_features_norm['Regime']\n",
    "\n",
    "# Compute SHAP Regime Consistency (SRC) with normalized SHAP metrics\n",
    "SRC_result = compute_src(shap_features_norm_df, regime_labels)\n",
    "print(\"Normalized SHAP Regime Consistency (SRC):\", SRC_result)\n",
    "\n",
    "# Compute Explanation Risk Premium (ERP) using normalized data\n",
    "ERP_result = compute_erp(shap_features_norm_df, stock_returns_norm)\n",
    "print(\"Normalized Explanation Risk Premium (ERP):\", ERP_result)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b18b3b06a0149fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
