{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "895e7096145459d4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "adb5bfdfc9c419d8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# stage 1 training and prediction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import ta\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 0. Load and align data\n",
    "# ------------------------------------------------------------------------\n",
    "factors = pd.read_csv(\"aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns = pd.read_csv(\"daily_returns_10ETFs.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Align dates to ensure matching indices\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors = factors.loc[dates]\n",
    "returns = returns.loc[dates]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1. Compute technical indicators and lagged features per ETF\n",
    "# ------------------------------------------------------------------------\n",
    "all_tech_features = []\n",
    "\n",
    "for etf in returns.columns:\n",
    "    close = (1 + returns[etf]).cumprod()\n",
    "    tech_df = pd.DataFrame(index=returns.index)\n",
    "\n",
    "    # Selected indicators (others commented out to reduce noise)\n",
    "    tech_df[f'{etf}_SMA_5']   = ta.trend.sma_indicator(close, window=5)\n",
    "    tech_df[f'{etf}_EMA_12']  = ta.trend.ema_indicator(close, window=12)\n",
    "    tech_df[f'{etf}_RSI_7']   = ta.momentum.rsi(close, window=7)\n",
    "    tech_df[f'{etf}_MACD']    = ta.trend.macd_diff(close)\n",
    "    tech_df[f'{etf}_ATR']     = ta.volatility.average_true_range(\n",
    "        high=close * 1.01, low=close * 0.99, close=close, window=10\n",
    "    )\n",
    "    tech_df[f'{etf}_Vol_5']   = returns[etf].rolling(window=5).std()\n",
    "    tech_df[f'{etf}_Mom_3']   = returns[etf].rolling(window=3).mean()\n",
    "\n",
    "    # Lagged returns (shifted so only past information is used)\n",
    "    for lag in [1, 2, 3]:\n",
    "        tech_df[f'{etf}_LagRet_{lag}'] = returns[etf].shift(lag)\n",
    "\n",
    "    all_tech_features.append(tech_df)\n",
    "\n",
    "# Concatenate technical indicators for all ETFs\n",
    "technical_features = pd.concat(all_tech_features, axis=1)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2. Create lagged factor features\n",
    "# ------------------------------------------------------------------------\n",
    "for factor in ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']:\n",
    "    for lag in [1, 2, 3]:\n",
    "        factors[f'{factor}_lag_{lag}'] = factors[factor].shift(lag)\n",
    "\n",
    "# Drop rows with NA values arising from lagging\n",
    "factors = factors.dropna()\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3. Combine factors, technical features, and VIX change\n",
    "# ------------------------------------------------------------------------\n",
    "features = pd.concat([factors, technical_features], axis=1).dropna()\n",
    "vix = pd.read_csv(\"VIX_History.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Align VIX to our feature dates and compute lagged change\n",
    "vix_aligned = vix['CLOSE'].reindex(features.index).ffill()\n",
    "features['VIX'] = vix_aligned.pct_change(fill_method=None).shift(1)\n",
    "features['VIX'] = features['VIX'].fillna(0)\n",
    "\n",
    "# Define the target: next-day return per ETF\n",
    "target_returns = returns.shift(-1).loc[features.index].dropna()\n",
    "features = features.loc[target_returns.index]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4. Define rolling window parameters\n",
    "# ------------------------------------------------------------------------\n",
    "train_years = 12      # years used for training\n",
    "valid_years = 1       # years used for validation\n",
    "test_years  = 1       # years used for testing/prediction\n",
    "retrain_frequency = 1 # years between retrainings\n",
    "start_year = 2009\n",
    "end_year   = 2024\n",
    "\n",
    "# List generic features used for SHAP importance ranking\n",
    "all_generic_features = [\n",
    "    'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA',\n",
    "    'Mkt-RF_lag_1', 'Mkt-RF_lag_2', 'Mkt-RF_lag_3',\n",
    "    'SMB_lag_1', 'SMB_lag_2', 'SMB_lag_3',\n",
    "    'HML_lag_1', 'HML_lag_2', 'HML_lag_3',\n",
    "    'RMW_lag_1', 'RMW_lag_2', 'RMW_lag_3',\n",
    "    'CMA_lag_1', 'CMA_lag_2', 'CMA_lag_3',\n",
    "    'SMA_5', 'EMA_12', 'RSI_7', 'MACD',\n",
    "    'Vol_5', 'Mom_3',\n",
    "    'LagRet_1', 'LagRet_2', 'LagRet_3', 'VIX'\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 5. Compute generic feature importance via SHAP\n",
    "#    (aggregated across ETFs, using only the initial training window)\n",
    "# ------------------------------------------------------------------------\n",
    "shap_importances = pd.DataFrame(0.0, index=all_generic_features, columns=['SHAP_Value'])\n",
    "\n",
    "# Use a fixed period (e.g. up to year 2009) for computing importances\n",
    "base_train_start = pd.Timestamp(start_year - train_years, 1, 1)\n",
    "base_train_end   = pd.Timestamp(start_year - valid_years - 1, 12, 31)\n",
    "\n",
    "for etf in returns.columns:\n",
    "    print(f\"Computing SHAP importances for ETF: {etf}\")\n",
    "    # Filter columns relevant to this ETF (generic + factor features)\n",
    "    etf_cols = [\n",
    "        col for col in features.columns\n",
    "        if (etf in col and any(k in col for k in ['SMA_5', 'EMA_12', 'RSI_7',\n",
    "                                                  'MACD', 'Vol_5', 'Mom_3',\n",
    "                                                  'LagRet_1','LagRet_2','LagRet_3', 'VIX']))\n",
    "        or col in ['Mkt-RF','SMB','HML','RMW','CMA',\n",
    "                   'Mkt-RF_lag_1','Mkt-RF_lag_2','Mkt-RF_lag_3',\n",
    "                   'SMB_lag_1','SMB_lag_2','SMB_lag_3',\n",
    "                   'HML_lag_1','HML_lag_2','HML_lag_3',\n",
    "                   'RMW_lag_1','RMW_lag_2','RMW_lag_3',\n",
    "                   'CMA_lag_1','CMA_lag_2','CMA_lag_3']\n",
    "    ]\n",
    "    X_base  = features.loc[base_train_start:base_train_end, etf_cols]\n",
    "    y_base  = target_returns[etf].loc[base_train_start:base_train_end]\n",
    "\n",
    "    # Fit a quick model to compute SHAP\n",
    "    model_base = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        tree_method='hist',\n",
    "        random_state=42,\n",
    "        device='cuda'\n",
    "    ).fit(X_base, y_base)\n",
    "\n",
    "    explainer_base = shap.Explainer(model_base)\n",
    "    shap_vals = explainer_base(X_base)\n",
    "\n",
    "    # Aggregate SHAP values per generic feature\n",
    "    for gen_feat in all_generic_features:\n",
    "        cols = [c for c in X_base.columns if gen_feat in c]\n",
    "        if cols:\n",
    "            idx = [X_base.columns.get_loc(c) for c in cols]\n",
    "            shap_importances.loc[gen_feat] += np.mean(np.abs(shap_vals.values[:, idx]))\n",
    "\n",
    "# Average importance across ETFs and select top N\n",
    "shap_importances /= len(returns.columns)\n",
    "top_generic_features = (\n",
    "    shap_importances.sort_values('SHAP_Value', ascending=False)\n",
    "                    .head(10)\n",
    "                    .index\n",
    "                    .tolist()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 6. Retrain models using the selected generic features in rolling windows\n",
    "# ------------------------------------------------------------------------\n",
    "all_predictions = []\n",
    "\n",
    "for etf in returns.columns:\n",
    "    print(f\"\\n==== Training models for ETF: {etf} ====\")\n",
    "    # Select columns containing any of the top_generic_features or factor names\n",
    "    selected_features = [\n",
    "        f for f in features.columns\n",
    "        if any(gen in f for gen in top_generic_features) or f in factors.columns\n",
    "    ]\n",
    "\n",
    "    year = start_year\n",
    "    while year <= end_year - test_years + 1:\n",
    "        print(f\"\\nTraining window starting {year}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Define periods\n",
    "        train_start = pd.Timestamp(year - train_years, 1, 1)\n",
    "        train_end   = pd.Timestamp(year - valid_years - 1, 12, 31)\n",
    "        valid_start = pd.Timestamp(year - valid_years, 1, 1)\n",
    "        valid_end   = pd.Timestamp(year - 1, 12, 31)\n",
    "        test_start  = pd.Timestamp(year, 1, 1)\n",
    "        test_end    = pd.Timestamp(year + test_years - 1, 12, 31)\n",
    "\n",
    "        # Extract data\n",
    "        X_train = features.loc[train_start:train_end, selected_features]\n",
    "        y_train = target_returns[etf].loc[train_start:train_end]\n",
    "        X_valid = features.loc[valid_start:valid_end, selected_features]\n",
    "        y_valid = target_returns[etf].loc[valid_start:valid_end]\n",
    "        X_test  = features.loc[test_start:test_end, selected_features]\n",
    "        y_test  = target_returns[etf].loc[test_start:test_end]\n",
    "\n",
    "        # Base model with early stopping\n",
    "        base_model = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            tree_method='hist',\n",
    "            random_state=42,\n",
    "            n_jobs=4\n",
    "        )\n",
    "\n",
    "        param_grid = {\n",
    "            'n_estimators': [200, 400],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'learning_rate': [0.03, 0.05],\n",
    "            'subsample': [0.7, 0.8],\n",
    "            'colsample_bytree': [0.7, 0.8]\n",
    "        }\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            base_model,\n",
    "            param_grid,\n",
    "            cv=tscv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            verbose=0,\n",
    "            n_jobs=4\n",
    "        )\n",
    "\n",
    "        # Fit with early stopping on the explicit validation set\n",
    "        grid_search.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            eval_metric='rmse',\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Predict on the test period\n",
    "        preds = best_model.predict(X_test)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        mse  = mean_squared_error(y_test, preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae  = mean_absolute_error(y_test, preds)\n",
    "        r2   = r2_score(y_test, preds)\n",
    "        dir_acc = np.mean((np.sign(y_test) == np.sign(preds)).astype(int))\n",
    "\n",
    "        print(f\"MSE: {mse:.6f}  RMSE: {rmse:.6f}  MAE: {mae:.6f}  \"\n",
    "              f\"R²: {r2:.6f}  DirAcc: {dir_acc:.2%}\")\n",
    "\n",
    "        # Save the model for reproducibility\n",
    "        joblib.dump(best_model, f\"best_model_{etf}_{year}.joblib\")\n",
    "\n",
    "        # Save predictions\n",
    "        preds_df = pd.DataFrame({\n",
    "            'Date': X_test.index,\n",
    "            'ETF': etf,\n",
    "            'Year': year,\n",
    "            'Actual_Return': y_test,\n",
    "            'Predicted_Return': preds\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "        # Compute SHAP values on the test set\n",
    "        explainer_test = shap.Explainer(best_model)\n",
    "        shap_vals_test = explainer_test(X_test)\n",
    "\n",
    "        shap_df = pd.DataFrame(\n",
    "            shap_vals_test.values,\n",
    "            columns=[f'SHAP_{col}' for col in X_test.columns],\n",
    "            index=X_test.index\n",
    "        ).reset_index().rename(columns={'index': 'Date'})\n",
    "\n",
    "        # Merge SHAP values with predictions\n",
    "        preds_df = preds_df.merge(shap_df, on='Date', how='left')\n",
    "\n",
    "        all_predictions.append(preds_df)\n",
    "\n",
    "        # Advance the window\n",
    "        year += retrain_frequency\n",
    "        print(f\"Window processed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Concatenate and save all predictions and SHAP values\n",
    "final_predictions_df = pd.concat(all_predictions, ignore_index=True)\n",
    "final_predictions_df.to_csv(\"stage1_predictions_with_shap_10ETFs.csv\", index=False)\n",
    "\n",
    "print(\"Stage 1 completed and data saved for Stage 2.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f0500c59fdc3167",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ff812fee4b59a927",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "top_generic_features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "110622934baf18b4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6efb3e1a4faf0b68",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load stage 1 predictions with SHAP values explicitly\n",
    "stage1_df = pd.read_csv(\"stage1_predictions_with_shap_10ETFs.csv\", parse_dates=['Date'])\n",
    "etfs = stage1_df['ETF'].unique()\n",
    "\n",
    "# Initialize DataFrame explicitly for aggregated daily data\n",
    "dates = sorted(stage1_df['Date'].unique())\n",
    "aggregated_data = pd.DataFrame({'Date': dates})\n",
    "\n",
    "# Pivot tables for efficient cross-sectional computations\n",
    "predicted_returns = stage1_df.pivot(index='Date', columns='ETF', values='Predicted_Return')\n",
    "actual_returns = stage1_df.pivot(index='Date', columns='ETF', values='Actual_Return')\n",
    "\n",
    "# Compute ETF-specific volatility (rolling 5-day window)\n",
    "volatility = actual_returns.rolling(window=5).std()\n",
    "\n",
    "# Merge explicitly into aggregated_data\n",
    "for etf in etfs:\n",
    "    aggregated_data[f'Predicted_Return_{etf}'] = aggregated_data['Date'].map(predicted_returns[etf])\n",
    "    aggregated_data[f'Actual_Return_{etf}'] = aggregated_data['Date'].map(actual_returns[etf])\n",
    "    aggregated_data[f'Volatility_{etf}'] = aggregated_data['Date'].map(volatility[etf])\n",
    "\n",
    "# Dynamically load the top generic features from Stage 1 explicitly to maintain consistency\n",
    "generic_shap_features = ['Vol_5',\n",
    " 'SMB_lag_2',\n",
    " 'Mom_3',\n",
    " 'HML_lag_2',\n",
    " 'SMA_5',\n",
    " 'LagRet_1',\n",
    " 'EMA_12',\n",
    " 'SMB',\n",
    " 'RSI_7',\n",
    " 'CMA_lag_1']\n",
    "\n",
    "# Aggregate SHAP values (mean and std across ETFs) explicitly by generic feature\n",
    "shap_aggregated_features = {}\n",
    "\n",
    "for feature in generic_shap_features:\n",
    "    matching_cols = [col for col in stage1_df.columns if col.startswith('SHAP_') and col.endswith(f'_{feature}')]\n",
    "\n",
    "    if matching_cols:\n",
    "        shap_means = stage1_df.groupby('Date')[matching_cols].mean().mean(axis=1)\n",
    "        shap_stds = stage1_df.groupby('Date')[matching_cols].std().mean(axis=1)\n",
    "\n",
    "        shap_aggregated_features[f'Avg_SHAP_{feature}'] = shap_means\n",
    "        shap_aggregated_features[f'Std_SHAP_{feature}'] = shap_stds\n",
    "    else:\n",
    "        print(f\"Warning: No matches found for SHAP feature: {feature}\")\n",
    "\n",
    "# Convert aggregated SHAP features explicitly to DataFrame\n",
    "shap_aggregated_df = pd.DataFrame(shap_aggregated_features).reset_index()\n",
    "\n",
    "# Merge aggregated SHAP features explicitly\n",
    "aggregated_data = pd.merge(aggregated_data, shap_aggregated_df, on='Date', how='left')\n",
    "\n",
    "# Explicitly compute additional cross-sectional signals for richer Stage 2 observations\n",
    "# Cross-sectional mean and std of predicted returns\n",
    "aggregated_data['CrossSec_Mean_PredRet'] = predicted_returns.mean(axis=1).values\n",
    "aggregated_data['CrossSec_Std_PredRet'] = predicted_returns.std(axis=1).values\n",
    "\n",
    "# Cross-sectional mean volatility\n",
    "aggregated_data['CrossSec_Mean_Volatility'] = volatility.mean(axis=1).values\n",
    "\n",
    "# Rank ETFs by predicted return explicitly (percentile ranks)\n",
    "ranked_preds = predicted_returns.rank(axis=1, pct=True)\n",
    "for etf in etfs:\n",
    "    aggregated_data[f'Rank_PredRet_{etf}'] = aggregated_data['Date'].map(ranked_preds[etf])\n",
    "\n",
    "# Handle missing values explicitly and robustly:\n",
    "# Forward-fill only SHAP and cross-sectional features explicitly\n",
    "shap_and_crosssec_cols = [col for col in aggregated_data.columns if 'SHAP' in col or 'CrossSec' in col]\n",
    "aggregated_data[shap_and_crosssec_cols] = aggregated_data[shap_and_crosssec_cols].ffill()\n",
    "\n",
    "# Drop rows explicitly where ETF volatility calculations have initial NaNs\n",
    "vol_cols = [f'Volatility_{etf}' for etf in etfs]\n",
    "aggregated_data.dropna(subset=vol_cols, inplace=True)\n",
    "\n",
    "# Final sanity checks explicitly for data quality assurance\n",
    "if aggregated_data.empty:\n",
    "    raise ValueError(\"Aggregated dataset is empty after preprocessing. Verify your input data.\")\n",
    "else:\n",
    "    # Quick summary statistics explicitly for diagnostics\n",
    "    print(\"Aggregated DataFrame shape:\", aggregated_data.shape)\n",
    "    print(\"Aggregated DataFrame summary stats:\")\n",
    "    print(aggregated_data.describe().transpose())\n",
    "\n",
    "    # Save optimized data explicitly for Stage 2\n",
    "    aggregated_data.to_csv(\"stage2_rl_observations_optimized_10ETFs.csv\", index=False)\n",
    "    print(\"Optimized Stage 2 RL dataset successfully saved.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "626373434f64d0c4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import random\n",
    "# from dataclasses import dataclass\n",
    "# from typing import Dict, List, Tuple\n",
    "# \n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import ParameterSampler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from scipy.stats import ttest_1samp\n",
    "# \n",
    "# import torch\n",
    "# from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "# import gymnasium as gym\n",
    "# from gymnasium import spaces\n",
    "# \n",
    "# # ---------------------------------------------------------------------\n",
    "# # Configuration dataclass\n",
    "# # ---------------------------------------------------------------------\n",
    "# @dataclass\n",
    "# class TrainingConfig:\n",
    "#     train_window_days: int = 252 * 10      # 10 years for training\n",
    "#     validation_window_days: int = 126      # ~6 months for validation\n",
    "#     prediction_window_days: int = 126      # ~6 months for prediction\n",
    "#     lookback_period: int = 10              # lookback for observations\n",
    "#     rebalance_period: int = 10             # rebalance every 10 days\n",
    "#     n_iter_tuning: int = 8                 # number of hyperparameter samples\n",
    "#     tuning_timesteps: int = 5_000          # timesteps for each tune\n",
    "#     incremental_timesteps: int = 3_000     # PPO training step size\n",
    "#     max_timesteps: int = 30_000            # maximum PPO timesteps\n",
    "#     patience: int = 3                      # early stopping patience\n",
    "#     policy_arch: Tuple[int, int] = (256, 256)  # network architecture\n",
    "#     num_iterations: int = 3                # number of outer iterations (seeds)\n",
    "#     base_seed: int = 42                    # base random seed\n",
    "#     default_risk_coeff: float = 0.5        # default risk coefficient\n",
    "# \n",
    "# # ---------------------------------------------------------------------\n",
    "# # Seed-setting utility\n",
    "# # ---------------------------------------------------------------------\n",
    "# def set_global_seed(seed: int) -> None:\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "# \n",
    "# # ---------------------------------------------------------------------\n",
    "# # Feature engineering\n",
    "# # ---------------------------------------------------------------------\n",
    "# def add_stable_features(df: pd.DataFrame, etf_list: List[str]) -> pd.DataFrame:\n",
    "#     data = df.copy()\n",
    "#     for etf in etf_list:\n",
    "#         price_col = f'Price_{etf}'\n",
    "#         data[f'Volatility_{etf}'] = data[price_col].pct_change().rolling(20).std()\n",
    "#         data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
    "#         data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
    "#         data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
    "#         data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
    "#         data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
    "#         data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
    "#     data.dropna(inplace=True)\n",
    "#     return data\n",
    "# \n",
    "# def filter_features(df: pd.DataFrame,\n",
    "#                     include_predicted_returns: bool = True,\n",
    "#                     include_shap_metrics: bool = True) -> pd.DataFrame:\n",
    "#     df_filtered = df.copy()\n",
    "#     if not include_predicted_returns:\n",
    "#         pred_cols = [c for c in df_filtered.columns if 'Predicted_Return' in c]\n",
    "#         df_filtered.drop(columns=pred_cols, inplace=True)\n",
    "#     if not include_shap_metrics:\n",
    "#         shap_cols = [c for c in df_filtered.columns if 'SHAP' in c]\n",
    "#         df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "#     return df_filtered\n",
    "# \n",
    "# # ---------------------------------------------------------------------\n",
    "# # Custom Gym environment\n",
    "# # ---------------------------------------------------------------------\n",
    "# class PortfolioEnv(gym.Env):\n",
    "#     metadata = {'render_modes': []}\n",
    "# \n",
    "#     def __init__(self, data: pd.DataFrame, etf_list: List[str],\n",
    "#                  reward_type: str = 'mean_cvar', risk_coefficient: float = 0.5,\n",
    "#                  rebalance_period: int = 21, lookback_period: int = 21):\n",
    "#         super().__init__()\n",
    "#         self.data = data.reset_index(drop=True)\n",
    "#         self.etf_list = etf_list\n",
    "#         self.reward_type = reward_type\n",
    "#         self.risk_coefficient = risk_coefficient\n",
    "#         self.rebalance_period = rebalance_period\n",
    "#         self.lookback_period = lookback_period\n",
    "#         self.action_space = spaces.Box(low=-1.0, high=1.0,\n",
    "#                                        shape=(len(etf_list),), dtype=np.float32)\n",
    "#         self.feature_cols = [c for c in data.columns\n",
    "#                              if c != 'Date' and not c.startswith('Actual_Return')]\n",
    "#         self.num_features_per_day = len(self.feature_cols)\n",
    "#         self.observation_space = spaces.Box(\n",
    "#             low=-np.inf, high=np.inf,\n",
    "#             shape=(self.num_features_per_day * lookback_period,),\n",
    "#             dtype=np.float32\n",
    "#         )\n",
    "#         self.current_step = lookback_period\n",
    "#         self.cumulative_wealth = 1.0\n",
    "#         self.current_weights = np.array([1.0 / len(etf_list)] * len(etf_list))\n",
    "# \n",
    "#     def reset(self, seed=None, options=None):\n",
    "#         super().reset(seed=seed)\n",
    "#         if seed is not None:\n",
    "#             np.random.seed(seed)\n",
    "#         self.current_step = self.lookback_period\n",
    "#         self.cumulative_wealth = 1.0\n",
    "#         self.current_weights = np.array([1.0 / len(self.etf_list)] * len(self.etf_list))\n",
    "#         return self._get_obs(), {}\n",
    "# \n",
    "#     def _get_obs(self):\n",
    "#         obs_window = self.data.iloc[self.current_step - self.lookback_period : self.current_step]\n",
    "#         return obs_window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "# \n",
    "#     def calculate_reward(self, portfolio_return, asset_returns):\n",
    "#         if self.reward_type == 'cumulative_return':\n",
    "#             return self.cumulative_wealth - 1.0\n",
    "#         elif self.reward_type == 'log_wealth':\n",
    "#             return np.log(self.cumulative_wealth)\n",
    "#         elif self.reward_type == 'mean_var':\n",
    "#             return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "#         elif self.reward_type == 'mean_cvar':\n",
    "#             alpha = 0.05\n",
    "#             var = np.percentile(asset_returns, 100 * alpha)\n",
    "#             cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "#             return portfolio_return - self.risk_coefficient * cvar\n",
    "#         else:\n",
    "#             raise ValueError(f\"Invalid reward type: {self.reward_type}\")\n",
    "# \n",
    "#     def step(self, action):\n",
    "#         next_step = self.current_step + 1\n",
    "#         # Rebalance on schedule\n",
    "#         if self.current_step % self.rebalance_period == 0:\n",
    "#             desired_long, desired_short = 1.20, 0.20\n",
    "#             clip_bounds = (-0.2, 0.8)\n",
    "#             raw = action.copy()\n",
    "#             long_w = np.maximum(raw, 0)\n",
    "#             short_w = np.abs(np.minimum(raw, 0))\n",
    "#             has_longs, has_shorts = (np.sum(long_w) > 0), (np.sum(short_w) > 0)\n",
    "#             if has_longs and has_shorts:\n",
    "#                 norm_long = desired_long * long_w / np.sum(long_w)\n",
    "#                 norm_short = desired_short * short_w / np.sum(short_w)\n",
    "#             elif has_longs and not has_shorts:\n",
    "#                 norm_long = long_w / np.sum(long_w); norm_short = np.zeros_like(short_w)\n",
    "#             elif not has_longs and has_shorts:\n",
    "#                 n = len(raw)\n",
    "#                 norm_long = np.ones(n) / n; norm_short = np.zeros(n)\n",
    "#             else:\n",
    "#                 n = len(raw)\n",
    "#                 norm_long = np.ones(n) / n; norm_short = np.zeros(n)\n",
    "#             combined = norm_long - norm_short\n",
    "#             clipped = np.clip(combined, clip_bounds[0], clip_bounds[1])\n",
    "#             long_c = np.maximum(clipped, 0.0)\n",
    "#             short_c = np.abs(np.minimum(clipped, 0.0))\n",
    "#             has_long_c, has_short_c = (np.sum(long_c) > 0), (np.sum(short_c) > 0)\n",
    "#             if has_long_c and has_short_c:\n",
    "#                 final_long = desired_long * long_c / np.sum(long_c)\n",
    "#                 final_short = desired_short * short_c / np.sum(short_c)\n",
    "#             elif has_long_c and not has_short_c:\n",
    "#                 final_long = long_c / np.sum(long_c); final_short = np.zeros_like(short_c)\n",
    "#             else:\n",
    "#                 n = len(raw)\n",
    "#                 final_long = np.ones(n) / n; final_short = np.zeros(n)\n",
    "#             self.current_weights = final_long - final_short\n",
    "#         else:\n",
    "#             # Passive reweighting between rebalances\n",
    "#             returns_today = np.array([self.data.loc[self.current_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "#             self.current_weights *= (1 + returns_today)\n",
    "#             self.current_weights /= np.sum(self.current_weights)\n",
    "# \n",
    "#         if next_step >= len(self.data):\n",
    "#             terminated = True; reward = 0.0\n",
    "#         else:\n",
    "#             asset_returns = np.array([\n",
    "#                 self.data.loc[next_step, f'Actual_Return_{etf}']\n",
    "#                 for etf in self.etf_list\n",
    "#             ])\n",
    "#             portfolio_return = np.dot(self.current_weights, asset_returns)\n",
    "#             self.cumulative_wealth *= (1 + portfolio_return)\n",
    "#             reward = self.calculate_reward(portfolio_return, asset_returns)\n",
    "#             terminated = next_step >= len(self.data) - 1\n",
    "#         self.current_step += 1\n",
    "#         return self._get_obs(), reward, terminated, False, {}\n",
    "# \n",
    "# # ---------------------------------------------------------------------\n",
    "# # Hyperparameter tuning function\n",
    "# # ---------------------------------------------------------------------\n",
    "# def validate_and_tune(train_data: pd.DataFrame, val_data: pd.DataFrame,\n",
    "#                       etf_list: List[str], cfg: TrainingConfig) -> Dict[str, float]:\n",
    "#     param_dist = {\n",
    "#         'learning_rate': [3e-4, 1e-4],\n",
    "#         'n_steps': [20, 40],\n",
    "#         'batch_size': [10, 20],\n",
    "#         'gamma': [0.95, 0.98],\n",
    "#         'risk_coefficient': [0.1, 0.5, 1.0],\n",
    "#         'seed': [cfg.base_seed, cfg.base_seed + 17, cfg.base_seed + 42],\n",
    "#     }\n",
    "#     sampled_params = list(ParameterSampler(param_dist, n_iter=cfg.n_iter_tuning,\n",
    "#                                            random_state=cfg.base_seed))\n",
    "#     best_reward = -np.inf; best_params = None\n",
    "#     for params in sampled_params:\n",
    "#         seed = params.pop('seed')\n",
    "#         risk_coeff = params.pop('risk_coefficient', cfg.default_risk_coeff)\n",
    "#         set_global_seed(seed)\n",
    "#         env = make_vec_env(lambda: PortfolioEnv(train_data, etf_list,\n",
    "#                                                 'mean_cvar', risk_coeff,\n",
    "#                                                 cfg.rebalance_period,\n",
    "#                                                 cfg.lookback_period),\n",
    "#                            n_envs=1, seed=seed)\n",
    "#         model = PPO('MlpPolicy', env, ent_coef=0.01, clip_range=0.2,\n",
    "#                     seed=seed, **params, verbose=0)\n",
    "#         model.learn(total_timesteps=cfg.tuning_timesteps)\n",
    "#         # Evaluate on validation\n",
    "#         val_env = PortfolioEnv(val_data, etf_list, 'mean_cvar', risk_coeff,\n",
    "#                                cfg.rebalance_period, cfg.lookback_period)\n",
    "#         obs, _ = val_env.reset(seed=seed)\n",
    "#         done = False; total_reward = 0.0\n",
    "#         while not done:\n",
    "#             action, _ = model.predict(obs, deterministic=True)\n",
    "#             obs, reward, done, _, _ = val_env.step(action)\n",
    "#             total_reward += reward\n",
    "#         if total_reward > best_reward:\n",
    "#             best_reward = total_reward\n",
    "#             best_params = params.copy()\n",
    "#             best_params['risk_coefficient'] = risk_coeff\n",
    "#             best_params['seed'] = seed\n",
    "#     return best_params\n",
    "# \n",
    "# # ---------------------------------------------------------------------\n",
    "# # Training and prediction function\n",
    "# # ---------------------------------------------------------------------\n",
    "# def train_and_predict(train_df: pd.DataFrame, val_df: pd.DataFrame,\n",
    "#                       pred_df: pd.DataFrame, etf_list: List[str],\n",
    "#                       cfg: TrainingConfig, best_params: Dict[str, float],\n",
    "#                       model_path: str) -> Tuple[List[List[float]], List[pd.Timestamp]]:\n",
    "#     risk_coeff = best_params.pop('risk_coefficient')\n",
    "#     seed = best_params.pop('seed')\n",
    "#     set_global_seed(seed)\n",
    "#     env_train = make_vec_env(lambda: PortfolioEnv(train_df, etf_list,\n",
    "#                                                   'mean_cvar', risk_coeff,\n",
    "#                                                   cfg.rebalance_period,\n",
    "#                                                   cfg.lookback_period),\n",
    "#                              n_envs=1, seed=seed)\n",
    "#     policy_kwargs = dict(net_arch=list(cfg.policy_arch))\n",
    "#     model = PPO('MlpPolicy', env_train,\n",
    "#                 policy_kwargs=policy_kwargs,\n",
    "#                 ent_coef=0.01,\n",
    "#                 clip_range=0.2,\n",
    "#                 seed=seed,\n",
    "#                 **best_params,\n",
    "#                 verbose=0)\n",
    "#     best_val_reward = -np.inf; no_improve = 0\n",
    "#     # Early stopping loop\n",
    "#     for step in range(0, cfg.max_timesteps, cfg.incremental_timesteps):\n",
    "#         model.learn(total_timesteps=cfg.incremental_timesteps)\n",
    "#         # Evaluate on validation set\n",
    "#         val_env = PortfolioEnv(val_df, etf_list, 'mean_cvar', risk_coeff,\n",
    "#                                cfg.rebalance_period, cfg.lookback_period)\n",
    "#         obs, _ = val_env.reset(seed=seed)\n",
    "#         done = False; val_reward = 0.0\n",
    "#         while not done:\n",
    "#             action, _ = model.predict(obs, deterministic=True)\n",
    "#             obs, reward, done, _, _ = val_env.step(action)\n",
    "#             val_reward += reward\n",
    "#         if val_reward > best_val_reward:\n",
    "#             best_val_reward = val_reward\n",
    "#             no_improve = 0\n",
    "#             model.save(model_path)\n",
    "#         else:\n",
    "#             no_improve += 1\n",
    "#             if no_improve >= cfg.patience:\n",
    "#                 break\n",
    "#     # Load best model and predict on pred_df\n",
    "#     best_model = PPO.load(model_path)\n",
    "#     env_pred = PortfolioEnv(pred_df, etf_list, 'mean_cvar', risk_coeff,\n",
    "#                             cfg.rebalance_period, cfg.lookback_period)\n",
    "#     obs, _ = env_pred.reset()\n",
    "#     done = False\n",
    "#     weights_list: List[List[float]] = []\n",
    "#     dates_list: List[pd.Timestamp] = []\n",
    "#     action = np.zeros(len(etf_list), dtype=np.float32)\n",
    "#     while not done:\n",
    "#         if env_pred.current_step >= cfg.lookback_period and (\n",
    "#             env_pred.current_step % cfg.rebalance_period == 0\n",
    "#         ):\n",
    "#             action, _ = best_model.predict(obs, deterministic=True)\n",
    "#             # Normalization logic (same as env)\n",
    "#             desired_long, desired_short = 1.20, 0.20\n",
    "#             clip_bounds = (-0.2, 0.8)\n",
    "#             raw = action.copy()\n",
    "#             long_w = np.maximum(raw, 0.0)\n",
    "#             short_w = np.abs(np.minimum(raw, 0.0))\n",
    "#             has_longs, has_shorts = np.sum(long_w) > 0, np.sum(short_w) > 0\n",
    "#             if has_longs and has_shorts:\n",
    "#                 norm_long = desired_long * long_w / np.sum(long_w)\n",
    "#                 norm_short = desired_short * short_w / np.sum(short_w)\n",
    "#             elif has_longs and not has_shorts:\n",
    "#                 norm_long = long_w / np.sum(long_w); norm_short = np.zeros_like(short_w)\n",
    "#             elif not has_longs and has_shorts:\n",
    "#                 n = len(raw); norm_long = np.ones(n)/n; norm_short = np.zeros(n)\n",
    "#             else:\n",
    "#                 n = len(raw); norm_long = np.ones(n)/n; norm_short = np.zeros(n)\n",
    "#             combined = norm_long - norm_short\n",
    "#             clipped = np.clip(combined, clip_bounds[0], clip_bounds[1])\n",
    "#             long_c = np.maximum(clipped, 0.0); short_c = np.abs(np.minimum(clipped, 0.0))\n",
    "#             has_long_c, has_short_c = np.sum(long_c) > 0, np.sum(short_c) > 0\n",
    "#             if has_long_c and has_short_c:\n",
    "#                 final_long = desired_long * long_c / np.sum(long_c)\n",
    "#                 final_short = desired_short * short_c / np.sum(short_c)\n",
    "#             elif has_long_c and not has_short_c:\n",
    "#                 final_long = long_c / np.sum(long_c); final_short = np.zeros_like(short_c)\n",
    "#             else:\n",
    "#                 n = len(raw); final_long = np.ones(n)/n; final_short = np.zeros(n)\n",
    "#             final_w = final_long - final_short\n",
    "#             weights_list.append(final_w.tolist())\n",
    "#             dates_list.append(env_pred.data.loc[env_pred.current_step, 'Date'])\n",
    "#         obs, _, done, _, _ = env_pred.step(action)\n",
    "#     return weights_list, dates_list\n",
    "# \n",
    "# # ---------------------------------------------------------------------\n",
    "# # Data loading and overall training loop\n",
    "# # ---------------------------------------------------------------------\n",
    "# cfg = TrainingConfig()\n",
    "# \n",
    "# # Load your prepared Stage‑2 dataset and price data\n",
    "# data = pd.read_csv('stage2_rl_observations_optimized_10ETFs.csv', parse_dates=['Date'])\n",
    "# price_data = pd.read_csv('stock_prices_10ETFs.csv')\n",
    "# price_data['Date'] = pd.to_datetime(price_data['Date'], utc=True).dt.tz_localize(None)\n",
    "# price_cols = {col: f'Price_{col}' for col in price_data.columns if col != 'Date'}\n",
    "# price_data.rename(columns=price_cols, inplace=True)\n",
    "# \n",
    "# merged_data = pd.merge(data, price_data, on='Date', how='inner').reset_index(drop=True)\n",
    "# if len(merged_data) != len(data):\n",
    "#     print(\"Warning: data length mismatch after merge.\")\n",
    "# \n",
    "# etf_list = ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU']\n",
    "# feature_data = add_stable_features(merged_data, etf_list)\n",
    "# feature_data = filter_features(feature_data, include_predicted_returns=True, include_shap_metrics=True)\n",
    "# \n",
    "# # Rolling windows\n",
    "# total_len = len(feature_data)\n",
    "# start_indices = range(0,\n",
    "#                       total_len - (cfg.train_window_days + cfg.validation_window_days + cfg.prediction_window_days),\n",
    "#                       cfg.prediction_window_days)\n",
    "# \n",
    "# # Prepare directory for outputs\n",
    "# output_dir = 'stage2_iterations'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# \n",
    "# # Collect metrics for all iterations\n",
    "# summary_records = []\n",
    "# \n",
    "# for iter_num in range(cfg.num_iterations):\n",
    "#     iter_seed = cfg.base_seed + iter_num\n",
    "#     iter_dir = os.path.join(output_dir, f'iteration_{iter_num:02d}')\n",
    "#     os.makedirs(iter_dir, exist_ok=True)\n",
    "# \n",
    "#     previous_model_path = None  # Path for incremental retraining within iterations\n",
    "#     iter_returns = []\n",
    "#     print(f\"\\nStarting iteration {iter_num+1}/{cfg.num_iterations} (Seed: {iter_seed}) at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "#     for idx, start_idx in enumerate(start_indices):\n",
    "#         window_start_time = time.time()\n",
    "#         print(f\"  - Starting window {idx+1}/{len(start_indices)} at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "# \n",
    "#         train_start = start_idx\n",
    "#         train_end = train_start + cfg.train_window_days\n",
    "#         val_start = train_end\n",
    "#         val_end = val_start + cfg.validation_window_days\n",
    "#         pred_start = val_end\n",
    "#         pred_end = pred_start + cfg.prediction_window_days\n",
    "# \n",
    "#         train_df = feature_data.iloc[train_start:train_end].reset_index(drop=True)\n",
    "#         val_df   = feature_data.iloc[val_start:val_end].reset_index(drop=True)\n",
    "#         pred_df  = feature_data.iloc[pred_start:pred_end].reset_index(drop=True)\n",
    "# \n",
    "#         feature_cols = [c for c in train_df.columns if c != 'Date' and not c.startswith('Actual_Return')]\n",
    "#         scaler = StandardScaler()\n",
    "#         scaler.fit(train_df[feature_cols])\n",
    "# \n",
    "#         train_scaled = train_df.copy()\n",
    "#         train_scaled[feature_cols] = scaler.transform(train_df[feature_cols])\n",
    "#         val_scaled = val_df.copy()\n",
    "#         val_scaled[feature_cols] = scaler.transform(val_df[feature_cols])\n",
    "#         pred_scaled = pred_df.copy()\n",
    "#         pred_scaled[feature_cols] = scaler.transform(pred_df[feature_cols])\n",
    "# \n",
    "#         # Hyperparameter tuning for first window\n",
    "#         if idx == 0:\n",
    "#             best_params = validate_and_tune(train_scaled, val_scaled, etf_list, cfg)\n",
    "#         else:\n",
    "#             best_params = best_params.copy()\n",
    "# \n",
    "#         window_dir = os.path.join(iter_dir, f'window_{idx:02d}')\n",
    "#         os.makedirs(window_dir, exist_ok=True)\n",
    "#         model_path = os.path.join(window_dir, 'best_ppo.zip')\n",
    "# \n",
    "#         # Incremental Retraining Logic\n",
    "#         risk_coeff = best_params.pop('risk_coefficient', cfg.default_risk_coeff)\n",
    "#         seed = best_params.pop('seed', iter_seed)\n",
    "#         set_global_seed(seed)\n",
    "# \n",
    "#         env_train = make_vec_env(lambda: PortfolioEnv(train_scaled, etf_list, 'mean_cvar', risk_coeff, cfg.rebalance_period, cfg.lookback_period), n_envs=1, seed=seed)\n",
    "#         policy_kwargs = dict(net_arch=list(cfg.policy_arch))\n",
    "# \n",
    "#         if previous_model_path and os.path.exists(previous_model_path):\n",
    "#             model = PPO.load(previous_model_path, env=env_train)\n",
    "#             model.set_env(env_train)\n",
    "#         else:\n",
    "#             model = PPO('MlpPolicy', env_train, policy_kwargs=policy_kwargs, ent_coef=0.01, clip_range=0.2, seed=seed, **best_params, verbose=0)\n",
    "# \n",
    "#         best_val_reward = -np.inf\n",
    "#         no_improve = 0\n",
    "#         training_log = []\n",
    "# \n",
    "#         for step in range(0, cfg.max_timesteps, cfg.incremental_timesteps):\n",
    "#             model.learn(total_timesteps=cfg.incremental_timesteps)\n",
    "# \n",
    "#             val_env = PortfolioEnv(val_scaled, etf_list, 'mean_cvar', risk_coeff, cfg.rebalance_period, cfg.lookback_period)\n",
    "#             obs, _ = val_env.reset(seed=seed)\n",
    "#             done, val_reward = False, 0.0\n",
    "# \n",
    "#             while not done:\n",
    "#                 action, _ = model.predict(obs, deterministic=True)\n",
    "#                 obs, reward, done, _, _ = val_env.step(action)\n",
    "#                 val_reward += reward\n",
    "# \n",
    "#             training_log.append({'training_step': step + cfg.incremental_timesteps, 'validation_reward': val_reward})\n",
    "# \n",
    "#             if val_reward > best_val_reward:\n",
    "#                 best_val_reward = val_reward\n",
    "#                 no_improve = 0\n",
    "#                 model.save(model_path)\n",
    "#             else:\n",
    "#                 no_improve += 1\n",
    "#                 if no_improve >= cfg.patience:\n",
    "#                     break\n",
    "# \n",
    "#         # Save training logs to CSV\n",
    "#         pd.DataFrame(training_log).to_csv(os.path.join(window_dir, 'training_validation_log.csv'), index=False)\n",
    "# \n",
    "#         previous_model_path = model_path\n",
    "# \n",
    "#         # Prediction step:\n",
    "#         best_model = PPO.load(model_path)\n",
    "#         env_pred = PortfolioEnv(pred_scaled, etf_list, 'mean_cvar', risk_coeff, cfg.rebalance_period, cfg.lookback_period)\n",
    "#         obs, _ = env_pred.reset()\n",
    "#         done = False\n",
    "#         weights_list, dates_list = [], []\n",
    "# \n",
    "#         action = np.zeros(len(etf_list), dtype=np.float32)\n",
    "#         while not done:\n",
    "#             if env_pred.current_step >= cfg.lookback_period and (\n",
    "#                 env_pred.current_step % cfg.rebalance_period == 0\n",
    "#             ):\n",
    "#                 action, _ = best_model.predict(obs, deterministic=True)\n",
    "# \n",
    "#                 desired_long, desired_short = 1.20, 0.20\n",
    "#                 clip_bounds = (-0.2, 0.8)\n",
    "#                 raw = action.copy()\n",
    "#                 long_w = np.maximum(raw, 0.0)\n",
    "#                 short_w = np.abs(np.minimum(raw, 0.0))\n",
    "#                 has_longs, has_shorts = np.sum(long_w) > 0, np.sum(short_w) > 0\n",
    "# \n",
    "#                 if has_longs and has_shorts:\n",
    "#                     norm_long = desired_long * long_w / np.sum(long_w)\n",
    "#                     norm_short = desired_short * short_w / np.sum(short_w)\n",
    "#                 elif has_longs and not has_shorts:\n",
    "#                     norm_long = long_w / np.sum(long_w); norm_short = np.zeros_like(short_w)\n",
    "#                 elif not has_longs and has_shorts:\n",
    "#                     n = len(raw); norm_long = np.ones(n)/n; norm_short = np.zeros(n)\n",
    "#                 else:\n",
    "#                     n = len(raw); norm_long = np.ones(n)/n; norm_short = np.zeros(n)\n",
    "# \n",
    "#                 combined = norm_long - norm_short\n",
    "#                 clipped = np.clip(combined, clip_bounds[0], clip_bounds[1])\n",
    "# \n",
    "#                 long_c = np.maximum(clipped, 0.0); short_c = np.abs(np.minimum(clipped, 0.0))\n",
    "#                 has_long_c, has_short_c = np.sum(long_c) > 0, np.sum(short_c) > 0\n",
    "#                 if has_long_c and has_short_c:\n",
    "#                     final_long = desired_long * long_c / np.sum(long_c)\n",
    "#                     final_short = desired_short * short_c / np.sum(short_c)\n",
    "#                 elif has_long_c and not has_short_c:\n",
    "#                     final_long = long_c / np.sum(long_c); final_short = np.zeros_like(short_c)\n",
    "#                 else:\n",
    "#                     n = len(raw); final_long = np.ones(n)/n; final_short = np.zeros(n)\n",
    "# \n",
    "#                 final_w = final_long - final_short\n",
    "#                 weights_list.append(final_w.tolist())\n",
    "#                 dates_list.append(env_pred.data.loc[env_pred.current_step, 'Date'])\n",
    "# \n",
    "#             obs, _, done, _, _ = env_pred.step(action)\n",
    "# \n",
    "#         # Save predicted weights to CSV\n",
    "#         weights_df = pd.DataFrame(weights_list, columns=etf_list)\n",
    "#         weights_df.insert(0, 'Date', dates_list)\n",
    "#         weights_df.to_csv(os.path.join(window_dir, 'weights.csv'), index=False)\n",
    "# \n",
    "#         # Compute cumulative return\n",
    "#         cum_wealth = 1.0\n",
    "#         returns_log = []\n",
    "# \n",
    "#         for t, w in zip(dates_list, weights_list):\n",
    "#             step_idx = pred_scaled[pred_scaled['Date'] == t].index[0]\n",
    "#             asset_returns = np.array([\n",
    "#                 pred_scaled.loc[step_idx + 1, f'Actual_Return_{etf}']\n",
    "#                 for etf in etf_list\n",
    "#             ])\n",
    "#             port_ret = np.dot(w, asset_returns)\n",
    "#             cum_wealth *= (1 + port_ret)\n",
    "#             returns_log.append({'Date': t, 'Portfolio_Return': port_ret, 'Cumulative_Wealth': cum_wealth})\n",
    "# \n",
    "#         iter_returns.append(cum_wealth - 1.0)\n",
    "# \n",
    "#         # Save returns log to CSV\n",
    "#         pd.DataFrame(returns_log).to_csv(os.path.join(window_dir, 'returns_log.csv'), index=False)\n",
    "#         window_end_time = time.time()\n",
    "#         elapsed_window_time = window_end_time - window_start_time\n",
    "#         print(f\"  - Completed window {idx+1}/{len(start_indices)} in {elapsed_window_time/60:.2f} minutes.\")\n",
    "# \n",
    "# \n",
    "#     mean_ret = np.mean(iter_returns)\n",
    "#     std_ret  = np.std(iter_returns, ddof=1)\n",
    "#     sharpe   = (mean_ret / std_ret) * np.sqrt(len(iter_returns)) if std_ret != 0 else np.nan\n",
    "#     summary_records.append({'iteration': iter_num, 'seed': iter_seed,\n",
    "#                             'mean_return': mean_ret, 'sharpe': sharpe})\n",
    "# \n",
    "# # Save overall summary and significance results:\n",
    "# summary_df = pd.DataFrame(summary_records)\n",
    "# summary_df.to_csv(os.path.join(output_dir, 'iterations_summary.csv'), index=False)\n",
    "# \n",
    "# t_stat, p_val = ttest_1samp(summary_df['mean_return'], 0.0)\n",
    "# with open(os.path.join(output_dir, 't_test_result.csv'), 'w') as f:\n",
    "#     f.write(f\"t-statistic,{t_stat}\\np-value,{p_val}\\n\")\n",
    "# \n",
    "# print(summary_df)\n",
    "# print(f\"Overall t-statistic={t_stat:.3f}, p-value={p_val:.3f}\")\n",
    "# \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8f3f0813e659ead",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting iteration 1/10 (Seed: 42) at 2025-08-01 08:51:58\n",
      "  - Starting window 1/8 at 2025-08-01 08:51:58\n",
      "triam new model and saved under stage2_iterations\\iteration_00\\window_00\\best_ppo.zip\n",
      "  - Completed window 1/8 in 20.50 minutes.\n",
      "  - Starting window 2/8 at 2025-08-01 09:12:28\n",
      "load the exising model from stage2_iterations\\iteration_00\\window_00\\best_ppo.zip and retrain\n",
      "  - Completed window 2/8 in 4.88 minutes.\n",
      "  - Starting window 3/8 at 2025-08-01 09:17:21\n",
      "load the exising model from stage2_iterations\\iteration_00\\window_01\\best_ppo.zip and retrain\n",
      "  - Completed window 3/8 in 3.91 minutes.\n",
      "  - Starting window 4/8 at 2025-08-01 09:21:16\n",
      "load the exising model from stage2_iterations\\iteration_00\\window_02\\best_ppo.zip and retrain\n",
      "  - Completed window 4/8 in 4.90 minutes.\n",
      "  - Starting window 5/8 at 2025-08-01 09:26:10\n",
      "load the exising model from stage2_iterations\\iteration_00\\window_03\\best_ppo.zip and retrain\n",
      "  - Completed window 5/8 in 3.87 minutes.\n",
      "  - Starting window 6/8 at 2025-08-01 09:30:02\n",
      "load the exising model from stage2_iterations\\iteration_00\\window_04\\best_ppo.zip and retrain\n",
      "  - Completed window 6/8 in 3.87 minutes.\n",
      "  - Starting window 7/8 at 2025-08-01 09:33:54\n",
      "load the exising model from stage2_iterations\\iteration_00\\window_05\\best_ppo.zip and retrain\n",
      "  - Completed window 7/8 in 3.88 minutes.\n",
      "  - Starting window 8/8 at 2025-08-01 09:37:47\n",
      "load the exising model from stage2_iterations\\iteration_00\\window_06\\best_ppo.zip and retrain\n",
      "  - Completed window 8/8 in 4.85 minutes.\n",
      "\n",
      "Starting iteration 2/10 (Seed: 43) at 2025-08-01 09:42:38\n",
      "  - Starting window 1/8 at 2025-08-01 09:42:38\n",
      "triam new model and saved under stage2_iterations\\iteration_01\\window_00\\best_ppo.zip\n",
      "  - Completed window 1/8 in 20.60 minutes.\n",
      "  - Starting window 2/8 at 2025-08-01 10:03:14\n",
      "load the exising model from stage2_iterations\\iteration_01\\window_00\\best_ppo.zip and retrain\n",
      "  - Completed window 2/8 in 4.88 minutes.\n",
      "  - Starting window 3/8 at 2025-08-01 10:08:07\n",
      "load the exising model from stage2_iterations\\iteration_01\\window_01\\best_ppo.zip and retrain\n",
      "  - Completed window 3/8 in 4.90 minutes.\n",
      "  - Starting window 4/8 at 2025-08-01 10:13:01\n",
      "load the exising model from stage2_iterations\\iteration_01\\window_02\\best_ppo.zip and retrain\n",
      "  - Completed window 4/8 in 4.90 minutes.\n",
      "  - Starting window 5/8 at 2025-08-01 10:17:55\n",
      "load the exising model from stage2_iterations\\iteration_01\\window_03\\best_ppo.zip and retrain\n",
      "  - Completed window 5/8 in 4.89 minutes.\n",
      "  - Starting window 6/8 at 2025-08-01 10:22:48\n",
      "load the exising model from stage2_iterations\\iteration_01\\window_04\\best_ppo.zip and retrain\n",
      "  - Completed window 6/8 in 3.92 minutes.\n",
      "  - Starting window 7/8 at 2025-08-01 10:26:43\n",
      "load the exising model from stage2_iterations\\iteration_01\\window_05\\best_ppo.zip and retrain\n",
      "  - Completed window 7/8 in 3.91 minutes.\n",
      "  - Starting window 8/8 at 2025-08-01 10:30:38\n",
      "load the exising model from stage2_iterations\\iteration_01\\window_06\\best_ppo.zip and retrain\n",
      "  - Completed window 8/8 in 3.90 minutes.\n",
      "\n",
      "Starting iteration 3/10 (Seed: 44) at 2025-08-01 10:34:33\n",
      "  - Starting window 1/8 at 2025-08-01 10:34:33\n",
      "triam new model and saved under stage2_iterations\\iteration_02\\window_00\\best_ppo.zip\n",
      "  - Completed window 1/8 in 21.22 minutes.\n",
      "  - Starting window 2/8 at 2025-08-01 10:55:46\n",
      "load the exising model from stage2_iterations\\iteration_02\\window_00\\best_ppo.zip and retrain\n",
      "  - Completed window 2/8 in 4.89 minutes.\n",
      "  - Starting window 3/8 at 2025-08-01 11:00:39\n",
      "load the exising model from stage2_iterations\\iteration_02\\window_01\\best_ppo.zip and retrain\n",
      "  - Completed window 3/8 in 3.91 minutes.\n",
      "  - Starting window 4/8 at 2025-08-01 11:04:34\n",
      "load the exising model from stage2_iterations\\iteration_02\\window_02\\best_ppo.zip and retrain\n",
      "  - Completed window 4/8 in 4.91 minutes.\n",
      "  - Starting window 5/8 at 2025-08-01 11:09:28\n",
      "load the exising model from stage2_iterations\\iteration_02\\window_03\\best_ppo.zip and retrain\n",
      "  - Completed window 5/8 in 4.89 minutes.\n",
      "  - Starting window 6/8 at 2025-08-01 11:14:22\n",
      "load the exising model from stage2_iterations\\iteration_02\\window_04\\best_ppo.zip and retrain\n",
      "  - Completed window 6/8 in 4.90 minutes.\n",
      "  - Starting window 7/8 at 2025-08-01 11:19:16\n",
      "load the exising model from stage2_iterations\\iteration_02\\window_05\\best_ppo.zip and retrain\n",
      "  - Completed window 7/8 in 3.92 minutes.\n",
      "  - Starting window 8/8 at 2025-08-01 11:23:11\n",
      "load the exising model from stage2_iterations\\iteration_02\\window_06\\best_ppo.zip and retrain\n",
      "  - Completed window 8/8 in 3.92 minutes.\n",
      "\n",
      "Starting iteration 4/10 (Seed: 45) at 2025-08-01 11:27:06\n",
      "  - Starting window 1/8 at 2025-08-01 11:27:06\n",
      "triam new model and saved under stage2_iterations\\iteration_03\\window_00\\best_ppo.zip\n",
      "  - Completed window 1/8 in 20.52 minutes.\n",
      "  - Starting window 2/8 at 2025-08-01 11:47:37\n",
      "load the exising model from stage2_iterations\\iteration_03\\window_00\\best_ppo.zip and retrain\n",
      "  - Completed window 2/8 in 3.90 minutes.\n",
      "  - Starting window 3/8 at 2025-08-01 11:51:32\n",
      "load the exising model from stage2_iterations\\iteration_03\\window_01\\best_ppo.zip and retrain\n",
      "  - Completed window 3/8 in 3.89 minutes.\n",
      "  - Starting window 4/8 at 2025-08-01 11:55:25\n",
      "load the exising model from stage2_iterations\\iteration_03\\window_02\\best_ppo.zip and retrain\n",
      "  - Completed window 4/8 in 3.90 minutes.\n",
      "  - Starting window 5/8 at 2025-08-01 11:59:19\n",
      "load the exising model from stage2_iterations\\iteration_03\\window_03\\best_ppo.zip and retrain\n",
      "  - Completed window 5/8 in 4.85 minutes.\n",
      "  - Starting window 6/8 at 2025-08-01 12:04:10\n",
      "load the exising model from stage2_iterations\\iteration_03\\window_04\\best_ppo.zip and retrain\n",
      "  - Completed window 6/8 in 4.86 minutes.\n",
      "  - Starting window 7/8 at 2025-08-01 12:09:01\n",
      "load the exising model from stage2_iterations\\iteration_03\\window_05\\best_ppo.zip and retrain\n",
      "  - Completed window 7/8 in 4.85 minutes.\n",
      "  - Starting window 8/8 at 2025-08-01 12:13:52\n",
      "load the exising model from stage2_iterations\\iteration_03\\window_06\\best_ppo.zip and retrain\n",
      "  - Completed window 8/8 in 4.85 minutes.\n",
      "\n",
      "Starting iteration 5/10 (Seed: 46) at 2025-08-01 12:18:44\n",
      "  - Starting window 1/8 at 2025-08-01 12:18:44\n",
      "triam new model and saved under stage2_iterations\\iteration_04\\window_00\\best_ppo.zip\n",
      "  - Completed window 1/8 in 23.22 minutes.\n",
      "  - Starting window 2/8 at 2025-08-01 12:41:56\n",
      "load the exising model from stage2_iterations\\iteration_04\\window_00\\best_ppo.zip and retrain\n",
      "  - Completed window 2/8 in 7.82 minutes.\n",
      "  - Starting window 3/8 at 2025-08-01 12:49:45\n",
      "load the exising model from stage2_iterations\\iteration_04\\window_01\\best_ppo.zip and retrain\n",
      "  - Completed window 3/8 in 6.26 minutes.\n",
      "  - Starting window 4/8 at 2025-08-01 12:56:01\n",
      "load the exising model from stage2_iterations\\iteration_04\\window_02\\best_ppo.zip and retrain\n",
      "  - Completed window 4/8 in 6.27 minutes.\n",
      "  - Starting window 5/8 at 2025-08-01 13:02:17\n",
      "load the exising model from stage2_iterations\\iteration_04\\window_03\\best_ppo.zip and retrain\n",
      "  - Completed window 5/8 in 6.26 minutes.\n",
      "  - Starting window 6/8 at 2025-08-01 13:08:33\n",
      "load the exising model from stage2_iterations\\iteration_04\\window_04\\best_ppo.zip and retrain\n",
      "  - Completed window 6/8 in 6.25 minutes.\n",
      "  - Starting window 7/8 at 2025-08-01 13:14:48\n",
      "load the exising model from stage2_iterations\\iteration_04\\window_05\\best_ppo.zip and retrain\n",
      "  - Completed window 7/8 in 6.25 minutes.\n",
      "  - Starting window 8/8 at 2025-08-01 13:21:03\n",
      "load the exising model from stage2_iterations\\iteration_04\\window_06\\best_ppo.zip and retrain\n",
      "  - Completed window 8/8 in 6.26 minutes.\n",
      "\n",
      "Starting iteration 6/10 (Seed: 47) at 2025-08-01 13:27:19\n",
      "  - Starting window 1/8 at 2025-08-01 13:27:19\n",
      "triam new model and saved under stage2_iterations\\iteration_05\\window_00\\best_ppo.zip\n",
      "  - Completed window 1/8 in 19.21 minutes.\n",
      "  - Starting window 2/8 at 2025-08-01 13:46:31\n",
      "load the exising model from stage2_iterations\\iteration_05\\window_00\\best_ppo.zip and retrain\n",
      "  - Completed window 2/8 in 3.88 minutes.\n",
      "  - Starting window 3/8 at 2025-08-01 13:50:24\n",
      "load the exising model from stage2_iterations\\iteration_05\\window_01\\best_ppo.zip and retrain\n",
      "  - Completed window 3/8 in 4.86 minutes.\n",
      "  - Starting window 4/8 at 2025-08-01 13:55:15\n",
      "load the exising model from stage2_iterations\\iteration_05\\window_02\\best_ppo.zip and retrain\n",
      "  - Completed window 4/8 in 4.86 minutes.\n",
      "  - Starting window 5/8 at 2025-08-01 14:00:07\n",
      "load the exising model from stage2_iterations\\iteration_05\\window_03\\best_ppo.zip and retrain\n",
      "  - Completed window 5/8 in 4.84 minutes.\n",
      "  - Starting window 6/8 at 2025-08-01 14:04:58\n",
      "load the exising model from stage2_iterations\\iteration_05\\window_04\\best_ppo.zip and retrain\n",
      "  - Completed window 6/8 in 3.89 minutes.\n",
      "  - Starting window 7/8 at 2025-08-01 14:08:51\n",
      "load the exising model from stage2_iterations\\iteration_05\\window_05\\best_ppo.zip and retrain\n",
      "  - Completed window 7/8 in 3.89 minutes.\n",
      "  - Starting window 8/8 at 2025-08-01 14:12:45\n",
      "load the exising model from stage2_iterations\\iteration_05\\window_06\\best_ppo.zip and retrain\n",
      "  - Completed window 8/8 in 3.89 minutes.\n",
      "\n",
      "Starting iteration 7/10 (Seed: 48) at 2025-08-01 14:16:39\n",
      "  - Starting window 1/8 at 2025-08-01 14:16:39\n",
      "triam new model and saved under stage2_iterations\\iteration_06\\window_00\\best_ppo.zip\n",
      "  - Completed window 1/8 in 18.88 minutes.\n",
      "  - Starting window 2/8 at 2025-08-01 14:35:31\n",
      "load the exising model from stage2_iterations\\iteration_06\\window_00\\best_ppo.zip and retrain\n",
      "  - Completed window 2/8 in 3.87 minutes.\n",
      "  - Starting window 3/8 at 2025-08-01 14:39:24\n",
      "load the exising model from stage2_iterations\\iteration_06\\window_01\\best_ppo.zip and retrain\n",
      "  - Completed window 3/8 in 3.89 minutes.\n",
      "  - Starting window 4/8 at 2025-08-01 14:43:17\n",
      "load the exising model from stage2_iterations\\iteration_06\\window_02\\best_ppo.zip and retrain\n",
      "  - Completed window 4/8 in 4.87 minutes.\n",
      "  - Starting window 5/8 at 2025-08-01 14:48:09\n",
      "load the exising model from stage2_iterations\\iteration_06\\window_03\\best_ppo.zip and retrain\n",
      "  - Completed window 5/8 in 4.85 minutes.\n",
      "  - Starting window 6/8 at 2025-08-01 14:53:00\n",
      "load the exising model from stage2_iterations\\iteration_06\\window_04\\best_ppo.zip and retrain\n",
      "  - Completed window 6/8 in 3.88 minutes.\n",
      "  - Starting window 7/8 at 2025-08-01 14:56:53\n",
      "load the exising model from stage2_iterations\\iteration_06\\window_05\\best_ppo.zip and retrain\n",
      "  - Completed window 7/8 in 3.88 minutes.\n",
      "  - Starting window 8/8 at 2025-08-01 15:00:46\n",
      "load the exising model from stage2_iterations\\iteration_06\\window_06\\best_ppo.zip and retrain\n",
      "  - Completed window 8/8 in 3.88 minutes.\n",
      "\n",
      "Starting iteration 8/10 (Seed: 49) at 2025-08-01 15:04:39\n",
      "  - Starting window 1/8 at 2025-08-01 15:04:39\n",
      "triam new model and saved under stage2_iterations\\iteration_07\\window_00\\best_ppo.zip\n",
      "  - Completed window 1/8 in 19.54 minutes.\n",
      "  - Starting window 2/8 at 2025-08-01 15:24:11\n",
      "load the exising model from stage2_iterations\\iteration_07\\window_00\\best_ppo.zip and retrain\n",
      "  - Completed window 2/8 in 3.87 minutes.\n",
      "  - Starting window 3/8 at 2025-08-01 15:28:03\n",
      "load the exising model from stage2_iterations\\iteration_07\\window_01\\best_ppo.zip and retrain\n",
      "  - Completed window 3/8 in 3.88 minutes.\n",
      "  - Starting window 4/8 at 2025-08-01 15:31:56\n",
      "load the exising model from stage2_iterations\\iteration_07\\window_02\\best_ppo.zip and retrain\n",
      "  - Completed window 4/8 in 3.89 minutes.\n",
      "  - Starting window 5/8 at 2025-08-01 15:35:50\n",
      "load the exising model from stage2_iterations\\iteration_07\\window_03\\best_ppo.zip and retrain\n",
      "  - Completed window 5/8 in 3.89 minutes.\n",
      "  - Starting window 6/8 at 2025-08-01 15:39:43\n",
      "load the exising model from stage2_iterations\\iteration_07\\window_04\\best_ppo.zip and retrain\n",
      "  - Completed window 6/8 in 4.85 minutes.\n",
      "  - Starting window 7/8 at 2025-08-01 15:44:34\n",
      "load the exising model from stage2_iterations\\iteration_07\\window_05\\best_ppo.zip and retrain\n",
      "  - Completed window 7/8 in 3.88 minutes.\n",
      "  - Starting window 8/8 at 2025-08-01 15:48:27\n",
      "load the exising model from stage2_iterations\\iteration_07\\window_06\\best_ppo.zip and retrain\n",
      "  - Completed window 8/8 in 3.88 minutes.\n",
      "\n",
      "Starting iteration 9/10 (Seed: 50) at 2025-08-01 15:52:20\n",
      "  - Starting window 1/8 at 2025-08-01 15:52:20\n",
      "triam new model and saved under stage2_iterations\\iteration_08\\window_00\\best_ppo.zip\n",
      "  - Completed window 1/8 in 18.48 minutes.\n",
      "  - Starting window 2/8 at 2025-08-01 16:10:49\n",
      "load the exising model from stage2_iterations\\iteration_08\\window_00\\best_ppo.zip and retrain\n",
      "  - Completed window 2/8 in 3.86 minutes.\n",
      "  - Starting window 3/8 at 2025-08-01 16:14:40\n",
      "load the exising model from stage2_iterations\\iteration_08\\window_01\\best_ppo.zip and retrain\n",
      "  - Completed window 3/8 in 3.88 minutes.\n",
      "  - Starting window 4/8 at 2025-08-01 16:18:33\n",
      "load the exising model from stage2_iterations\\iteration_08\\window_02\\best_ppo.zip and retrain\n",
      "  - Completed window 4/8 in 3.89 minutes.\n",
      "  - Starting window 5/8 at 2025-08-01 16:22:27\n",
      "load the exising model from stage2_iterations\\iteration_08\\window_03\\best_ppo.zip and retrain\n",
      "  - Completed window 5/8 in 3.88 minutes.\n",
      "  - Starting window 6/8 at 2025-08-01 16:26:20\n",
      "load the exising model from stage2_iterations\\iteration_08\\window_04\\best_ppo.zip and retrain\n",
      "  - Completed window 6/8 in 3.88 minutes.\n",
      "  - Starting window 7/8 at 2025-08-01 16:30:12\n",
      "load the exising model from stage2_iterations\\iteration_08\\window_05\\best_ppo.zip and retrain\n",
      "  - Completed window 7/8 in 3.88 minutes.\n",
      "  - Starting window 8/8 at 2025-08-01 16:34:05\n",
      "load the exising model from stage2_iterations\\iteration_08\\window_06\\best_ppo.zip and retrain\n",
      "  - Completed window 8/8 in 3.88 minutes.\n",
      "\n",
      "Starting iteration 10/10 (Seed: 51) at 2025-08-01 16:37:58\n",
      "  - Starting window 1/8 at 2025-08-01 16:37:58\n",
      "triam new model and saved under stage2_iterations\\iteration_09\\window_00\\best_ppo.zip\n",
      "  - Completed window 1/8 in 22.56 minutes.\n",
      "  - Starting window 2/8 at 2025-08-01 17:00:32\n",
      "load the exising model from stage2_iterations\\iteration_09\\window_00\\best_ppo.zip and retrain\n",
      "  - Completed window 2/8 in 7.78 minutes.\n",
      "  - Starting window 3/8 at 2025-08-01 17:08:18\n",
      "load the exising model from stage2_iterations\\iteration_09\\window_01\\best_ppo.zip and retrain\n",
      "  - Completed window 3/8 in 7.83 minutes.\n",
      "  - Starting window 4/8 at 2025-08-01 17:16:08\n",
      "load the exising model from stage2_iterations\\iteration_09\\window_02\\best_ppo.zip and retrain\n",
      "  - Completed window 4/8 in 7.83 minutes.\n",
      "  - Starting window 5/8 at 2025-08-01 17:23:58\n",
      "load the exising model from stage2_iterations\\iteration_09\\window_03\\best_ppo.zip and retrain\n",
      "  - Completed window 5/8 in 6.24 minutes.\n",
      "  - Starting window 6/8 at 2025-08-01 17:30:12\n",
      "load the exising model from stage2_iterations\\iteration_09\\window_04\\best_ppo.zip and retrain\n",
      "  - Completed window 6/8 in 7.81 minutes.\n",
      "  - Starting window 7/8 at 2025-08-01 17:38:01\n",
      "load the exising model from stage2_iterations\\iteration_09\\window_05\\best_ppo.zip and retrain\n",
      "  - Completed window 7/8 in 7.81 minutes.\n",
      "  - Starting window 8/8 at 2025-08-01 17:45:50\n",
      "load the exising model from stage2_iterations\\iteration_09\\window_06\\best_ppo.zip and retrain\n",
      "  - Completed window 8/8 in 7.82 minutes.\n",
      "   iteration  seed  mean_return    sharpe\n",
      "0          0    42    -0.003227 -0.292479\n",
      "1          1    43     0.005660  0.446435\n",
      "2          2    44     0.008072  0.685706\n",
      "3          3    45     0.006109  0.417665\n",
      "4          4    46     0.002597  0.227263\n",
      "5          5    47     0.004894  0.551505\n",
      "6          6    48     0.009757  0.901433\n",
      "7          7    49     0.005274  0.387764\n",
      "8          8    50     0.006689  0.590288\n",
      "9          9    51     0.004731  0.439352\n",
      "Overall t-statistic=4.569, p-value=0.001\n"
     ]
    }
   ],
   "source": [
    "# fine tune to use change‑based actions\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "import time\n",
    "from gymnasium import spaces\n",
    "import gc\n",
    "# ---------------------------------------------------------------------\n",
    "# Configuration dataclass\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    train_window_days: int = 252 * 7      # 10 years for training\n",
    "    validation_window_days: int = 252      # ~6 months for validation\n",
    "    prediction_window_days: int = 252      # ~6 months for prediction\n",
    "    lookback_period: int = 21              # lookback for observations\n",
    "    rebalance_period: int = 21             # rebalance every 10 days\n",
    "    n_iter_tuning: int = 20                # number of hyperparameter samples\n",
    "    tuning_timesteps: int = 10_000          # timesteps for each tune\n",
    "    incremental_timesteps: int = 10_000     # PPO training step size\n",
    "    max_timesteps: int = 50_000            # maximum PPO timesteps\n",
    "    patience: int = 3                      # early stopping patience\n",
    "    policy_arch: Tuple[int, int] = (256, 256)  # network architecture\n",
    "    num_iterations: int = 10                # number of outer iterations (seeds)\n",
    "    base_seed: int = 42                    # base random seed\n",
    "    default_risk_coeff: float = 0.5        # default risk coefficient\n",
    "    desired_long: float = 1.0       # Default no leverage, 100% allocation\n",
    "    desired_short: float = 0.0      # Default no short selling\n",
    "    weight_bounds: Tuple[float, float] = (0.0, 1.0)  # Default bounds [0,1] for no shorts\n",
    "    lambda_hhi: float = 0.1\n",
    "    lambda_turnover: float = 0.005\n",
    "    transaction_cost_rate: float = 0.0\n",
    "    model_retrain: bool  = False\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Seed-setting utility\n",
    "# ---------------------------------------------------------------------\n",
    "def set_global_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Feature engineering\n",
    "# ---------------------------------------------------------------------\n",
    "def add_stable_features(df: pd.DataFrame, etf_list: List[str]) -> pd.DataFrame:\n",
    "    data = df.copy()\n",
    "    for etf in etf_list:\n",
    "        price_col = f'Price_{etf}'\n",
    "        data[f'Volatility_{etf}'] = data[price_col].pct_change().rolling(20).std()\n",
    "        data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
    "        data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
    "        data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
    "        data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
    "        data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
    "        data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "def filter_features(df: pd.DataFrame,\n",
    "                    include_predicted_returns: bool = True,\n",
    "                    include_shap_metrics: bool = True) -> pd.DataFrame:\n",
    "    df_filtered = df.copy()\n",
    "    if not include_predicted_returns:\n",
    "        pred_cols = [c for c in df_filtered.columns if 'Predicted_Return' in c]\n",
    "        df_filtered.drop(columns=pred_cols, inplace=True)\n",
    "    if not include_shap_metrics:\n",
    "        shap_cols = [c for c in df_filtered.columns if 'SHAP' in c]\n",
    "        df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "    return df_filtered\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Custom Gym environment\n",
    "# ---------------------------------------------------------------------\n",
    "class PortfolioEnv(gym.Env):\n",
    "    metadata = {'render_modes': []}\n",
    "\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar',\n",
    "                 risk_coefficient=0.5, rebalance_period=21,\n",
    "                 lookback_period=21, weight_bounds=(0.0, 1.0),\n",
    "                 desired_long=1.0, desired_short=0.0,\n",
    "                 use_baseline=False, baseline_fn=None,\n",
    "                 transaction_cost_rate=0.0,\n",
    "                 lambda_turnover=0.001,   # <- Add explicitly\n",
    "                 lambda_hhi=0.1):         # <- Add explicitly\n",
    "        super().__init__()\n",
    "        self.transaction_cost_rate = transaction_cost_rate\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "        self.weight_bounds = weight_bounds\n",
    "        self.desired_long = desired_long       # Add this explicitly\n",
    "        self.desired_short = desired_short     # Add this explicitly\n",
    "        self.use_baseline = use_baseline\n",
    "        self.baseline_fn = baseline_fn\n",
    "        self.transaction_cost_rate = transaction_cost_rate\n",
    "        self.lambda_turnover = lambda_turnover\n",
    "        self.lambda_hhi = lambda_hhi\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0,\n",
    "                                       shape=(len(etf_list),), dtype=np.float32)\n",
    "        self.feature_cols = [c for c in data.columns\n",
    "                             if c != 'Date' and not c.startswith('Actual_Return')]\n",
    "        self.num_features_per_day = len(self.feature_cols)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.num_features_per_day * lookback_period,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = lookback_period\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(etf_list)] * len(etf_list))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(self.etf_list)] * len(self.etf_list))\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs_window = self.data.iloc[self.current_step - self.lookback_period : self.current_step]\n",
    "        obs_values = obs_window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "        \n",
    "        if np.isnan(obs_values).any() or np.isinf(obs_values).any():\n",
    "            obs_values = np.nan_to_num(obs_values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return obs_values\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns, turnover):\n",
    "        hhi = np.sum(np.square(np.abs(self.current_weights)))\n",
    "    \n",
    "        if np.isnan(portfolio_return) or np.isinf(portfolio_return):\n",
    "            portfolio_return = 0.0  # safeguard explicitly\n",
    "    \n",
    "        portfolio_return = np.clip(portfolio_return, -0.5, 0.5)  # explicitly clip returns\n",
    "    \n",
    "        if self.reward_type == 'cumulative_return':\n",
    "            base_reward = portfolio_return\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            base_reward = np.log(max(1 + portfolio_return, 1e-8))\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            base_reward = portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100 * alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            base_reward = portfolio_return - self.risk_coefficient * cvar\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid reward type: {self.reward_type}\")\n",
    "    \n",
    "        reward = base_reward \\\n",
    "                 - self.lambda_turnover * turnover \\\n",
    "                 - self.lambda_hhi * hhi\n",
    "    \n",
    "        if np.isnan(reward) or np.isinf(reward):\n",
    "            reward = -1.0  # explicit fallback\n",
    "    \n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        next_step = self.current_step + 1\n",
    "        prev_weights = self.current_weights.copy()\n",
    "    \n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            if self.use_baseline and self.baseline_fn is not None:\n",
    "                current_date = self.data.loc[self.current_step, 'Date']\n",
    "                baseline_w = self.baseline_fn(current_date)\n",
    "                raw = baseline_w * (1.0 + action)\n",
    "            else:\n",
    "                raw = self.current_weights + action\n",
    "    \n",
    "            long_w = np.maximum(raw, 0.0)\n",
    "            short_w = np.abs(np.minimum(raw, 0.0))\n",
    "    \n",
    "            has_long = long_w.sum() > 0\n",
    "            has_short = short_w.sum() > 0\n",
    "    \n",
    "            if has_long and has_short:\n",
    "                norm_long = self.desired_long * long_w / long_w.sum()\n",
    "                norm_short = self.desired_short * short_w / short_w.sum()\n",
    "                combined = norm_long - norm_short\n",
    "            elif has_long and not has_short:\n",
    "                # explicitly no leverage if no shorts\n",
    "                combined = long_w / long_w.sum()\n",
    "            elif not has_long and has_short:\n",
    "                # explicitly full short if no longs\n",
    "                combined = -short_w / short_w.sum()\n",
    "            else:\n",
    "                # fallback explicitly to equal weights\n",
    "                combined = np.ones(len(raw)) / len(raw)\n",
    "    \n",
    "            clipped = np.clip(combined, self.weight_bounds[0], self.weight_bounds[1])\n",
    "    \n",
    "            # After clipping explicitly re-normalize\n",
    "            long_c = np.maximum(clipped, 0.0)\n",
    "            short_c = np.abs(np.minimum(clipped, 0.0))\n",
    "    \n",
    "            if long_c.sum() > 0 and short_c.sum() > 0:\n",
    "                final_long = self.desired_long * long_c / long_c.sum()\n",
    "                final_short = self.desired_short * short_c / short_c.sum()\n",
    "                self.current_weights = final_long - final_short\n",
    "            elif long_c.sum() > 0:\n",
    "                self.current_weights = long_c / long_c.sum()\n",
    "            elif short_c.sum() > 0:\n",
    "                self.current_weights = -short_c / short_c.sum()\n",
    "            else:\n",
    "                self.current_weights = np.ones(len(raw)) / len(raw)\n",
    "    \n",
    "            turnover = np.sum(np.abs(self.current_weights - prev_weights))\n",
    "        else:\n",
    "            # Passive reweighting between rebalances\n",
    "            returns_today = np.array([\n",
    "                self.data.loc[self.current_step, f\"Actual_Return_{etf}\"]\n",
    "                for etf in self.etf_list\n",
    "            ])\n",
    "            self.current_weights *= (1.0 + returns_today)\n",
    "            self.current_weights /= np.sum(np.abs(self.current_weights))\n",
    "            turnover = 0.0\n",
    "    \n",
    "        # Check for termination\n",
    "        if next_step >= len(self.data):\n",
    "            reward = 0.0\n",
    "            terminated = True\n",
    "        else:\n",
    "            asset_returns = np.array([\n",
    "                self.data.loc[next_step, f\"Actual_Return_{etf}\"]\n",
    "                for etf in self.etf_list\n",
    "            ])\n",
    "            portfolio_return = np.dot(self.current_weights, asset_returns)\n",
    "            portfolio_return = np.nan_to_num(portfolio_return, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "            self.cumulative_wealth *= (1.0 + portfolio_return)\n",
    "            reward = self.calculate_reward(portfolio_return, asset_returns, turnover)\n",
    "            reward -= self.transaction_cost_rate * turnover\n",
    "            terminated = next_step >= len(self.data) - 1\n",
    "    \n",
    "        self.current_step += 1\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "    \n",
    "        # advance time and return observation, reward, termination flags\n",
    "        self.current_step += 1\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Hyperparameter tuning function\n",
    "# ---------------------------------------------------------------------\n",
    "def equal_weight_baseline(date):\n",
    "    return np.ones(len(etf_list)) / len(etf_list)\n",
    "\n",
    "def validate_and_tune(train_data: pd.DataFrame, val_data: pd.DataFrame,\n",
    "                      etf_list: List[str], cfg: TrainingConfig,\n",
    "                      random_seed: int) -> Dict[str, float]:\n",
    "\n",
    "    param_dist = {\n",
    "        'learning_rate': [5e-4, 1e-5, 5e-5],\n",
    "        'n_steps': [20, 40],\n",
    "        'batch_size': [10, 20],\n",
    "        'gamma': [0.95, 0.98],\n",
    "        'risk_coefficient': [0.1, 0.5, 1.0, 5.0],\n",
    "        'lambda_turnover': [0.005, 0.01, 0.05],\n",
    "        'lambda_hhi': [0.5, 1, 5],\n",
    "        'seed': [random_seed, random_seed + 11, random_seed + 23]  # explicitly vary seeds\n",
    "    }\n",
    "\n",
    "    # Crucial: explicitly pass random_seed to ParameterSampler\n",
    "    sampled_params = list(ParameterSampler(\n",
    "        param_dist, n_iter=cfg.n_iter_tuning, random_state=random_seed\n",
    "    ))\n",
    "\n",
    "    best_reward = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for params in sampled_params:\n",
    "        seed = params.pop('seed')\n",
    "        risk_coeff = params.pop('risk_coefficient', cfg.default_risk_coeff)\n",
    "        lambda_turnover = params.pop('lambda_turnover', cfg.lambda_turnover)\n",
    "        lambda_hhi = params.pop('lambda_hhi', cfg.lambda_hhi)\n",
    "        \n",
    "        set_global_seed(seed)\n",
    "\n",
    "\n",
    "        env = make_vec_env(lambda: PortfolioEnv(\n",
    "            train_data, etf_list, 'mean_cvar', risk_coeff,\n",
    "            cfg.rebalance_period, cfg.lookback_period,\n",
    "            use_baseline=True, baseline_fn=equal_weight_baseline,\n",
    "            transaction_cost_rate=0.0005,\n",
    "            lambda_turnover=lambda_turnover,\n",
    "            lambda_hhi=lambda_hhi\n",
    "        ), n_envs=1, seed=seed)\n",
    "\n",
    "        model = PPO('MlpPolicy', env, ent_coef=0.01, clip_range=0.2, seed=seed, **params, verbose=0)\n",
    "        model.learn(total_timesteps=cfg.tuning_timesteps)\n",
    "\n",
    "        # Evaluate explicitly on validation set\n",
    "        val_env = PortfolioEnv(\n",
    "            val_data, etf_list, 'mean_cvar', risk_coeff,\n",
    "            cfg.rebalance_period, cfg.lookback_period,\n",
    "            use_baseline=True, baseline_fn=equal_weight_baseline,\n",
    "            transaction_cost_rate=0.0005,\n",
    "            lambda_turnover=lambda_turnover,\n",
    "            lambda_hhi=lambda_hhi\n",
    "        )\n",
    "\n",
    "        obs, _ = val_env.reset(seed=seed)\n",
    "        done, total_reward = False, 0.0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_params = params.copy()\n",
    "            best_params.update({\n",
    "                'risk_coefficient': risk_coeff,\n",
    "                'lambda_turnover': lambda_turnover,\n",
    "                'lambda_hhi': lambda_hhi,\n",
    "                'seed': seed\n",
    "            })\n",
    "\n",
    "    return best_params\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Training and prediction function\n",
    "# ---------------------------------------------------------------------\n",
    "def train_and_predict(train_df: pd.DataFrame, val_df: pd.DataFrame,\n",
    "                      pred_df: pd.DataFrame, etf_list: List[str],\n",
    "                      cfg: TrainingConfig, best_params: Dict[str, float],\n",
    "                      model_path: str) -> Tuple[List[List[float]], List[pd.Timestamp]]:\n",
    "    risk_coeff = best_params.pop('risk_coefficient')\n",
    "    seed = best_params.pop('seed')\n",
    "    set_global_seed(seed)\n",
    "\n",
    "    # Initialize training environment\n",
    "    env_train = make_vec_env(\n",
    "        lambda: PortfolioEnv(\n",
    "            train_df, etf_list,\n",
    "            'mean_cvar', risk_coeff,\n",
    "            cfg.rebalance_period,\n",
    "            cfg.lookback_period,\n",
    "            use_baseline=True,\n",
    "            baseline_fn=equal_weight_baseline,\n",
    "            transaction_cost_rate=cfg.transaction_cost_rate,\n",
    "\t\t\tdesired_long=cfg.desired_long,\n",
    "\t\t    desired_short=cfg.desired_short,\n",
    "\t\t    weight_bounds=cfg.weight_bounds,\n",
    "            lambda_turnover=cfg.lambda_turnover,\n",
    "            lambda_hhi=cfg.lambda_hhi\n",
    "        ),\n",
    "        n_envs=1, seed=seed\n",
    "    )\n",
    "\n",
    "    policy_kwargs = dict(net_arch=list(cfg.policy_arch))\n",
    "    model = PPO(\n",
    "        'MlpPolicy', env_train,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        ent_coef=0.01,\n",
    "        clip_range=0.2,\n",
    "        seed=seed,\n",
    "        verbose=0,\n",
    "        **best_params\n",
    "    )\n",
    "\n",
    "    best_val_reward = -np.inf\n",
    "    no_improve = 0\n",
    "\n",
    "    # Early stopping loop\n",
    "    for step in range(0, cfg.max_timesteps, cfg.incremental_timesteps):\n",
    "        model.learn(total_timesteps=cfg.incremental_timesteps)\n",
    "\n",
    "        # Initialize validation environment\n",
    "        val_env = PortfolioEnv(\n",
    "            val_df, etf_list,\n",
    "            'mean_cvar', risk_coeff,\n",
    "            cfg.rebalance_period,\n",
    "            cfg.lookback_period,\n",
    "            use_baseline=True,\n",
    "            baseline_fn=equal_weight_baseline,\n",
    "            transaction_cost_rate=cfg.transaction_cost_rate,\n",
    "\t\t\tdesired_long=cfg.desired_long,\n",
    "\t\t    desired_short=cfg.desired_short,\n",
    "\t\t    weight_bounds=cfg.weight_bounds,\n",
    "            lambda_turnover=cfg.lambda_turnover,\n",
    "            lambda_hhi=cfg.lambda_hhi\n",
    "        )\n",
    "\n",
    "        obs, _ = val_env.reset(seed=seed)\n",
    "        done = False\n",
    "        val_reward = 0.0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            val_reward += reward\n",
    "\n",
    "        if val_reward > best_val_reward:\n",
    "            best_val_reward = val_reward\n",
    "            no_improve = 0\n",
    "            model.save(model_path)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= cfg.patience:\n",
    "                break\n",
    "\n",
    "    # Load best model and predict on pred_df\n",
    "    best_model = PPO.load(model_path)\n",
    "\n",
    "    env_pred = PortfolioEnv(\n",
    "        pred_df, etf_list,\n",
    "        reward_type='mean_cvar',  # Use Mean-CVaR reward explicitly\n",
    "        risk_coefficient=risk_coeff,\n",
    "        rebalance_period=cfg.rebalance_period,\n",
    "        lookback_period=cfg.lookback_period,\n",
    "        use_baseline=False,  # Set baseline to False for delta actions\n",
    "        transaction_cost_rate=cfg.transaction_cost_rate,\n",
    "\t\tdesired_long=cfg.desired_long,\n",
    "\t\tdesired_short=cfg.desired_short,\n",
    "\t\tweight_bounds=cfg.weight_bounds,\n",
    "        lambda_turnover=cfg.lambda_turnover,\n",
    "        lambda_hhi=cfg.lambda_hhi\n",
    "    )\n",
    "\n",
    "    obs, _ = env_pred.reset()\n",
    "    done = False\n",
    "    weights_list, dates_list = [], []\n",
    "\n",
    "    while not done:\n",
    "        if env_pred.current_step >= cfg.lookback_period and (\n",
    "            env_pred.current_step % cfg.rebalance_period == 0\n",
    "        ):\n",
    "            action, _ = best_model.predict(obs, deterministic=True)\n",
    "            obs, _, done, _, _ = env_pred.step(action)  # step first, then record\n",
    "\n",
    "            # Record weights AFTER applying the action\n",
    "            weights_list.append(env_pred.current_weights.tolist())\n",
    "            dates_list.append(env_pred.data.loc[env_pred.current_step, 'Date'])\n",
    "        else:\n",
    "            obs, _, done, _, _ = env_pred.step(np.zeros(len(etf_list), dtype=np.float32))\n",
    "\n",
    "    return weights_list, dates_list\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Data loading and overall training loop\n",
    "# ---------------------------------------------------------------------\n",
    "cfg = TrainingConfig(model_retrain=False)\n",
    "\n",
    "# Load your prepared Stage‑2 dataset and price data\n",
    "data = pd.read_csv('stage2_rl_observations_optimized_10ETFs.csv', parse_dates=['Date'])\n",
    "price_data = pd.read_csv('stock_prices_10ETFs.csv')\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'], utc=True).dt.tz_localize(None)\n",
    "price_cols = {col: f'Price_{col}' for col in price_data.columns if col != 'Date'}\n",
    "price_data.rename(columns=price_cols, inplace=True)\n",
    "\n",
    "merged_data = pd.merge(data, price_data, on='Date', how='inner').reset_index(drop=True)\n",
    "if len(merged_data) != len(data):\n",
    "    print(\"Warning: data length mismatch after merge.\")\n",
    "\n",
    "etf_list = ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU']\n",
    "feature_data = add_stable_features(merged_data, etf_list)\n",
    "feature_data = filter_features(feature_data, include_predicted_returns=True, include_shap_metrics=True)\n",
    "\n",
    "# Rolling windows\n",
    "total_len = len(feature_data)\n",
    "# start_indices = range(0,\n",
    "#                       total_len - (cfg.train_window_days + cfg.validation_window_days + cfg.prediction_window_days),\n",
    "#                       cfg.prediction_window_days)\n",
    "\n",
    "start_indices = []\n",
    "current_start = 0\n",
    "\n",
    "while True:\n",
    "    train_start = current_start\n",
    "    train_end = train_start + cfg.train_window_days\n",
    "    val_end = train_end + cfg.validation_window_days\n",
    "    pred_end = val_end + cfg.prediction_window_days\n",
    "    \n",
    "    if pred_end > total_len:\n",
    "        break\n",
    "    \n",
    "    start_indices.append(current_start)\n",
    "    \n",
    "    # move to next window ensuring continuity without gap\n",
    "    current_start += cfg.prediction_window_days - cfg.rebalance_period\n",
    "\n",
    "# Prepare directory for outputs\n",
    "output_dir = 'stage2_iterations'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Collect metrics for all iterations\n",
    "summary_records = []\n",
    "for iter_num in range(cfg.num_iterations):\n",
    "\t\n",
    "    iter_seed = cfg.base_seed + iter_num\n",
    "    set_global_seed(iter_seed)\n",
    "    tuned_seed = iter_seed\n",
    "    iter_dir = os.path.join(output_dir, f'iteration_{iter_num:02d}')\n",
    "    os.makedirs(iter_dir, exist_ok=True)\n",
    "    \n",
    "    previous_model_path = None\n",
    "    iter_returns = []\n",
    "    print(f\"\\nStarting iteration {iter_num+1}/{cfg.num_iterations} (Seed: {tuned_seed}) at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\t\n",
    "    for idx, start_idx in enumerate(start_indices):\n",
    "        window_start_time = time.time()\n",
    "        print(f\"  - Starting window {idx+1}/{len(start_indices)} at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "        train_start = start_idx\n",
    "        train_end = train_start + cfg.train_window_days\n",
    "        val_start = train_end\n",
    "        val_end = val_start + cfg.validation_window_days\n",
    "        pred_start = val_end\n",
    "        pred_end = pred_start + cfg.prediction_window_days\n",
    "\n",
    "        train_df = feature_data.iloc[train_start:train_end].reset_index(drop=True)\n",
    "        val_df = feature_data.iloc[val_start:val_end].reset_index(drop=True)\n",
    "        pred_df = feature_data.iloc[pred_start:pred_end].reset_index(drop=True)\n",
    "        \n",
    "        train_df.ffill(inplace=True)\n",
    "        train_df.bfill(inplace=True)\n",
    "        \n",
    "        val_df.ffill(inplace=True)\n",
    "        val_df.bfill(inplace=True)\n",
    "        \n",
    "        pred_df.ffill(inplace=True)\n",
    "        pred_df.bfill(inplace=True)\n",
    "        \n",
    "\n",
    "        feature_cols = [c for c in train_df.columns if c != 'Date' and not c.startswith('Actual_Return')]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_df[feature_cols])\n",
    "\t\t\n",
    "        scale = scaler.scale_\n",
    "        scale[scale < 1e-8] = 1.0\n",
    "        scaler.scale_ = scale\n",
    "        \n",
    "        train_scaled = train_df.copy()\n",
    "        train_scaled[feature_cols] = scaler.transform(train_df[feature_cols])\n",
    "        val_scaled = val_df.copy()\n",
    "        val_scaled[feature_cols] = scaler.transform(val_df[feature_cols])\n",
    "        pred_scaled = pred_df.copy()\n",
    "        pred_scaled[feature_cols] = scaler.transform(pred_df[feature_cols])\n",
    "\n",
    "        # Explicit seed handling\n",
    "        if idx == 0:\n",
    "            best_params = validate_and_tune(\n",
    "                train_scaled, val_scaled, etf_list, cfg, random_seed=iter_seed\n",
    "            )\n",
    "            \n",
    "            # Fetch tuned seed explicitly from best_params\n",
    "            tuned_seed = best_params.get('seed', iter_seed)\n",
    "        else:\n",
    "            # Keep explicitly using previously found best_params\n",
    "            best_params = best_params.copy()\n",
    "\n",
    "        seed = tuned_seed\n",
    "        set_global_seed(seed)\n",
    "\n",
    "        window_dir = os.path.join(iter_dir, f'window_{idx:02d}')\n",
    "        os.makedirs(window_dir, exist_ok=True)\n",
    "        model_path = os.path.join(window_dir, 'best_ppo.zip')\n",
    "\n",
    "        # Use tuned parameters explicitly\n",
    "        risk_coeff = best_params.get('risk_coefficient', cfg.default_risk_coeff)\n",
    "        lambda_turnover = best_params.get('lambda_turnover', cfg.lambda_turnover)\n",
    "        lambda_hhi = best_params.get('lambda_hhi', cfg.lambda_hhi)\n",
    "        \n",
    "\n",
    "        env_train = make_vec_env(lambda: PortfolioEnv(\n",
    "                train_scaled, etf_list, 'mean_cvar', risk_coeff, cfg.rebalance_period, cfg.lookback_period,\n",
    "                use_baseline=True,\n",
    "                baseline_fn=equal_weight_baseline,\n",
    "                transaction_cost_rate=0.0005,\n",
    "                desired_long=cfg.desired_long,\n",
    "                desired_short=cfg.desired_short,\n",
    "                weight_bounds=cfg.weight_bounds,\n",
    "                lambda_turnover=lambda_turnover,\n",
    "                lambda_hhi=lambda_hhi\n",
    "            ), n_envs=1, seed=seed)\n",
    "\n",
    "        policy_kwargs = dict(net_arch=list(cfg.policy_arch))\n",
    "\n",
    "        if previous_model_path and os.path.exists(previous_model_path) and not cfg.model_retrain:\n",
    "            print(f'load the exising model from {previous_model_path} and retrain')\n",
    "            model = PPO.load(previous_model_path, env=env_train)\n",
    "            model.set_env(env_train)\n",
    "        else:\n",
    "            print(f'triam new model and saved under {model_path}')\n",
    "            model = PPO('MlpPolicy', env_train, policy_kwargs=policy_kwargs,\n",
    "                ent_coef=0.01,\n",
    "                clip_range=0.2,\n",
    "                seed=seed,\n",
    "                learning_rate=best_params.get('learning_rate', 1e-4),\n",
    "                n_steps=best_params.get('n_steps', 20),\n",
    "                batch_size=best_params.get('batch_size', 10),\n",
    "                gamma=best_params.get('gamma', 0.98),\n",
    "                verbose=0)\n",
    "            \n",
    "        best_val_reward = -np.inf\n",
    "        no_improve = 0\n",
    "        training_log = []\n",
    "\n",
    "        for step in range(0, cfg.max_timesteps, cfg.incremental_timesteps):\n",
    "            model.learn(total_timesteps=cfg.incremental_timesteps)\n",
    "\n",
    "            val_env = PortfolioEnv(val_scaled, etf_list, 'mean_cvar', risk_coeff,\n",
    "                                   cfg.rebalance_period, cfg.lookback_period,\n",
    "                                   use_baseline=True,\n",
    "                                   baseline_fn=equal_weight_baseline,\n",
    "                                   transaction_cost_rate=0.0005, \t\t\t\n",
    "\t\t\t\t\t\t\t\t   desired_long=cfg.desired_long,\n",
    "\t\t\t\t\t\t\t\t   desired_short=cfg.desired_short,\n",
    "\t\t\t\t\t\t\t\t   weight_bounds=cfg.weight_bounds, lambda_turnover=lambda_turnover,\n",
    "                lambda_hhi=lambda_hhi)\n",
    "            obs, _ = val_env.reset(seed=seed)\n",
    "            done, val_reward = False, 0.0\n",
    "\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, _, _ = val_env.step(action)\n",
    "                val_reward += reward\n",
    "\n",
    "            training_log.append({'training_step': step + cfg.incremental_timesteps, 'validation_reward': val_reward})\n",
    "\n",
    "            if val_reward > best_val_reward:\n",
    "                best_val_reward = val_reward\n",
    "                no_improve = 0\n",
    "                model.save(model_path)\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= cfg.patience:\n",
    "                    break\n",
    "\n",
    "        pd.DataFrame(training_log).to_csv(os.path.join(window_dir, 'training_validation_log.csv'), index=False)\n",
    "        previous_model_path = model_path\n",
    "\n",
    "        best_model = PPO.load(model_path)\n",
    "        env_pred = PortfolioEnv(\n",
    "            pred_scaled, etf_list,\n",
    "            reward_type='mean_cvar',\n",
    "            risk_coefficient=risk_coeff,\n",
    "            rebalance_period=cfg.rebalance_period,\n",
    "            lookback_period=cfg.lookback_period,\n",
    "            use_baseline=False,\n",
    "            transaction_cost_rate=0.0005,\n",
    "\t\t\tdesired_long=cfg.desired_long,\n",
    "\t\t    desired_short=cfg.desired_short,\n",
    "\t\t    weight_bounds=cfg.weight_bounds, lambda_turnover=lambda_turnover,\n",
    "                lambda_hhi=lambda_hhi\n",
    "        )\n",
    "\n",
    "        obs, _ = env_pred.reset()\n",
    "        done = False\n",
    "        weights_list, dates_list = [], []\n",
    "\n",
    "        while not done:\n",
    "            if env_pred.current_step >= cfg.lookback_period and (\n",
    "                env_pred.current_step % cfg.rebalance_period == 0\n",
    "            ):\n",
    "                action, _ = best_model.predict(obs, deterministic=True)\n",
    "                obs, _, done, _, _ = env_pred.step(action)\n",
    "\n",
    "                weights_list.append(env_pred.current_weights.tolist())\n",
    "                dates_list.append(env_pred.data.loc[env_pred.current_step, 'Date'])\n",
    "            else:\n",
    "                obs, _, done, _, _ = env_pred.step(np.zeros(len(etf_list), dtype=np.float32))\n",
    "\n",
    "        weights_df = pd.DataFrame(weights_list, columns=etf_list)\n",
    "        weights_df.insert(0, 'Date', dates_list)\n",
    "        weights_df.to_csv(os.path.join(window_dir, 'weights.csv'), index=False)\n",
    "\n",
    "        cum_wealth = 1.0\n",
    "        returns_log = []\n",
    "\n",
    "        for t, w in zip(dates_list, weights_list):\n",
    "            step_idx = pred_scaled[pred_scaled['Date'] == t].index[0]\n",
    "            asset_returns = np.array([\n",
    "                pred_scaled.loc[step_idx + 1, f'Actual_Return_{etf}']\n",
    "                for etf in etf_list\n",
    "            ])\n",
    "            port_ret = np.dot(w, asset_returns)\n",
    "            cum_wealth *= (1 + port_ret)\n",
    "            returns_log.append({'Date': t, 'Portfolio_Return': port_ret, 'Cumulative_Wealth': cum_wealth})\n",
    "\n",
    "        iter_returns.append(cum_wealth - 1.0)\n",
    "\n",
    "        pd.DataFrame(returns_log).to_csv(os.path.join(window_dir, 'returns_log.csv'), index=False)\n",
    "        window_end_time = time.time()\n",
    "        elapsed_window_time = window_end_time - window_start_time\n",
    "        print(f\"  - Completed window {idx+1}/{len(start_indices)} in {elapsed_window_time/60:.2f} minutes.\")\n",
    "\n",
    "    mean_ret = np.mean(iter_returns)\n",
    "    std_ret = np.std(iter_returns, ddof=1)\n",
    "    sharpe = (mean_ret / std_ret) * np.sqrt(len(iter_returns)) if std_ret != 0 else np.nan\n",
    "    summary_records.append({\n",
    "        'iteration': iter_num,\n",
    "        'seed': iter_seed,\n",
    "        'mean_return': mean_ret,\n",
    "        'sharpe': sharpe\n",
    "    })\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()  # Only if you're using GPU explicitly\n",
    "\n",
    "summary_df = pd.DataFrame(summary_records)\n",
    "summary_df.to_csv(os.path.join(output_dir, 'iterations_summary.csv'), index=False)\n",
    "\n",
    "t_stat, p_val = ttest_1samp(summary_df['mean_return'], 0.0)\n",
    "with open(os.path.join(output_dir, 't_test_result.csv'), 'w') as f:\n",
    "    f.write(f\"t-statistic,{t_stat}\\np-value,{p_val}\\n\")\n",
    "\n",
    "print(summary_df)\n",
    "print(f\"Overall t-statistic={t_stat:.3f}, p-value={p_val:.3f}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()  # Only if you're using GPU explicitly\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-01T21:53:40.108753Z",
     "start_time": "2025-08-01T12:51:58.621612Z"
    }
   },
   "id": "cfe656732378c6e7",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jun chenprevious_model_path and os.path.exists(previous_model_path) and not cfg.model_retrain"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-01T04:34:59.094636Z",
     "start_time": "2025-08-01T04:34:59.087630Z"
    }
   },
   "id": "7aba3a43020d11ca",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model_retrain"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-01T04:35:15.621027Z",
     "start_time": "2025-08-01T04:35:15.603008Z"
    }
   },
   "id": "93ecdbb778338c51",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'stage2_iterations\\\\iteration_01\\\\window_15\\\\best_ppo.zip'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reprevious_model_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-30T02:01:42.443625Z",
     "start_time": "2025-07-30T02:01:42.412378Z"
    }
   },
   "id": "5556d1f98d4833ec",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined weights saved to: stage2_iterations\\combined_weights.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "output_dir = 'stage2_iterations'  # Adjust if your path is different\n",
    "pattern = os.path.join(output_dir, 'iteration_*', 'window_*', 'weights.csv')\n",
    "\n",
    "# Find all weight files matching the pattern\n",
    "files = glob.glob(pattern)\n",
    "\n",
    "# Initialize an empty list to collect DataFrames\n",
    "all_weights = []\n",
    "\n",
    "for file_path in files:\n",
    "    # Extract iteration and window numbers\n",
    "    parts = file_path.split(os.sep)\n",
    "    iteration = int(parts[-3].split('_')[1])\n",
    "    window = int(parts[-2].split('_')[1])\n",
    "\n",
    "    # Load weights file\n",
    "    df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "    # Add columns for iteration and window\n",
    "    df.insert(0, 'Window', window)\n",
    "    df.insert(0, 'Iteration', iteration)\n",
    "\n",
    "    # Append to the list\n",
    "    all_weights.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(all_weights, ignore_index=True)\n",
    "\n",
    "# Sort by iteration, window, and date\n",
    "combined_df.sort_values(['Iteration', 'Window', 'Date'], inplace=True)\n",
    "\n",
    "# Save combined data\n",
    "combined_df.to_csv(os.path.join(output_dir, 'combined_weights.csv'), index=False)\n",
    "\n",
    "print(f\"Combined weights saved to: {os.path.join(output_dir, 'combined_weights.csv')}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-01T23:39:55.827471Z",
     "start_time": "2025-08-01T23:39:55.604420Z"
    }
   },
   "id": "9b01351041b154b6",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2b0f510c1ff23295",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "10563904fd258e2f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "637f2b485e42e5a6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# start of stage 2 training\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "SEED = 42\n",
    "def set_global_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    set_random_seed(seed)\n",
    "\n",
    "set_global_seed(SEED)\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar', risk_coefficient=0.5, rebalance_period=21, lookback_period=21):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(etf_list),), dtype=np.float32)\n",
    "\n",
    "        # Explicitly select feature columns (excluding Date and returns used only for calculating reward)\n",
    "        self.feature_cols = [col for col in data.columns if col not in ['Date'] and not col.startswith('Actual_Return')]\n",
    "        self.num_features_per_day = len(self.feature_cols)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.num_features_per_day * self.lookback_period,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(etf_list)] * len(etf_list))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(self.etf_list)] * len(self.etf_list))\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        next_step = self.current_step + 1\n",
    "\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            # v2 long short\n",
    "            desired_long = 1.20  # 120% long exposure explicitly\n",
    "            desired_short = 0.20  # 20% short exposure explicitly\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                # Normal 120/20 explicitly0\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                # Only long explicitly: default realistically to 100% long\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                # Only short explicitly (unrealistic), fallback clearly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                # All zeros explicitly: fallback explicitly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            # Apply explicit clipping\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate explicitly after clipping\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            # Final explicit normalization after clipping\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)  # exactly 100% long\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                # Realistic fallback explicitly: equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "            self.current_weights = final_weights\n",
    "            \n",
    "            # v1 softmax normalization\n",
    "            \n",
    "            # temperature = 0.5  # Explicitly lower for higher concentration (try 0.2 to 0.8)\n",
    "            # scaled_action = action / temperature\n",
    "            # self.current_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "\n",
    "        else:\n",
    "            returns_today = np.array([self.data.loc[self.current_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            self.current_weights *= (1 + returns_today)\n",
    "            self.current_weights /= np.sum(self.current_weights)\n",
    "\n",
    "        if next_step >= len(self.data):\n",
    "            terminated = True\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            returns = np.array([self.data.loc[next_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            portfolio_return = np.dot(self.current_weights, returns)\n",
    "            self.cumulative_wealth *= (1 + portfolio_return)\n",
    "            reward = self.calculate_reward(portfolio_return, returns)\n",
    "            terminated = next_step >= len(self.data) - 1\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "        # def _get_obs(self):\n",
    "        #     obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        #     obs_window = obs_window.drop(columns=['Date']).values.flatten().astype(np.float32)\n",
    "        #     return obs_window\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        obs_window = obs_window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "        return obs_window\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns):\n",
    "        if self.reward_type == 'cumulative_return':\n",
    "            return self.cumulative_wealth - 1.0\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            return np.log(self.cumulative_wealth)\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100 * alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            return portfolio_return - self.risk_coefficient * cvar\n",
    "        else:\n",
    "            raise ValueError('Invalid reward type')\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_stable_features(df, etf_list):\n",
    "    data = df.copy()\n",
    "\n",
    "    for etf in etf_list:\n",
    "        price_col = f'Price_{etf}'\n",
    "\n",
    "        # Volatility (20-day)\n",
    "        data[f'Volatility_{etf}'] = data[price_col].pct_change().rolling(20).std()\n",
    "\n",
    "        # Momentum indicators (returns over 5, 10, 20 days)\n",
    "        data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
    "        data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
    "        data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
    "\n",
    "        # Moving averages (5-day and 20-day)\n",
    "        data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
    "        data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
    "\n",
    "        # Moving average crossover (5-day MA - 20-day MA)\n",
    "        data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
    "\n",
    "    # Drop NaN values due to rolling calculations\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # Explicit patterns to identify columns\n",
    "    predicted_return_pattern = 'Predicted_Return'\n",
    "    shap_metric_pattern = 'SHAP'\n",
    "\n",
    "    # Exclude Predicted Returns explicitly if requested\n",
    "    if not include_predicted_returns:\n",
    "        predicted_cols = [col for col in df_filtered.columns if predicted_return_pattern in col]\n",
    "        df_filtered.drop(columns=predicted_cols, inplace=True)\n",
    "        print(f\"Excluded predicted return columns: {predicted_cols}\")\n",
    "\n",
    "    # Exclude SHAP-related metrics explicitly if requested\n",
    "    if not include_shap_metrics:\n",
    "        shap_cols = [col for col in df_filtered.columns if shap_metric_pattern in col]\n",
    "        df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "        print(f\"Excluded SHAP-related columns: {shap_cols}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# ETFs\n",
    "etf_list = ['XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLY', 'XLV', 'XLU']\n",
    "\n",
    "# etf_list = ['BA',\n",
    "# 'AMGN',\n",
    "# 'DIS',\n",
    "# 'NKE',\n",
    "# 'HON',\n",
    "# 'MMM',\n",
    "# 'CAT',\n",
    "# 'KO',\n",
    "# 'PG',\n",
    "# 'AXP',\n",
    "# 'JPM',\n",
    "# 'MCD',\n",
    "# 'HD',\n",
    "# 'AAPL',\n",
    "# 'CSCO',\n",
    "# 'IBM',\n",
    "# 'MSFT',\n",
    "# 'TRV',\n",
    "# 'UNH',\n",
    "# 'CVX',\n",
    "# 'JNJ',\n",
    "# 'MRK',\n",
    "# 'AMZN',\n",
    "# 'WMT',\n",
    "# 'INTC',\n",
    "# 'VZ']\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 5e-5],\n",
    "    'n_steps': [20, 40],\n",
    "    'batch_size': [5, 10],\n",
    "    'gamma': [0.98, 0.99]\n",
    "}\n",
    "consolidated_file = 'stage2_rl_observations_optimized_10ETFs.csv'\n",
    "reward_type = 'mean_cvar'\n",
    "# data = pd.read_csv(consolidated_file, parse_dates=['Date'])\n",
    "# data = data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "data = pd.read_csv('stage2_rl_observations_optimized_10ETFs.csv', parse_dates=['Date'])\n",
    "price_data = pd.read_csv('stock_prices_10ETFs.csv')\n",
    "# price_data = pd.read_csv('stock_prices_10ETFs.csv')\n",
    "# Convert the Date column in price data, handling the timezone correctly\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'], utc=True)\n",
    "price_data['Date'] = price_data['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Rename price columns explicitly to 'price_{ticker}'\n",
    "price_cols = {col: f'Price_{col}' for col in price_data.columns if col != 'Date'}\n",
    "price_data.rename(columns=price_cols, inplace=True)\n",
    "\n",
    "# Merge datasets on Date\n",
    "merged_data = pd.merge(data, price_data, on='Date', how='inner')\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "# Check if merge was successful\n",
    "if len(merged_data) != len(data):\n",
    "    print(f\"Warning: Data length mismatch after merging (Original: {len(data)}, Merged: {len(merged_data)}).\")\n",
    "else:\n",
    "    print(\"Merged successfully with aligned dates.\")\n",
    "\n",
    "data_with_features_raw = add_stable_features(merged_data, etf_list)\n",
    "data_with_features_raw.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Usage Example clearly for benchmark (only price metrics, no predicted return or SHAP):\n",
    "data_with_features = filter_features(data_with_features_raw, \n",
    "                                 include_predicted_returns=True, \n",
    "                                 include_shap_metrics=True)\n",
    "################### override data to use SHAP only\n",
    "# data_with_features = data\n",
    "################### END override \n",
    "\n",
    "# Define your rolling window lengths clearly:\n",
    "train_window_days = 252 * 10\n",
    "validation_window_days = 126\n",
    "prediction_window_days = 126\n",
    "lookback_period = 10\n",
    "rebalance_period = 10\n",
    "\n",
    "start_indices = range(0, len(data) - (train_window_days + validation_window_days + prediction_window_days), prediction_window_days)\n",
    "all_weights = []\n",
    "model_path = 'ppo_single_train_best_model_10ETFs.zip'\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "def validate_and_tune(train_data, val_data, reward_type, rebalance_period=10, lookback_period=10, n_iter=8, timesteps=5000):\n",
    "    best_reward, best_params = -np.inf, None\n",
    "\n",
    "    # Narrow and meaningful parameter distribution\n",
    "    param_dist = {\n",
    "        'learning_rate': [3e-4, 1e-4],\n",
    "        'n_steps': [20, 40],\n",
    "        'batch_size': [10, 20],\n",
    "        'gamma': [0.95, 0.98],\n",
    "        'risk_coefficient': [0.1, 0.5, 1.0] if reward_type in ['mean_var', 'mean_cvar'] else [0.5],\n",
    "        'seed': [42, 100, 2024, 12345, 579]\n",
    "    }\n",
    "\n",
    "    sampled_params = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=SEED))\n",
    "\n",
    "    for params in sampled_params:\n",
    "        seed = params.pop('seed')\n",
    "        risk_coeff = params.pop('risk_coefficient', 0.5)\n",
    "        set_global_seed(seed)\n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1, seed=seed)\n",
    "        model = PPO('MlpPolicy', env,\n",
    "                    ent_coef=0.01,    # explicitly encourages exploration\n",
    "                    clip_range=0.2,\n",
    "                    seed=seed,\n",
    "                    **params, verbose=0)\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        val_env = PortfolioEnv(val_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        obs, _ = val_env.reset(seed=seed)\n",
    "        done, total_reward = False, 0\n",
    "\n",
    "        while not done:\n",
    "            # num_samples = 100  # Recommended starting point\n",
    "            # action_samples = []\n",
    "            # \n",
    "            # for _ in range(num_samples):\n",
    "            #     sampled_action, _ = model.predict(obs, deterministic=False)  # obs directly\n",
    "            #     action_samples.append(sampled_action)\n",
    "            # \n",
    "            # action = np.mean(action_samples, axis=0)\n",
    "            \n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_params = {**params, 'risk_coefficient': risk_coeff, 'seed': seed}\n",
    "    with open('best_params.json', 'w') as f:\n",
    "        json.dump(best_params, f)\n",
    "    return best_params\n",
    "\n",
    "def scale_data(df, feature_cols, scaler):\n",
    "    scaled_features = scaler.transform(df[feature_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=feature_cols, index=df.index)\n",
    "\n",
    "    # Re-add columns that were not scaled (e.g., Date, Actual_Return_*)\n",
    "    for col in df.columns:\n",
    "        if col not in feature_cols:\n",
    "            scaled_df[col] = df[col].values\n",
    "\n",
    "    # Keep original column order\n",
    "    scaled_df = scaled_df[df.columns]\n",
    "    return scaled_df\n",
    "\n",
    "# Main execution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for idx, start_idx in enumerate(start_indices):\n",
    "    # for start_idx in range(0, 252*2, 252):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Explicit indices for training, validation, and prediction datasets\n",
    "    train_start_idx = start_idx\n",
    "    train_end_idx = train_start_idx + train_window_days\n",
    "\n",
    "    val_start_idx = train_end_idx\n",
    "    val_end_idx = val_start_idx + validation_window_days\n",
    "\n",
    "    pred_start_idx = val_end_idx\n",
    "    pred_end_idx = pred_start_idx + prediction_window_days\n",
    "\n",
    "    # Corresponding dates explicitly\n",
    "    train_start_date = data_with_features.loc[train_start_idx, 'Date']\n",
    "    train_end_date = data_with_features.loc[train_end_idx - 1, 'Date']\n",
    "\n",
    "    val_start_date = data_with_features.loc[val_start_idx, 'Date']\n",
    "    val_end_date = data_with_features.loc[val_end_idx - 1, 'Date']\n",
    "\n",
    "    pred_start_date = data_with_features.loc[pred_start_idx, 'Date']\n",
    "    pred_end_date = data_with_features.loc[pred_end_idx - 1, 'Date']\n",
    "\n",
    "    # Clearly print ranges for clarity\n",
    "    print(f\"Training period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "    print(f\"Validation period: {val_start_date.date()} to {val_end_date.date()}\")\n",
    "    print(f\"Prediction period: {pred_start_date.date()} to {pred_end_date.date()}\")\n",
    "\n",
    "    # Explicitly subset data accordingly\n",
    "    train_data = data_with_features.iloc[train_start_idx:train_end_idx].reset_index(drop=True)\n",
    "    val_data = data_with_features.iloc[val_start_idx:val_end_idx].reset_index(drop=True)\n",
    "    pred_data = data_with_features.iloc[pred_start_idx:pred_end_idx].reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [col for col in train_data.columns if col != 'Date' and not col.startswith('Actual_Return')]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data[feature_cols])\n",
    "\n",
    "    train_data_scaled = scale_data(train_data, feature_cols, scaler)\n",
    "    val_data_scaled = scale_data(val_data, feature_cols, scaler)\n",
    "    pred_data_scaled = scale_data(pred_data, feature_cols, scaler)\n",
    "\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    best_params = validate_and_tune(train_data_scaled, val_data_scaled, reward_type)\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "    incremental_timesteps = 3000    \n",
    "    max_timesteps = 30000\n",
    "    patience = 3\n",
    "    \n",
    "    best_val_reward = -np.inf\n",
    "    no_improve_steps = 0\n",
    "\n",
    "    # risk_coeff = best_params.pop('risk_coefficient',0.5)\n",
    "    policy_kwargs = dict(net_arch=[256, 256])\n",
    "\n",
    "    with open('best_params.json', 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "    \n",
    "    risk_coeff = best_params.pop('risk_coefficient')\n",
    "    seed = best_params.pop('seed')\n",
    "    \n",
    "    set_global_seed(seed)\n",
    "    env = make_vec_env(lambda: PortfolioEnv(train_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1, seed=seed)\n",
    "    \n",
    "    # Load previous model if exists\n",
    "    # if idx > 0 and os.path.exists(model_path):\n",
    "    #     print(f\"Loading previous model from {model_path}...\")\n",
    "    #     model = PPO.load(model_path, env=env)\n",
    "    #     model.set_env(env)\n",
    "    # else:\n",
    "    #     print(\"Initializing new PPO model...\")\n",
    "    #     model = PPO('MlpPolicy', env,\n",
    "    #                 policy_kwargs=policy_kwargs,\n",
    "    #                 ent_coef=0.01,\n",
    "    #                 clip_range=0.2,\n",
    "    #                 seed=seed, \n",
    "    #                 **best_params, verbose=0)\n",
    "     # always retrain\n",
    "    model = PPO('MlpPolicy', env,\n",
    "                    policy_kwargs=policy_kwargs,\n",
    "                    ent_coef=0.01,\n",
    "                    clip_range=0.2,\n",
    "                    seed=seed, \n",
    "                    **best_params, verbose=0)\n",
    "    # model.learn(total_timesteps=20000)\n",
    "    print(\"Starting model training with early stopping...\")\n",
    "\n",
    "    for step in range(0, max_timesteps, incremental_timesteps):\n",
    "        model.learn(total_timesteps=incremental_timesteps)\n",
    "    \n",
    "        # Evaluate on validation environment\n",
    "        val_env = PortfolioEnv(val_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        val_obs, _ = val_env.reset()\n",
    "        val_done = False\n",
    "        val_total_reward = 0.0\n",
    "    \n",
    "        while not val_done:\n",
    "            val_action, _ = model.predict(val_obs, deterministic=True)\n",
    "            # num_samples = 100  # Recommended\n",
    "            # value_action_samples = []\n",
    "            # \n",
    "            # for _ in range(num_samples):\n",
    "            #     value_sampled_action, _ = model.predict(val_obs, deterministic=False)\n",
    "            #     value_action_samples.append(value_sampled_action)\n",
    "            # \n",
    "            # val_action = np.mean(value_action_samples, axis=0)    \n",
    "            \n",
    "            val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "            val_total_reward += val_reward\n",
    "    \n",
    "        print(f\"Step: {step + incremental_timesteps}, Validation Total Reward: {val_total_reward:.4f}\")\n",
    "    \n",
    "        # Early stopping check\n",
    "        if val_total_reward > best_val_reward:\n",
    "            best_val_reward = val_total_reward\n",
    "            no_improve_steps = 0\n",
    "            # model.save(\"best_ppo_model.zip\")\n",
    "            model.save(model_path)\n",
    "            print(f\"Improved validation reward; model saved at step {step + incremental_timesteps}\")\n",
    "        else:\n",
    "            no_improve_steps += 1\n",
    "            print(f\"No improvement ({no_improve_steps}/{patience})\")\n",
    "    \n",
    "            if no_improve_steps >= patience:\n",
    "                print(\"Early stopping explicitly triggered.\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model explicitly\n",
    "    model = PPO.load(model_path)\n",
    "    print(\"Loaded the best PPO model explicitly for prediction.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Ensure historical context explicitly available in prediction\n",
    "    full_data = pd.concat([train_data_scaled, val_data_scaled, pred_data_scaled])\n",
    "    pred_data_with_history = full_data[full_data['Date'] >= (pred_start_date - pd.Timedelta(days=lookback_period))].reset_index(drop=True)\n",
    "\n",
    "    pred_env = PortfolioEnv(pred_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "    # pred_env = PortfolioEnv(pred_data_with_history, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "\n",
    "    obs, info = pred_env.reset()\n",
    "    done = False\n",
    "\n",
    "    action = np.zeros(len(etf_list), dtype=np.float32)\n",
    "\n",
    "    while not done:\n",
    "        if pred_env.current_step >= lookback_period and pred_env.current_step % pred_env.rebalance_period == 0:\n",
    "            # obs_for_agent = pred_data_with_history.drop(columns=['Date']).iloc[pred_env.current_step - lookback_period:pred_env.current_step].values.flatten().astype(np.float32)\n",
    "            # action, _ = model.predict(obs_for_agent, deterministic=True)\n",
    "\n",
    "            # v1 normalize weight\n",
    "            # action, _ = model.predict(obs, deterministic=True)\n",
    "            # use determinstic = FALSE       \n",
    "            # num_samples = 100  # Recommended\n",
    "            # action_samples = []\n",
    "            # for _ in range(num_samples):\n",
    "            #     sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "            #     action_samples.append(sampled_action)\n",
    "            # action = np.mean(action_samples, axis=0)    \n",
    "            # \n",
    "            # temperature = 0.5\n",
    "            # scaled_action = action / temperature\n",
    "            # weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "            # rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "            # all_weights.append([rebalance_date] + weights.tolist())\n",
    "\n",
    "\n",
    "            # v2 long short normalization\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # uncomment this for predictopm\n",
    "            # num_samples = 100  # Recommended\n",
    "            # action_samples = []\n",
    "            # \n",
    "            # for _ in range(num_samples):\n",
    "            #     sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "            #     action_samples.append(sampled_action)\n",
    "            # \n",
    "            # action = np.mean(action_samples, axis=0)    \n",
    "\n",
    "            # Explicitly apply your new 120/20 normalization logic (to match environment step)\n",
    "            desired_long = 1.20  # Explicitly 120% long exposure\n",
    "            desired_short = 0.20  # Explicitly 20% short exposure\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate after clipping explicitly\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "\n",
    "            rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "            all_weights.append([rebalance_date] + final_weights.tolist())\n",
    "\n",
    "        obs, _, done, _, _ = pred_env.step(action)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "columns = ['Date'] + etf_list\n",
    "weights_df = pd.DataFrame(all_weights, columns=columns)\n",
    "weights_df.to_csv('ppo_multi_year_weights_10ETFs.csv', index=False)\n",
    "print(\"Saved predictions to ppo_multi_year_weights_10ETFs.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c770edf2acf6b5a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "############################## This is start to run 25 iterations ##############################\n",
    "########################################################################################################################"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8acf6abff959b252",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ITERATION - final variable: 128/20 - retrain - 50kx30k sample - mean cvar - determinstic false with 50 - 7 yr train by 21 day test\n",
    "# start of stage 2 training\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "def set_global_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    set_random_seed(seed)\n",
    "\n",
    "set_global_seed(SEED)\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar', risk_coefficient=0.5, rebalance_period=21, lookback_period=21):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(etf_list),), dtype=np.float32)\n",
    "\n",
    "        # Explicitly select feature columns (excluding Date and returns used only for calculating reward)\n",
    "        self.feature_cols = [col for col in data.columns if col not in ['Date'] and not col.startswith('Actual_Return')]\n",
    "        self.num_features_per_day = len(self.feature_cols)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.num_features_per_day * self.lookback_period,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(etf_list)] * len(etf_list))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(self.etf_list)] * len(self.etf_list))\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        next_step = self.current_step + 1\n",
    "\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            # v2 long short\n",
    "            desired_long = 1.20  # 120% long exposure explicitly\n",
    "            desired_short = 0.20  # 20% short exposure explicitly\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                # Normal 120/20 explicitly0\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                # Only long explicitly: default realistically to 100% long\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                # Only short explicitly (unrealistic), fallback clearly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                # All zeros explicitly: fallback explicitly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            # Apply explicit clipping\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate explicitly after clipping\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            # Final explicit normalization after clipping\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)  # exactly 100% long\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                # Realistic fallback explicitly: equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "            self.current_weights = final_weights\n",
    "            \n",
    "            # v1 softmax normalization\n",
    "            \n",
    "            # temperature = 0.5  # Explicitly lower for higher concentration (try 0.2 to 0.8)\n",
    "            # scaled_action = action / temperature\n",
    "            # self.current_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "\n",
    "        else:\n",
    "            returns_today = np.array([self.data.loc[self.current_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            self.current_weights *= (1 + returns_today)\n",
    "            self.current_weights /= np.sum(self.current_weights)\n",
    "\n",
    "        if next_step >= len(self.data):\n",
    "            terminated = True\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            returns = np.array([self.data.loc[next_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            portfolio_return = np.dot(self.current_weights, returns)\n",
    "            self.cumulative_wealth *= (1 + portfolio_return)\n",
    "            reward = self.calculate_reward(portfolio_return, returns)\n",
    "            terminated = next_step >= len(self.data) - 1\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "        # def _get_obs(self):\n",
    "        #     obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        #     obs_window = obs_window.drop(columns=['Date']).values.flatten().astype(np.float32)\n",
    "        #     return obs_window\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        obs_window = obs_window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "        return obs_window\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns):\n",
    "        if self.reward_type == 'cumulative_return':\n",
    "            return self.cumulative_wealth - 1.0\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            return np.log(self.cumulative_wealth)\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100 * alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            return portfolio_return - self.risk_coefficient * cvar\n",
    "        else:\n",
    "            raise ValueError('Invalid reward type')\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_stable_features(df, etf_list):\n",
    "    data = df.copy()\n",
    "\n",
    "    for etf in etf_list:\n",
    "        price_col = f'Price_{etf}'\n",
    "\n",
    "        # Volatility (20-day)\n",
    "        data[f'Volatility_{etf}'] = data[price_col].pct_change().rolling(20).std()\n",
    "\n",
    "        # Momentum indicators (returns over 5, 10, 20 days)\n",
    "        data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
    "        data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
    "        data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
    "\n",
    "        # Moving averages (5-day and 20-day)\n",
    "        data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
    "        data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
    "\n",
    "        # Moving average crossover (5-day MA - 20-day MA)\n",
    "        data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
    "\n",
    "    # Drop NaN values due to rolling calculations\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # Explicit patterns to identify columns\n",
    "    predicted_return_pattern = 'Predicted_Return'\n",
    "    shap_metric_pattern = 'SHAP'\n",
    "\n",
    "    # Exclude Predicted Returns explicitly if requested\n",
    "    if not include_predicted_returns:\n",
    "        predicted_cols = [col for col in df_filtered.columns if predicted_return_pattern in col]\n",
    "        df_filtered.drop(columns=predicted_cols, inplace=True)\n",
    "        print(f\"Excluded predicted return columns: {predicted_cols}\")\n",
    "\n",
    "    # Exclude SHAP-related metrics explicitly if requested\n",
    "    if not include_shap_metrics:\n",
    "        shap_cols = [col for col in df_filtered.columns if shap_metric_pattern in col]\n",
    "        df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "        print(f\"Excluded SHAP-related columns: {shap_cols}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# ETFs\n",
    "etf_list = ['XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLY', 'XLV', 'XLU']\n",
    "# etf_list = ['BA',\n",
    "# 'AMGN',\n",
    "# 'DIS',\n",
    "# 'NKE',\n",
    "# 'HON',\n",
    "# 'MMM',\n",
    "# 'CAT',\n",
    "# 'KO',\n",
    "# 'PG',\n",
    "# 'AXP',\n",
    "# 'JPM',\n",
    "# 'MCD',\n",
    "# 'HD',\n",
    "# 'AAPL',\n",
    "# 'CSCO',\n",
    "# 'IBM',\n",
    "# 'MSFT',\n",
    "# 'TRV',\n",
    "# 'UNH',\n",
    "# 'CVX',\n",
    "# 'JNJ',\n",
    "# 'MRK',\n",
    "# 'AMZN',\n",
    "# 'WMT',\n",
    "# 'INTC',\n",
    "# 'VZ']\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 3e-4, 5e-4],\n",
    "    'gamma': [0.90, 0.95, 0.98],\n",
    "    'clip_range': [0.1, 0.2, 0.25],\n",
    "    'gae_lambda': [0.8, 0.9, 0.95]\n",
    "}\n",
    "consolidated_file = 'stage2_rl_observations_optimized_DIA_ETF.csv'\n",
    "reward_type = 'mean_cvar'\n",
    "# data = pd.read_csv(consolidated_file, parse_dates=['Date'])\n",
    "# data = data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "data = pd.read_csv('stage2_rl_observations_optimized_DIA_ETF.csv', parse_dates=['Date'])\n",
    "price_data = pd.read_csv('stock_prices_DIA_ETF.csv')\n",
    "\n",
    "# Convert the Date column in price data, handling the timezone correctly\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'], utc=True)\n",
    "price_data['Date'] = price_data['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Rename price columns explicitly to 'price_{ticker}'\n",
    "price_cols = {col: f'Price_{col}' for col in price_data.columns if col != 'Date'}\n",
    "price_data.rename(columns=price_cols, inplace=True)\n",
    "\n",
    "# Merge datasets on Date\n",
    "merged_data = pd.merge(data, price_data, on='Date', how='inner')\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "# Check if merge was successful\n",
    "if len(merged_data) != len(data):\n",
    "    print(f\"Warning: Data length mismatch after merging (Original: {len(data)}, Merged: {len(merged_data)}).\")\n",
    "else:\n",
    "    print(\"Merged successfully with aligned dates.\")\n",
    "\n",
    "data_with_features_raw = add_stable_features(merged_data, etf_list)\n",
    "data_with_features_raw.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Usage Example clearly for benchmark (only price metrics, no predicted return or SHAP):\n",
    "data_with_features = filter_features(data_with_features_raw, \n",
    "                                 include_predicted_returns=True, \n",
    "                                 include_shap_metrics=True)\n",
    "################### override data to use SHAP only\n",
    "# data_with_features = data\n",
    "################### END override \n",
    "\n",
    "# Define your rolling window lengths clearly:\n",
    "train_window_days = 252 * 7\n",
    "validation_window_days = 252\n",
    "prediction_window_days = 252\n",
    "lookback_period = 21\n",
    "rebalance_period = 21\n",
    "\n",
    "start_indices = range(0, len(data) - (train_window_days + validation_window_days + prediction_window_days), prediction_window_days)\n",
    "all_weights = []\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "def validate_and_tune(train_data, val_data, reward_type, rebalance_period=10, lookback_period=10, n_iter=8, timesteps=5000):\n",
    "    best_reward, best_params = -np.inf, None\n",
    "\n",
    "    # Narrow and meaningful parameter distribution\n",
    "    param_dist = {\n",
    "        'learning_rate': [3e-4, 1e-4],\n",
    "        'n_steps': [20, 40],\n",
    "        'batch_size': [10, 20],\n",
    "        'gamma': [0.95, 0.98],\n",
    "        'risk_coefficient': [0.1, 0.5, 1.0] if reward_type in ['mean_var', 'mean_cvar'] else [0.5],\n",
    "    }\n",
    "\n",
    "    sampled_params = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=42))\n",
    "\n",
    "    for params in sampled_params:\n",
    "        risk_coeff = params.pop('risk_coefficient', 0.5)\n",
    "\n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "        model = PPO('MlpPolicy', env,\n",
    "                    ent_coef=0.01,    # explicitly encourages exploration\n",
    "                    clip_range=0.2,\n",
    "                    **params, verbose=0)\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        val_env = PortfolioEnv(val_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        obs, _ = val_env.reset()\n",
    "        done, total_reward = False, 0\n",
    "        \n",
    "        # while not done:\n",
    "        #     action, _ = model.predict(obs, deterministic=True)\n",
    "        #     obs, reward, done, _, _ = val_env.step(action)\n",
    "        #     total_reward += reward\n",
    "        \n",
    "        while not done:\n",
    "            num_samples = 100  # Recommended starting point\n",
    "            action_samples = []\n",
    "        \n",
    "            for _ in range(num_samples):\n",
    "                sampled_action, _ = model.predict(obs, deterministic=False)  # obs directly\n",
    "                action_samples.append(sampled_action)\n",
    "        \n",
    "            action = np.mean(action_samples, axis=0)\n",
    "        \n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_params = {**params, 'risk_coefficient': risk_coeff}\n",
    "\n",
    "    return best_params\n",
    "\n",
    "def scale_data(df, feature_cols, scaler):\n",
    "    scaled_features = scaler.transform(df[feature_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=feature_cols, index=df.index)\n",
    "\n",
    "    # Re-add columns that were not scaled (e.g., Date, Actual_Return_*)\n",
    "    for col in df.columns:\n",
    "        if col not in feature_cols:\n",
    "            scaled_df[col] = df[col].values\n",
    "\n",
    "    # Keep original column order\n",
    "    scaled_df = scaled_df[df.columns]\n",
    "    return scaled_df\n",
    "\n",
    "# Main execution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "iterations = 10\n",
    "all_weights_iterations = []\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    print(f\"\\n==== Starting Iteration {iteration + 1}/{iterations} ====\")\n",
    "    model_path = f\"ppo_train_best_model_iteration_{iteration}.zip\"\n",
    "    for idx, start_idx in enumerate(start_indices):\n",
    "        # for start_idx in range(0, 252*2, 252):\n",
    "        start_time = time.time()\n",
    "    \n",
    "        # Explicit indices for training, validation, and prediction datasets\n",
    "        train_start_idx = start_idx\n",
    "        train_end_idx = train_start_idx + train_window_days\n",
    "    \n",
    "        val_start_idx = train_end_idx\n",
    "        val_end_idx = val_start_idx + validation_window_days\n",
    "    \n",
    "        pred_start_idx = val_end_idx\n",
    "        pred_end_idx = pred_start_idx + prediction_window_days\n",
    "    \n",
    "        # Corresponding dates explicitly\n",
    "        train_start_date = data_with_features.loc[train_start_idx, 'Date']\n",
    "        train_end_date = data_with_features.loc[train_end_idx - 1, 'Date']\n",
    "    \n",
    "        val_start_date = data_with_features.loc[val_start_idx, 'Date']\n",
    "        val_end_date = data_with_features.loc[val_end_idx - 1, 'Date']\n",
    "    \n",
    "        pred_start_date = data_with_features.loc[pred_start_idx, 'Date']\n",
    "        pred_end_date = data_with_features.loc[pred_end_idx - 1, 'Date']\n",
    "    \n",
    "        # Clearly print ranges for clarity\n",
    "        print(f\"Training period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "        print(f\"Validation period: {val_start_date.date()} to {val_end_date.date()}\")\n",
    "        print(f\"Prediction period: {pred_start_date.date()} to {pred_end_date.date()}\")\n",
    "    \n",
    "        # Explicitly subset data accordingly\n",
    "        train_data = data_with_features.iloc[train_start_idx:train_end_idx].reset_index(drop=True)\n",
    "        val_data = data_with_features.iloc[val_start_idx:val_end_idx].reset_index(drop=True)\n",
    "        pred_data = data_with_features.iloc[pred_start_idx:pred_end_idx].reset_index(drop=True)\n",
    "    \n",
    "        feature_cols = [col for col in train_data.columns if col != 'Date' and not col.startswith('Actual_Return')]\n",
    "    \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_data[feature_cols])\n",
    "    \n",
    "        train_data_scaled = scale_data(train_data, feature_cols, scaler)\n",
    "        val_data_scaled = scale_data(val_data, feature_cols, scaler)\n",
    "        pred_data_scaled = scale_data(pred_data, feature_cols, scaler)\n",
    "    \n",
    "        print(\"Starting hyperparameter tuning...\")\n",
    "        best_params = validate_and_tune(train_data_scaled, val_data_scaled, reward_type)\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "        incremental_timesteps = 5000\n",
    "        max_timesteps = 30000\n",
    "        patience = 3\n",
    "        \n",
    "        best_val_reward = -np.inf\n",
    "        no_improve_steps = 0\n",
    "    \n",
    "        risk_coeff = best_params.pop('risk_coefficient',0.5)\n",
    "        policy_kwargs = dict(net_arch=[256, 256])\n",
    "    \n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "        \n",
    "        # Load previous model if exists\n",
    "        if idx > 0 and os.path.exists(model_path):\n",
    "            print(f\"Loading previous model from {model_path}...\")\n",
    "            model = PPO.load(model_path, env=env)\n",
    "            model.set_env(env)\n",
    "        else:\n",
    "            print(\"Initializing new PPO model...\")\n",
    "            model = PPO('MlpPolicy', env,\n",
    "                        policy_kwargs=policy_kwargs,\n",
    "                        ent_coef=0.01,\n",
    "                        clip_range=0.2,\n",
    "                        **best_params, verbose=0)\n",
    "         # always retrain\n",
    "        # model = PPO('MlpPolicy', env,\n",
    "        #             policy_kwargs=policy_kwargs,\n",
    "        #             ent_coef=0.01,    # explicitly encourages exploration\n",
    "        #             clip_range=0.2,\n",
    "        #             **best_params, verbose=0)\n",
    "        # print(\"Starting model training...\")\n",
    "        # model.learn(total_timesteps=20000)\n",
    "        print(\"Starting model training with early stopping...\")\n",
    "        \n",
    "        # model = PPO('MlpPolicy', env,\n",
    "        #             policy_kwargs=policy_kwargs,\n",
    "        #             ent_coef=0.01,    # explicitly encourages exploration\n",
    "        #             clip_range=0.2,\n",
    "        #             **best_params, verbose=0)\n",
    "        # print(\"Starting model training...\")\n",
    "        # model.learn(total_timesteps=20000)\n",
    "    \n",
    "        for step in range(0, max_timesteps, incremental_timesteps):\n",
    "            model.learn(total_timesteps=incremental_timesteps)\n",
    "        \n",
    "            # Evaluate on validation environment\n",
    "            val_env = PortfolioEnv(val_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "            val_obs, _ = val_env.reset()\n",
    "            val_done = False\n",
    "            val_total_reward = 0.0\n",
    "        \n",
    "            while not val_done:\n",
    "                # val_action, _ = model.predict(val_obs, deterministic=True)\n",
    "                # val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "                # val_total_reward += val_reward\n",
    "                \n",
    "                num_samples = 100  # Recommended\n",
    "                value_action_samples = []\n",
    "        \n",
    "                for _ in range(num_samples):\n",
    "                    value_sampled_action, _ = model.predict(val_obs, deterministic=False)\n",
    "                    value_action_samples.append(value_sampled_action)\n",
    "            \n",
    "                val_action = np.mean(value_action_samples, axis=0)    \n",
    "                \n",
    "                val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "                val_total_reward += val_reward\n",
    "        \n",
    "            print(f\"Step: {step + incremental_timesteps}, Validation Total Reward: {val_total_reward:.4f}\")\n",
    "        \n",
    "            # Early stopping check\n",
    "            if val_total_reward > best_val_reward:\n",
    "                best_val_reward = val_total_reward\n",
    "                no_improve_steps = 0\n",
    "                # model.save(\"best_ppo_model.zip\")\n",
    "                model.save(model_path)\n",
    "                print(f\"Improved validation reward; model saved at step {step + incremental_timesteps}\")\n",
    "            else:\n",
    "                no_improve_steps += 1\n",
    "                print(f\"No improvement ({no_improve_steps}/{patience})\")\n",
    "        \n",
    "                if no_improve_steps >= patience:\n",
    "                    print(\"Early stopping explicitly triggered.\")\n",
    "                    break\n",
    "        \n",
    "        # Load the best model explicitly\n",
    "        # model = PPO.load(\"best_ppo_model.zip\")\n",
    "        model = PPO.load(model_path)\n",
    "        \n",
    "        print(\"Loaded the best PPO model explicitly for prediction.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "        # Ensure historical context explicitly available in prediction\n",
    "        full_data = pd.concat([train_data_scaled, val_data_scaled, pred_data_scaled])\n",
    "        pred_data_with_history = full_data[full_data['Date'] >= (pred_start_date - pd.Timedelta(days=lookback_period))].reset_index(drop=True)\n",
    "    \n",
    "        pred_env = PortfolioEnv(pred_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        # pred_env = PortfolioEnv(pred_data_with_history, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "    \n",
    "        obs, info = pred_env.reset()\n",
    "        done = False\n",
    "    \n",
    "        action = np.zeros(len(etf_list), dtype=np.float32)\n",
    "    \n",
    "        while not done:\n",
    "            if pred_env.current_step >= lookback_period and pred_env.current_step % pred_env.rebalance_period == 0:\n",
    "                # obs_for_agent = pred_data_with_history.drop(columns=['Date']).iloc[pred_env.current_step - lookback_period:pred_env.current_step].values.flatten().astype(np.float32)\n",
    "                # action, _ = model.predict(obs_for_agent, deterministic=True)\n",
    "    \n",
    "                # v1 normalize weight\n",
    "                # action, _ = model.predict(obs, deterministic=True)\n",
    "                \n",
    "                # num_samples = 100  # Recommended\n",
    "                # action_samples = []\n",
    "                # \n",
    "                # for _ in range(num_samples):\n",
    "                #     sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "                #     action_samples.append(sampled_action)\n",
    "                # \n",
    "                # action = np.mean(action_samples, axis=0)    \n",
    "                # \n",
    "                # temperature = 0.5\n",
    "                # scaled_action = action / temperature\n",
    "                # final_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "                # rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "                # # all_weights.append([rebalance_date] + weights.tolist())\n",
    "                # all_weights_iterations.append([iteration + 1, rebalance_date] + final_weights.tolist())\n",
    "    \n",
    "                # v2 long short normalization\n",
    "                # action, _ = model.predict(obs, deterministic=True)\n",
    "                \n",
    "                num_samples = 100  # Recommended\n",
    "                action_samples = []\n",
    "\n",
    "                for _ in range(num_samples):\n",
    "                    sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "                    action_samples.append(sampled_action)\n",
    "\n",
    "                action = np.mean(action_samples, axis=0)    \n",
    "\n",
    "                # Explicitly apply your new 120/20 normalization logic (to match environment step)\n",
    "                desired_long = 1.20  # Explicitly 120% long exposure\n",
    "                desired_short = 0.20  # Explicitly 20% short exposure\n",
    "                clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "                raw_weights = action.copy()\n",
    "\n",
    "                # Separate explicitly positive (long) and negative (short) actions\n",
    "                long_weights = np.maximum(raw_weights, 0)\n",
    "                short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "                has_longs = np.sum(long_weights) > 0\n",
    "                has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "                if has_longs and has_shorts:\n",
    "                    normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                    normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "                elif has_longs and not has_shorts:\n",
    "                    normalized_long = long_weights / np.sum(long_weights)\n",
    "                    normalized_short = np.zeros_like(short_weights)\n",
    "                elif not has_longs and has_shorts:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    normalized_long = np.ones(num_assets) / num_assets\n",
    "                    normalized_short = np.zeros(num_assets)\n",
    "                else:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    normalized_long = np.ones(num_assets) / num_assets\n",
    "                    normalized_short = np.zeros(num_assets)\n",
    "\n",
    "                combined_weights = normalized_long - normalized_short\n",
    "                clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "                # Re-separate after clipping explicitly\n",
    "                long_clipped = np.maximum(clipped_weights, 0)\n",
    "                short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "                has_long_clipped = np.sum(long_clipped) > 0\n",
    "                has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "                if has_long_clipped and has_short_clipped:\n",
    "                    final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                    final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "                elif has_long_clipped and not has_short_clipped:\n",
    "                    final_long = long_clipped / np.sum(long_clipped)\n",
    "                    final_short = np.zeros_like(short_clipped)\n",
    "                else:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    final_long = np.ones(num_assets) / num_assets\n",
    "                    final_short = np.zeros(num_assets)\n",
    "\n",
    "                final_weights = final_long - final_short\n",
    "\n",
    "                rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "                # all_weights.append([rebalance_date] + final_weights.tolist())\n",
    "                all_weights_iterations.append([iteration + 1, rebalance_date] + final_weights.tolist())\n",
    "                # \n",
    "            obs, _, done, _, _ = pred_env.step(action)\n",
    "    \n",
    "        end_time = time.time()\n",
    "        print(f\"Iteration {iteration + 1}, start index {start_idx} completed in {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "columns = ['Iteration', 'Date'] + etf_list\n",
    "weights_df = pd.DataFrame(all_weights_iterations, columns=columns)\n",
    "weights_df.to_csv('ppo_allocations_multiple_iterations_DIA_ETF.csv', index=False)\n",
    "print(\"Saved all iterations' allocations to ppo_allocations_multiple_iterations_DIA_ETF.csv\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86d1a98b60121652",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Stage 2 PPO training with recommended enhancements\n",
    "# ==================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utility functions and seeding\n",
    "# -------------------------------------------------------------------\n",
    "SEED = 42\n",
    "\n",
    "def set_global_seed(seed):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_global_seed(SEED)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Environment definition with softmax normalisation and Mean‑CVaR reward\n",
    "# -------------------------------------------------------------------\n",
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gym environment for portfolio allocation.\n",
    "    Observations are flattened windows of features; actions are unconstrained\n",
    "    real numbers that are converted to portfolio weights via softmax.\n",
    "    Reward is computed at each rebalance period as mean minus λ × CVaR.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar',\n",
    "                 risk_coefficient=1.0, rebalance_period=21,\n",
    "                 lookback_period=60):\n",
    "        super().__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "\n",
    "        # Action space: one unbounded action per asset\n",
    "        self.action_space = spaces.Box(low=-10, high=10, shape=(len(etf_list),), dtype=np.float32)\n",
    "\n",
    "        # Observation space: flatten last lookback_period days of features\n",
    "        self.feature_cols = [c for c in data.columns\n",
    "                             if c not in ['Date'] and not c.startswith('Actual_Return')]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(len(self.feature_cols)*lookback_period,),\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "        self.current_step = self.lookback_period\n",
    "        self.current_weights = np.array([1/len(etf_list)]*len(etf_list), dtype=float)\n",
    "        self.cumulative_wealth = 1.0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.current_weights = np.array([1/len(self.etf_list)]*len(self.etf_list),\n",
    "                                        dtype=float)\n",
    "        self.cumulative_wealth = 1.0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Return a flattened window of recent features.\"\"\"\n",
    "        window = self.data.iloc[\n",
    "            self.current_step - self.lookback_period : self.current_step\n",
    "        ]\n",
    "        return window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "\n",
    "    def _action_to_weights(self, action):\n",
    "        \"\"\"\n",
    "        Convert raw action outputs into a valid long‑only weight vector via softmax.\n",
    "        This implements the 'continuous 10‑dimensional weights with softmax normalisation'\n",
    "        specification from your methodology (Step 4).\n",
    "        \"\"\"\n",
    "        # temperature scaling – adjust if you want more/less concentration\n",
    "        temperature = 1.0\n",
    "        scaled = action / temperature\n",
    "        exp_vals = np.exp(scaled - np.max(scaled))\n",
    "        return exp_vals / exp_vals.sum()\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns):\n",
    "        \"\"\"Compute reward according to the chosen risk measure.\"\"\"\n",
    "        if self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100*alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            return portfolio_return - self.risk_coefficient * cvar\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            return np.log(self.cumulative_wealth)\n",
    "        elif self.reward_type == 'cumulative_return':\n",
    "            return self.cumulative_wealth - 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown reward_type {self.reward_type}\")\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Update portfolio and compute reward.\"\"\"\n",
    "        next_step = self.current_step + 1\n",
    "\n",
    "        # rebalance portfolio at rebalance dates\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            self.current_weights = self._action_to_weights(action)\n",
    "        else:\n",
    "            # drift weights using actual returns\n",
    "            daily_rets = np.array([\n",
    "                self.data.loc[self.current_step, f'Actual_Return_{t}']\n",
    "                for t in self.etf_list\n",
    "            ])\n",
    "            self.current_weights *= (1 + daily_rets)\n",
    "            self.current_weights /= self.current_weights.sum()\n",
    "\n",
    "        # compute reward on the next day\n",
    "        if next_step >= len(self.data):\n",
    "            done = True\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            asset_returns = np.array([\n",
    "                self.data.loc[next_step, f'Actual_Return_{t}']\n",
    "                for t in self.etf_list\n",
    "            ])\n",
    "            portfolio_ret = float(np.dot(self.current_weights, asset_returns))\n",
    "            self.cumulative_wealth *= (1 + portfolio_ret)\n",
    "            reward = self.calculate_reward(portfolio_ret, asset_returns)\n",
    "            done = (next_step >= len(self.data) - 1)\n",
    "\n",
    "        self.current_step += 1\n",
    "        return self._get_obs(), reward, done, False, {}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Data preparation (Step 1)\n",
    "# -------------------------------------------------------------------\n",
    "# Load your Stage 2 RL observations (predicted returns, SHAP, etc.)\n",
    "stage2_file = 'stage2_rl_observations_optimized_10ETFs.csv'\n",
    "price_file = 'stock_prices_10ETFs.csv'\n",
    "\n",
    "stage2 = pd.read_csv(stage2_file, parse_dates=['Date'])\n",
    "prices = pd.read_csv(price_file)\n",
    "prices['Date'] = pd.to_datetime(prices['Date'], utc=True).dt.tz_localize(None)\n",
    "\n",
    "# Align data on Date\n",
    "prices.rename(columns={c: f'Price_{c}' for c in prices.columns if c != 'Date'},\n",
    "              inplace=True)\n",
    "data_merged = pd.merge(stage2, prices, on='Date', how='inner')\n",
    "\n",
    "# Compute technical indicators (volatility, momentum, moving averages)\n",
    "# as outlined in the methodology (20‑day volatility, 5/10/20‑day momentum,\n",
    "# 5‑ and 20‑day moving averages and crossover).\n",
    "\n",
    "\n",
    "def add_features(df, etfs):\n",
    "    df2 = df.copy()\n",
    "    for etf in etfs:\n",
    "        price_col = f'Price_{etf}'\n",
    "        returns = df2[price_col].pct_change()\n",
    "        df2[f'Volatility_{etf}'] = returns.rolling(20).std()\n",
    "        df2[f'Momentum_5d_{etf}'] = returns.rolling(5).sum()\n",
    "        df2[f'Momentum_10d_{etf}'] = returns.rolling(10).sum()\n",
    "        df2[f'Momentum_20d_{etf}'] = returns.rolling(20).sum()\n",
    "        df2[f'MA_5d_{etf}'] = df2[price_col].rolling(5).mean()\n",
    "        df2[f'MA_20d_{etf}'] = df2[price_col].rolling(20).mean()\n",
    "        df2[f'MA_Crossover_{etf}'] = df2[f'MA_5d_{etf}'] - df2[f'MA_20d_{etf}']\n",
    "    return df2.dropna()\n",
    "\n",
    "data_with_features = add_features(data_merged, ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'])\n",
    "\n",
    "# Optionally, filter out predicted returns or SHAP metrics; here we include both\n",
    "# because they are key inputs in Step 4’s observation space.\n",
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df2 = df.copy()\n",
    "    if not include_predicted_returns:\n",
    "        cols = [c for c in df2.columns if 'Predicted_Return' in c]\n",
    "        df2.drop(columns=cols, inplace=True)\n",
    "    if not include_shap_metrics:\n",
    "        cols = [c for c in df2.columns if 'SHAP' in c]\n",
    "        df2.drop(columns=cols, inplace=True)\n",
    "    return df2\n",
    "\n",
    "data_with_features = filter_features(data_with_features, True, True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Rolling window splits (Stage 2 Initial Training, Validation, Test)\n",
    "# -------------------------------------------------------------------\n",
    "# Use 10 years for training, 2 years for validation, 3 years for test\n",
    "train_days = 252*10\n",
    "val_days   = 252*2\n",
    "test_days  = 252*3\n",
    "\n",
    "lookback  = 60         # 60‑day lookback (recommended)\n",
    "rebalance = 21         # monthly rebalance (21 trading days)\n",
    "\n",
    "# In a real implementation you would loop over many start dates; here we take the first one\n",
    "start_idx = 0\n",
    "train_data = data_with_features.iloc[start_idx:start_idx+train_days].reset_index(drop=True)\n",
    "val_data   = data_with_features.iloc[start_idx+train_days:\n",
    "                                     start_idx+train_days+val_days].reset_index(drop=True)\n",
    "test_data  = data_with_features.iloc[start_idx+train_days+val_days:\n",
    "                                     start_idx+train_days+val_days+test_days].reset_index(drop=True)\n",
    "\n",
    "# Standardise features\n",
    "feature_cols = [c for c in train_data.columns if c not in ['Date']\n",
    "                and not c.startswith('Actual_Return')]\n",
    "scaler = StandardScaler().fit(train_data[feature_cols])\n",
    "\n",
    "def scale(df):\n",
    "    x = scaler.transform(df[feature_cols])\n",
    "    df_scaled = pd.DataFrame(x, columns=feature_cols, index=df.index)\n",
    "    for col in df.columns:\n",
    "        if col not in feature_cols:\n",
    "            df_scaled[col] = df[col]\n",
    "    return df_scaled[df.columns]\n",
    "\n",
    "train_scaled = scale(train_data)\n",
    "val_scaled   = scale(val_data)\n",
    "test_scaled  = scale(test_data)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# PPO Training with improved hyper‑parameters (Step 5)\n",
    "# -------------------------------------------------------------------\n",
    "def linear_schedule(initial_value, final_value):\n",
    "    def schedule(progress_remaining):\n",
    "        return final_value + progress_remaining * (initial_value - final_value)\n",
    "    return schedule\n",
    "\n",
    "# Use a vectorised environment with 10 parallel instances for faster training\n",
    "n_envs = 10\n",
    "def make_env():\n",
    "    return PortfolioEnv(train_scaled, \n",
    "                        ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                        reward_type='mean_cvar',\n",
    "                        risk_coefficient=1.0,\n",
    "                        rebalance_period=rebalance,\n",
    "                        lookback_period=lookback)\n",
    "\n",
    "vec_env = SubprocVecEnv([make_env for _ in range(n_envs)], start_method='spawn')\n",
    "\n",
    "# PPO hyper‑parameters inspired by recent research:contentReference[oaicite:0]{index=0}\n",
    "# n_steps collects 3 months of daily data per environment: 252 * 3 ≈ 756\n",
    "n_steps = 252 * 3\n",
    "ppo_model = PPO(\n",
    "    policy='MlpPolicy',\n",
    "    env=vec_env,\n",
    "    learning_rate=linear_schedule(3e-4, 1e-5),\n",
    "    n_steps=n_steps,\n",
    "    batch_size=1260,           # 252 * 5\n",
    "    n_epochs=16,\n",
    "    gamma=0.9,                 # lower discount to focus on near‑term returns\n",
    "    gae_lambda=0.9,\n",
    "    clip_range=0.25,\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=[64, 64],\n",
    "        activation_fn=torch.nn.Tanh,\n",
    "        log_std_init=-1.0\n",
    "    ),\n",
    "    ent_coef=0.01,\n",
    "    seed=SEED,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train for 7.5 million timesteps (≈600 episodes × 10 envs × 252×5 steps)\n",
    "total_timesteps = int(7.5e6)\n",
    "ppo_model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "# Optionally save the model\n",
    "ppo_model.save('ppo_stage2_best_model.zip')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Validation and early stopping (Step 6)\n",
    "# -------------------------------------------------------------------\n",
    "# After training, evaluate on the validation set without updating parameters\n",
    "val_env = PortfolioEnv(val_scaled,\n",
    "                       ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                       reward_type='mean_cvar',\n",
    "                       risk_coefficient=1.0,\n",
    "                       rebalance_period=rebalance,\n",
    "                       lookback_period=lookback)\n",
    "obs, _ = val_env.reset(seed=SEED)\n",
    "done = False\n",
    "val_reward = 0.0\n",
    "while not done:\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, _ = val_env.step(action)\n",
    "    val_reward += reward\n",
    "print(f\"Total validation reward: {val_reward:.4f}\")\n",
    "\n",
    "# If necessary, you can adjust hyperparameters and re‑train based on validation performance.\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Out‑of‑sample testing (2022–2024) and performance metrics (Step 6)\n",
    "# -------------------------------------------------------------------\n",
    "test_env = PortfolioEnv(test_scaled,\n",
    "                        ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                        reward_type='mean_cvar',\n",
    "                        risk_coefficient=1.0,\n",
    "                        rebalance_period=rebalance,\n",
    "                        lookback_period=lookback)\n",
    "\n",
    "obs, _ = test_env.reset()\n",
    "done = False\n",
    "rebalance_dates = []\n",
    "weights_history = []\n",
    "while not done:\n",
    "    # produce an action every step; env will apply it only on rebalance dates\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    obs, _, done, _, _ = test_env.step(action)\n",
    "    # record weights at rebalance points\n",
    "    if test_env.current_step % rebalance == 0:\n",
    "        date = test_scaled.loc[test_env.current_step-1, 'Date']\n",
    "        weights_history.append([date] + test_env.current_weights.tolist())\n",
    "\n",
    "# Save monthly weights to CSV\n",
    "weights_df = pd.DataFrame(weights_history, columns=['Date'] + ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'])\n",
    "weights_df.to_csv('ppo_stage2_weights.csv', index=False)\n",
    "\n",
    "# Compute drifted daily returns and compare to equal weights\n",
    "# (similar to your existing evaluation code)\n",
    "def compute_returns(weights, price_df):\n",
    "    # Explicitly define price columns by removing \"Price_\" prefix\n",
    "    price_df.columns = [c.replace('Price_', '') for c in price_df.columns]\n",
    "\n",
    "    common = [c for c in weights.columns if c in price_df.columns]\n",
    "    if len(common) == 0:\n",
    "        raise ValueError(\"No common ETFs found between weights and prices DataFrames.\")\n",
    "\n",
    "    price_df = price_df[common]\n",
    "    daily_returns = price_df.pct_change().dropna()\n",
    "    weights = weights.set_index('Date')\n",
    "    start = weights.index.min()\n",
    "    end = weights.index.max() + timedelta(days=rebalance)\n",
    "    daily_returns = daily_returns.loc[start:end]\n",
    "\n",
    "    eq_weight = np.array([1/len(common)]*len(common))\n",
    "    drifted = pd.DataFrame(index=daily_returns.index, columns=common)\n",
    "    eq_drift = pd.DataFrame(index=daily_returns.index, columns=common)\n",
    "    cur_w = weights.iloc[0].values\n",
    "    cur_eq = eq_weight\n",
    "\n",
    "    returns_df = pd.DataFrame(index=daily_returns.index, columns=['RL', 'Equal'])\n",
    "\n",
    "    for d in daily_returns.index:\n",
    "        rets = daily_returns.loc[d]  # <-- Define this here explicitly every loop iteration\n",
    "\n",
    "        if d in weights.index:\n",
    "            cur_w = weights.loc[d].values\n",
    "            cur_eq = eq_weight\n",
    "        else:\n",
    "            cur_w = (cur_w * (1 + rets.values))\n",
    "            cur_w /= cur_w.sum()\n",
    "            cur_eq = (cur_eq * (1 + rets.values))\n",
    "            cur_eq /= cur_eq.sum()\n",
    "\n",
    "        drifted.loc[d] = cur_w\n",
    "        eq_drift.loc[d] = cur_eq\n",
    "\n",
    "        shifted_rl = drifted.shift(1).loc[d]\n",
    "        shifted_eq = eq_drift.shift(1).loc[d]\n",
    "\n",
    "        if d == daily_returns.index[0]:\n",
    "            returns_df.loc[d, 'RL'] = np.dot(cur_w, rets)\n",
    "            returns_df.loc[d, 'Equal'] = np.dot(cur_eq, rets)\n",
    "        else:\n",
    "            returns_df.loc[d, 'RL'] = np.dot(shifted_rl, rets)\n",
    "            returns_df.loc[d, 'Equal'] = np.dot(shifted_eq, rets)\n",
    "\n",
    "    return returns_df.dropna()\n",
    "\n",
    "# Compute test returns\n",
    "test_returns = compute_returns(weights_df, prices.set_index('Date'))\n",
    "cum_rl    = (1 + test_returns['RL']).prod() - 1\n",
    "cum_equal = (1 + test_returns['Equal']).prod() - 1\n",
    "print(f\"Out‑of‑sample cumulative return (RL):    {cum_rl:.4%}\")\n",
    "print(f\"Out‑of‑sample cumulative return (Equal): {cum_equal:.4%}\")\n",
    "\n",
    "# You can also compute annualised return, volatility, Sharpe ratio and max drawdown\n",
    "def performance_metrics(returns, freq=252):\n",
    "    ann_return = (1 + returns).prod()**(freq/len(returns)) - 1\n",
    "    ann_vol    = returns.std() * np.sqrt(freq)\n",
    "    sharpe     = ann_return / ann_vol if ann_vol != 0 else np.nan\n",
    "    cum_pnl    = (1+returns).cumprod()\n",
    "    max_dd     = (cum_pnl / cum_pnl.cummax() - 1).min()\n",
    "    return ann_return, ann_vol, sharpe, max_dd\n",
    "\n",
    "rl_ann, rl_vol, rl_sharpe, rl_dd = performance_metrics(test_returns['RL'])\n",
    "eq_ann, eq_vol, eq_sharpe, eq_dd = performance_metrics(test_returns['Equal'])\n",
    "print(f\"RL annualised return:    {rl_ann:.4%}, Sharpe: {rl_sharpe:.3f}, Max Drawdown: {rl_dd:.4%}\")\n",
    "print(f\"Equal annualised return: {eq_ann:.4%}, Sharpe: {eq_sharpe:.3f}, Max Drawdown: {eq_dd:.4%}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6947d247eaecd896",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Validation and early stopping (Step 6)\n",
    "# -------------------------------------------------------------------\n",
    "# After training, evaluate on the validation set without updating parameters\n",
    "val_env = PortfolioEnv(val_scaled,\n",
    "                       ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                       reward_type='mean_cvar',\n",
    "                       risk_coefficient=1.0,\n",
    "                       rebalance_period=rebalance,\n",
    "                       lookback_period=lookback)\n",
    "obs, _ = val_env.reset(seed=SEED)\n",
    "done = False\n",
    "val_reward = 0.0\n",
    "while not done:\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, _ = val_env.step(action)\n",
    "    val_reward += reward\n",
    "print(f\"Total validation reward: {val_reward:.4f}\")\n",
    "\n",
    "# If necessary, you can adjust hyperparameters and re‑train based on validation performance.\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Out‑of‑sample testing (2022–2024) and performance metrics (Step 6)\n",
    "# -------------------------------------------------------------------\n",
    "test_env = PortfolioEnv(test_scaled,\n",
    "                        ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'],\n",
    "                        reward_type='mean_cvar',\n",
    "                        risk_coefficient=1.0,\n",
    "                        rebalance_period=rebalance,\n",
    "                        lookback_period=lookback)\n",
    "\n",
    "obs, _ = test_env.reset()\n",
    "done = False\n",
    "rebalance_dates = []\n",
    "weights_history = []\n",
    "while not done:\n",
    "    # produce an action every step; env will apply it only on rebalance dates\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    obs, _, done, _, _ = test_env.step(action)\n",
    "    # record weights at rebalance points\n",
    "    if test_env.current_step % rebalance == 0:\n",
    "        date = test_scaled.loc[test_env.current_step-1, 'Date']\n",
    "        weights_history.append([date] + test_env.current_weights.tolist())\n",
    "\n",
    "# Save monthly weights to CSV\n",
    "weights_df = pd.DataFrame(weights_history, columns=['Date'] + ['XLB','XLE','XLF','XLI','XLK','XLP','XLY','XLV','XLU'])\n",
    "weights_df.to_csv('ppo_stage2_weights.csv', index=False)\n",
    "\n",
    "# Compute drifted daily returns and compare to equal weights\n",
    "# (similar to your existing evaluation code)\n",
    "def compute_returns(weights, price_df):\n",
    "    # Explicitly define price columns by removing \"Price_\" prefix\n",
    "    price_df.columns = [c.replace('Price_', '') for c in price_df.columns]\n",
    "\n",
    "    common = [c for c in weights.columns if c in price_df.columns]\n",
    "    if len(common) == 0:\n",
    "        raise ValueError(\"No common ETFs found between weights and prices DataFrames.\")\n",
    "\n",
    "    price_df = price_df[common]\n",
    "    daily_returns = price_df.pct_change().dropna()\n",
    "    weights = weights.set_index('Date')\n",
    "    start = weights.index.min()\n",
    "    end = weights.index.max() + timedelta(days=rebalance)\n",
    "    daily_returns = daily_returns.loc[start:end]\n",
    "\n",
    "    eq_weight = np.array([1/len(common)]*len(common))\n",
    "    drifted = pd.DataFrame(index=daily_returns.index, columns=common)\n",
    "    eq_drift = pd.DataFrame(index=daily_returns.index, columns=common)\n",
    "    cur_w = weights.iloc[0].values\n",
    "    cur_eq = eq_weight\n",
    "\n",
    "    returns_df = pd.DataFrame(index=daily_returns.index, columns=['RL', 'Equal'])\n",
    "\n",
    "    for d in daily_returns.index:\n",
    "        rets = daily_returns.loc[d]  # <-- Define this here explicitly every loop iteration\n",
    "\n",
    "        if d in weights.index:\n",
    "            cur_w = weights.loc[d].values\n",
    "            cur_eq = eq_weight\n",
    "        else:\n",
    "            cur_w = (cur_w * (1 + rets.values))\n",
    "            cur_w /= cur_w.sum()\n",
    "            cur_eq = (cur_eq * (1 + rets.values))\n",
    "            cur_eq /= cur_eq.sum()\n",
    "\n",
    "        drifted.loc[d] = cur_w\n",
    "        eq_drift.loc[d] = cur_eq\n",
    "\n",
    "        shifted_rl = drifted.shift(1).loc[d]\n",
    "        shifted_eq = eq_drift.shift(1).loc[d]\n",
    "\n",
    "        if d == daily_returns.index[0]:\n",
    "            returns_df.loc[d, 'RL'] = np.dot(cur_w, rets)\n",
    "            returns_df.loc[d, 'Equal'] = np.dot(cur_eq, rets)\n",
    "        else:\n",
    "            returns_df.loc[d, 'RL'] = np.dot(shifted_rl, rets)\n",
    "            returns_df.loc[d, 'Equal'] = np.dot(shifted_eq, rets)\n",
    "\n",
    "    return returns_df.dropna()\n",
    "\n",
    "# Compute test returns\n",
    "test_returns = compute_returns(weights_df, prices.set_index('Date'))\n",
    "cum_rl    = (1 + test_returns['RL']).prod() - 1\n",
    "cum_equal = (1 + test_returns['Equal']).prod() - 1\n",
    "print(f\"Out‑of‑sample cumulative return (RL):    {cum_rl:.4%}\")\n",
    "print(f\"Out‑of‑sample cumulative return (Equal): {cum_equal:.4%}\")\n",
    "\n",
    "# You can also compute annualised return, volatility, Sharpe ratio and max drawdown\n",
    "def performance_metrics(returns, freq=252):\n",
    "    ann_return = (1 + returns).prod()**(freq/len(returns)) - 1\n",
    "    ann_vol    = returns.std() * np.sqrt(freq)\n",
    "    sharpe     = ann_return / ann_vol if ann_vol != 0 else np.nan\n",
    "    cum_pnl    = (1+returns).cumprod()\n",
    "    max_dd     = (cum_pnl / cum_pnl.cummax() - 1).min()\n",
    "    return ann_return, ann_vol, sharpe, max_dd\n",
    "\n",
    "rl_ann, rl_vol, rl_sharpe, rl_dd = performance_metrics(test_returns['RL'])\n",
    "eq_ann, eq_vol, eq_sharpe, eq_dd = performance_metrics(test_returns['Equal'])\n",
    "print(f\"RL annualised return:    {rl_ann:.4%}, Sharpe: {rl_sharpe:.3f}, Max Drawdown: {rl_dd:.4%}\")\n",
    "print(f\"Equal annualised return: {eq_ann:.4%}, Sharpe: {eq_sharpe:.3f}, Max Drawdown: {eq_dd:.4%}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3462ab258866c09e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data explicitly\n",
    "port_wts = pd.read_csv('ppo_allocations_multiple_iterations_DIA_ETF.csv', parse_dates=['Date'], index_col='Date')\n",
    "daily_returns = pd.read_csv('daily_returns_10ETFs.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "common_tickers = [col for col in port_wts.columns if col in daily_returns.columns]\n",
    "daily_returns = daily_returns[common_tickers]\n",
    "\n",
    "# Explicitly filter daily returns to the date range covered by portfolio weights\n",
    "start_date, end_date = port_wts.index.min(), port_wts.index.max() + pd.Timedelta(days=21)\n",
    "daily_returns = daily_returns.loc[start_date:end_date]\n",
    "\n",
    "# Initialize drifted weights with the first available rebalance weights\n",
    "initial_weights = port_wts.loc[start_date].values\n",
    "\n",
    "equal_weight = np.array([1.0 / len(common_tickers)] * len(common_tickers))\n",
    "\n",
    "# Create drifted weights DataFrame explicitly initialized\n",
    "drifted_weights = pd.DataFrame(index=daily_returns.index, columns=common_tickers)\n",
    "equal_weights = pd.DataFrame(index=daily_returns.index, columns=common_tickers)\n",
    "\n",
    "current_weights = initial_weights\n",
    "current_equal_weights = equal_weight\n",
    "\n",
    "# Initialize returns DataFrame explicitly\n",
    "returns_df = pd.DataFrame(index=daily_returns.index, columns=['Optimal_Portfolio_Return', 'Equal_Weight_Return'])\n",
    "\n",
    "for current_date in daily_returns.index:\n",
    "    if current_date in port_wts.index:\n",
    "        # Explicit rebalance date: assign new weights\n",
    "        current_weights = port_wts.loc[current_date].values\n",
    "        current_equal_weights = equal_weight\n",
    "    else:\n",
    "        # Explicitly drift weights using previous day's return\n",
    "        prev_day_return = daily_returns.loc[current_date]\n",
    "\n",
    "        drifted_wts_numerator = current_weights * (1 + prev_day_return.values)\n",
    "        current_weights = drifted_wts_numerator / np.sum(drifted_wts_numerator)\n",
    "\n",
    "        equal_drifted_numerator = current_equal_weights * (1 + prev_day_return.values)\n",
    "        current_equal_weights = equal_drifted_numerator / np.sum(equal_drifted_numerator)\n",
    "\n",
    "    drifted_weights.loc[current_date] = current_weights\n",
    "    equal_weights.loc[current_date] = current_equal_weights\n",
    "    shifted_drifted_weights = drifted_weights.shift(1)\n",
    "    shifted_equal_weights = equal_weights.shift(1)\n",
    "    if current_date == daily_returns.index[0]:\n",
    "        # On the first day, use initial weights directly\n",
    "        returns_df.loc[current_date, 'Optimal_Portfolio_Return'] = np.dot(\n",
    "            drifted_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "        returns_df.loc[current_date, 'Equal_Weight_Return'] = np.dot(\n",
    "            equal_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "    else:\n",
    "        # Explicitly use previous day's weights\n",
    "        returns_df.loc[current_date, 'Optimal_Portfolio_Return'] = np.dot(\n",
    "            shifted_drifted_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "        returns_df.loc[current_date, 'Equal_Weight_Return'] = np.dot(\n",
    "            shifted_equal_weights.loc[current_date], daily_returns.loc[current_date])\n",
    "\n",
    "# Check explicitly\n",
    "print(\"Drifted weights (head):\\n\", drifted_weights.head())\n",
    "print(\"\\nPortfolio returns (head):\\n\", returns_df.head())\n",
    "\n",
    "# Save explicitly\n",
    "drifted_weights.to_csv('drifted_weights_corrected.csv')\n",
    "equal_weights.to_csv('equal_weights.csv')\n",
    "returns_df.to_csv('portfolio_returns_combined.csv')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9e17792fa6118e4",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
