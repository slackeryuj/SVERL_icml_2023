{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "895e7096145459d4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "adb5bfdfc9c419d8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8f0500c59fdc3167"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# stage 1 training and prediction \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import ta\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "# Load data\n",
    "factors = pd.read_csv(\"aligned_factors.csv\", index_col=0, parse_dates=True)\n",
    "returns = pd.read_csv(\"daily_returns_10ETFs.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Align dates\n",
    "dates = factors.index.intersection(returns.index)\n",
    "factors = factors.loc[dates]\n",
    "returns = returns.loc[dates]\n",
    "\n",
    "# Compute expanded generic technical indicators\n",
    "all_tech_features = []\n",
    "\n",
    "for etf in returns.columns:\n",
    "    close = (1 + returns[etf]).cumprod()\n",
    "    etf_tech_features = pd.DataFrame(index=returns.index)\n",
    "\n",
    "    etf_tech_features[f'{etf}_SMA_5'] = ta.trend.sma_indicator(close, window=5)\n",
    "    etf_tech_features[f'{etf}_SMA_20'] = ta.trend.sma_indicator(close, window=20)\n",
    "    etf_tech_features[f'{etf}_SMA_50'] = ta.trend.sma_indicator(close, window=50)\n",
    "    etf_tech_features[f'{etf}_EMA_12'] = ta.trend.ema_indicator(close, window=12)\n",
    "    etf_tech_features[f'{etf}_EMA_26'] = ta.trend.ema_indicator(close, window=26)\n",
    "    etf_tech_features[f'{etf}_EMA_50'] = ta.trend.ema_indicator(close, window=50)\n",
    "    etf_tech_features[f'{etf}_RSI_7'] = ta.momentum.rsi(close, window=7)\n",
    "    etf_tech_features[f'{etf}_RSI_14'] = ta.momentum.rsi(close, window=14)\n",
    "    etf_tech_features[f'{etf}_MACD'] = ta.trend.macd_diff(close)\n",
    "    etf_tech_features[f'{etf}_ATR'] = ta.volatility.average_true_range(high=close*1.01, low=close*0.99, close=close, window=14)\n",
    "    etf_tech_features[f'{etf}_Volatility_5'] = returns[etf].rolling(window=5).std()\n",
    "    etf_tech_features[f'{etf}_Volatility_20'] = returns[etf].rolling(window=20).std()\n",
    "    etf_tech_features[f'{etf}_Volatility_50'] = returns[etf].rolling(window=50).std()\n",
    "    etf_tech_features[f'{etf}_Momentum_3'] = returns[etf].rolling(window=3).mean()\n",
    "    etf_tech_features[f'{etf}_Momentum_10'] = returns[etf].rolling(window=10).mean()\n",
    "\n",
    "    all_tech_features.append(etf_tech_features)\n",
    "\n",
    "# Concatenate all ETF technical features at once to prevent DataFrame fragmentation\n",
    "technical_features = pd.concat(all_tech_features, axis=1)\n",
    "\n",
    "# Combine original factors with technical indicators\n",
    "features = pd.concat([factors, technical_features], axis=1).dropna()\n",
    "\n",
    "# Shift target by 1 day for next-day prediction\n",
    "target_returns = returns.shift(-1).loc[features.index].dropna()\n",
    "features = features.loc[target_returns.index]\n",
    "\n",
    "# Define rolling window parameters\n",
    "train_years = 10          # Length of training data in years\n",
    "valid_years = 2           # Length of validation data in years\n",
    "test_years = 1            # Length of testing/prediction data in years (configurable)\n",
    "retrain_frequency = 1     # Retrain model every N years (configurable)\n",
    "start_year = 2009\n",
    "end_year = 2024\n",
    "\n",
    "# Generic feature names\n",
    "all_generic_features = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA',\n",
    "                        'SMA_5', 'SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', 'EMA_50',\n",
    "                        'RSI_7', 'RSI_14', 'MACD', 'ATR', 'Volatility_5', 'Volatility_20', 'Volatility_50',\n",
    "                        'Momentum_3', 'Momentum_10']\n",
    "\n",
    "# Step 1: Determine top N generic important features using aggregated SHAP across all ETFs\n",
    "shap_importances = pd.DataFrame(0.0, index=all_generic_features, columns=['SHAP_Value'])\n",
    "\n",
    "for etf in returns.columns:\n",
    "    print(f\"Computing SHAP for ETF: {etf}\")\n",
    "    train_start = pd.Timestamp(2009 - train_years, 1, 1)\n",
    "    train_end = pd.Timestamp(2009 - valid_years - 1, 12, 31)\n",
    "    \n",
    "    etf_features = [col for col in features.columns if etf in col or col in factors.columns]\n",
    "    X_train = features.loc[train_start:train_end, etf_features]\n",
    "    y_train = target_returns[etf].loc[train_start:train_end]\n",
    "\n",
    "    model = xgb.XGBRegressor(tree_method='hist', device='cuda').fit(X_train, y_train)\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_train)\n",
    "\n",
    "    for generic in all_generic_features:\n",
    "        cols = [col for col in X_train.columns if generic in col]\n",
    "        if cols:\n",
    "            idx = [X_train.columns.get_loc(c) for c in cols]\n",
    "            shap_importances.loc[generic] += np.mean(np.abs(shap_values.values[:, idx]))\n",
    "\n",
    "shap_importances /= len(returns.columns)\n",
    "common_generic_features = shap_importances.sort_values('SHAP_Value', ascending=False).head(10).index.tolist()\n",
    "\n",
    "# Step 2: Retrain models using selected top generic important features\n",
    "all_predictions = []\n",
    "for etf in returns.columns:  # Adjust this slice for all ETFs\n",
    "    \n",
    "    print(f\"Processing ETF: {etf}\")\n",
    "    selected_features = [f for f in features.columns if any(generic in f for generic in common_generic_features) or f in factors.columns]\n",
    "\n",
    "    # for year in range(2009, 2010):  # Adjust range for all years\n",
    "    year = start_year\n",
    "    while year <= end_year - test_years + 1:\n",
    "        print(f\"\\nTraining window starting year: {year}\")\n",
    "        start_time = time.time()\n",
    "        train_start = pd.Timestamp(year - train_years, 1, 1)\n",
    "        train_end = pd.Timestamp(year - valid_years - 1, 12, 31)\n",
    "        valid_start = pd.Timestamp(year - valid_years, 1, 1)\n",
    "        valid_end = pd.Timestamp(year - 1, 12, 31)\n",
    "        test_start = pd.Timestamp(year, 1, 1)\n",
    "        test_end = pd.Timestamp(year + test_years - 1, 12, 31)\n",
    "\n",
    "        X_train = features.loc[train_start:train_end, selected_features]\n",
    "        y_train = target_returns[etf].loc[train_start:train_end]\n",
    "\n",
    "        X_valid = features.loc[valid_start:valid_end, selected_features]\n",
    "        y_valid = target_returns[etf].loc[valid_start:valid_end]\n",
    "\n",
    "        X_test = features.loc[test_start:test_end, selected_features]\n",
    "        y_test = target_returns[etf].loc[test_start:test_end]\n",
    "\n",
    "        model = xgb.XGBRegressor(\n",
    "            tree_method='hist',\n",
    "            device='cuda',\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42,\n",
    "            n_jobs=4\n",
    "        )\n",
    "\n",
    "        params = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'learning_rate': [0.01, 0.03, 0.05],\n",
    "            'subsample': [0.7, 0.8],\n",
    "            'colsample_bytree': [0.7, 0.8]\n",
    "        }\n",
    "\n",
    "        grid_search = GridSearchCV(model, params, cv=TimeSeriesSplit(3), scoring='neg_mean_squared_error', verbose=1, n_jobs=4)\n",
    "        grid_search.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "       \n",
    "        # Convert test data explicitly to DMatrix for GPU\n",
    "        dtest = xgb.DMatrix(X_test, enable_categorical=False)\n",
    "        \n",
    "        # Make predictions using the best_model explicitly\n",
    "        preds = best_model.get_booster().predict(dtest)\n",
    "\n",
    "\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "        directional_accuracy = np.mean((np.sign(y_test) == np.sign(preds)).astype(int))\n",
    "\n",
    "        print(f\"MSE: {mse:.6f}, RMSE: {rmse:.6f}, MAE: {mae:.6f}, R2: {r2:.6f}, Directional Accuracy: {directional_accuracy:.2%}\")\n",
    "        joblib.dump(best_model, f\"best_model_{etf}_{year}.joblib\")\n",
    "\n",
    "        # Save daily predictions\n",
    "        predictions_df = pd.DataFrame({'Date': X_test.index, 'ETF': etf, 'Year': year, \n",
    "                                       'Actual_Return': y_test, 'Predicted_Return': preds})\n",
    "\n",
    "        # SHAP values explicitly aligned\n",
    "        explainer_test = shap.Explainer(best_model)\n",
    "        shap_values_test = explainer_test(X_test)\n",
    "        \n",
    "        # Create SHAP DataFrame explicitly with 'Date' to ensure correct merging\n",
    "        shap_df = pd.DataFrame(\n",
    "            shap_values_test.values,\n",
    "            columns=[f'SHAP_{col}' for col in X_test.columns],\n",
    "            index=X_test.index\n",
    "        ).reset_index().rename(columns={'index': 'Date'})\n",
    "        \n",
    "        # Ensure predictions_df has 'Date' column explicitly for merging\n",
    "        predictions_df = predictions_df.reset_index(drop=True)\n",
    "        \n",
    "        # Merge explicitly by 'Date' to align SHAP values correctly\n",
    "        predictions_df = pd.merge(predictions_df, shap_df, on='Date', how='left')\n",
    "        \n",
    "        # Append explicitly for each ETF-year combination\n",
    "        all_predictions.append(predictions_df)\n",
    "        year += retrain_frequency\n",
    "        end_time = time.time()\n",
    "        print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "final_predictions_df = pd.concat(all_predictions).reset_index(drop=True)\n",
    "final_predictions_df.to_csv(\"stage1_predictions_with_shap.csv\", index=False)\n",
    "\n",
    "print(\"Stage 1 completed and data saved for Stage 2.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "110622934baf18b4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d2eb4c9f8c79bcf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "125b6d6efc751134",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "85336205218912a7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load stage 1 data explicitly\n",
    "stage1_df = pd.read_csv(\"stage1_predictions_with_shap_DIA_ETF.csv\", parse_dates=['Date'])\n",
    "# stage1_df = pd.read_csv(\"stage1_predictions_with_shap.csv\", parse_dates=['Date'])\n",
    "etfs = stage1_df['ETF'].unique()\n",
    "\n",
    "# Initialize DataFrame for daily aggregated data explicitly\n",
    "dates = sorted(stage1_df['Date'].unique())\n",
    "aggregated_data = pd.DataFrame({'Date': dates})\n",
    "\n",
    "# ETF-specific predicted returns, actual returns, and volatility\n",
    "for etf in etfs:\n",
    "    etf_data = stage1_df[stage1_df['ETF'] == etf].set_index('Date').sort_index()\n",
    "\n",
    "    aggregated_data[f'Predicted_Return_{etf}'] = aggregated_data['Date'].map(etf_data['Predicted_Return'])\n",
    "    aggregated_data[f'Actual_Return_{etf}'] = aggregated_data['Date'].map(etf_data['Actual_Return'])\n",
    "\n",
    "    # Explicit rolling 5-day volatility calculation\n",
    "    volatility = etf_data['Actual_Return'].rolling(window=5).std()\n",
    "    aggregated_data[f'Volatility_{etf}'] = aggregated_data['Date'].map(volatility)\n",
    "\n",
    "# Define explicitly generic SHAP features to aggregate across ETFs\n",
    "# generic_shap_features = [\n",
    "#     'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', \n",
    "#     'SMA_5', 'SMA_20', 'SMA_50',\n",
    "#     'EMA_12', 'EMA_26', 'EMA_50',\n",
    "#     'RSI_7', 'RSI_14', 'MACD', 'ATR',\n",
    "#     'Volatility_5', 'Volatility_20', 'Volatility_50',\n",
    "#     'Momentum_3', 'Momentum_10'\n",
    "# ]\n",
    "generic_shap_features = ['Mkt-RF',\n",
    " 'Volatility_50',\n",
    " 'HML',\n",
    " 'Momentum_3',\n",
    " 'Volatility_5',\n",
    " 'Volatility_20',\n",
    " 'RMW',\n",
    " 'SMB',\n",
    " 'CMA',\n",
    " 'RSI_7']\n",
    "\n",
    "# Aggregate SHAP values explicitly across ETFs by generic feature\n",
    "shap_aggregated = {'Date': dates}\n",
    "shap_df_list = []\n",
    "\n",
    "for feature in generic_shap_features:\n",
    "    # Adjust matching explicitly for 'SHAP_{ETF}_{feature}' format\n",
    "    matching_shap_cols = [col for col in stage1_df.columns if col.startswith('SHAP_') and col.endswith(f'_{feature}')]\n",
    "    \n",
    "    if matching_shap_cols:\n",
    "        # Compute daily mean explicitly across selected SHAP columns\n",
    "        daily_shap_mean = stage1_df.groupby('Date')[matching_shap_cols].mean().mean(axis=1)\n",
    "        shap_df_list.append(daily_shap_mean.rename(f'Avg_SHAP_{feature}'))\n",
    "    else:\n",
    "        print(f\"Warning: No matches found explicitly for feature: {feature}\")\n",
    "\n",
    "# Concatenate aggregated SHAP features explicitly, ensuring alignment\n",
    "shap_aggregated_df = pd.concat(shap_df_list, axis=1).reset_index()\n",
    "\n",
    "# Explicit merge with ETF-specific metrics on Date to ensure alignment\n",
    "aggregated_data = pd.merge(aggregated_data, shap_aggregated_df, on='Date', how='left')\n",
    "\n",
    "# Explicit handling of missing values\n",
    "aggregated_data.sort_values('Date', inplace=True)\n",
    "aggregated_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Remove rows explicitly if initial volatility calculations have NaNs\n",
    "vol_cols = [f'Volatility_{etf}' for etf in etfs]\n",
    "aggregated_data.dropna(subset=vol_cols, inplace=True)\n",
    "\n",
    "# Check if aggregated_data is empty before saving explicitly\n",
    "if aggregated_data.empty:\n",
    "    print(\"Warning: aggregated_data is empty after processing. Please verify input data and alignment explicitly.\")\n",
    "else:\n",
    "    aggregated_data.to_csv(\"stage2_rl_observations_optimized_DIA_ETF.csv\", index=False)\n",
    "    print(f\"Optimized Stage 2 RL dataset created with shape: {aggregated_data.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "626373434f64d0c4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d8f3f0813e659ead",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# start of stage 2 training\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar', risk_coefficient=0.5, rebalance_period=21, lookback_period=21):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(etf_list),), dtype=np.float32)\n",
    "\n",
    "        # Explicitly select feature columns (excluding Date and returns used only for calculating reward)\n",
    "        self.feature_cols = [col for col in data.columns if col not in ['Date'] and not col.startswith('Actual_Return')]\n",
    "        self.num_features_per_day = len(self.feature_cols)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.num_features_per_day * self.lookback_period,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(etf_list)] * len(etf_list))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(self.etf_list)] * len(self.etf_list))\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        next_step = self.current_step + 1\n",
    "\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            # v2 long short\n",
    "            desired_long = 1.20  # 120% long exposure explicitly\n",
    "            desired_short = 0.20  # 20% short exposure explicitly\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                # Normal 120/20 explicitly0\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                # Only long explicitly: default realistically to 100% long\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                # Only short explicitly (unrealistic), fallback clearly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                # All zeros explicitly: fallback explicitly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            # Apply explicit clipping\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate explicitly after clipping\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            # Final explicit normalization after clipping\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)  # exactly 100% long\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                # Realistic fallback explicitly: equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "            self.current_weights = final_weights\n",
    "            \n",
    "            # v1 softmax normalization\n",
    "            \n",
    "            # temperature = 0.5  # Explicitly lower for higher concentration (try 0.2 to 0.8)\n",
    "            # scaled_action = action / temperature\n",
    "            # self.current_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "\n",
    "        else:\n",
    "            returns_today = np.array([self.data.loc[self.current_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            self.current_weights *= (1 + returns_today)\n",
    "            self.current_weights /= np.sum(self.current_weights)\n",
    "\n",
    "        if next_step >= len(self.data):\n",
    "            terminated = True\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            returns = np.array([self.data.loc[next_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            portfolio_return = np.dot(self.current_weights, returns)\n",
    "            self.cumulative_wealth *= (1 + portfolio_return)\n",
    "            reward = self.calculate_reward(portfolio_return, returns)\n",
    "            terminated = next_step >= len(self.data) - 1\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "        # def _get_obs(self):\n",
    "        #     obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        #     obs_window = obs_window.drop(columns=['Date']).values.flatten().astype(np.float32)\n",
    "        #     return obs_window\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        obs_window = obs_window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "        return obs_window\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns):\n",
    "        if self.reward_type == 'cumulative_return':\n",
    "            return self.cumulative_wealth - 1.0\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            return np.log(self.cumulative_wealth)\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100 * alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            return portfolio_return - self.risk_coefficient * cvar\n",
    "        else:\n",
    "            raise ValueError('Invalid reward type')\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_stable_features(df, etf_list):\n",
    "    data = df.copy()\n",
    "\n",
    "    for etf in etf_list:\n",
    "        price_col = f'Price_{etf}'\n",
    "\n",
    "        # Volatility (20-day)\n",
    "        data[f'Volatility_{etf}'] = data[price_col].pct_change().rolling(20).std()\n",
    "\n",
    "        # Momentum indicators (returns over 5, 10, 20 days)\n",
    "        data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
    "        data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
    "        data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
    "\n",
    "        # Moving averages (5-day and 20-day)\n",
    "        data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
    "        data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
    "\n",
    "        # Moving average crossover (5-day MA - 20-day MA)\n",
    "        data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
    "\n",
    "    # Drop NaN values due to rolling calculations\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # Explicit patterns to identify columns\n",
    "    predicted_return_pattern = 'Predicted_Return'\n",
    "    shap_metric_pattern = 'SHAP'\n",
    "\n",
    "    # Exclude Predicted Returns explicitly if requested\n",
    "    if not include_predicted_returns:\n",
    "        predicted_cols = [col for col in df_filtered.columns if predicted_return_pattern in col]\n",
    "        df_filtered.drop(columns=predicted_cols, inplace=True)\n",
    "        print(f\"Excluded predicted return columns: {predicted_cols}\")\n",
    "\n",
    "    # Exclude SHAP-related metrics explicitly if requested\n",
    "    if not include_shap_metrics:\n",
    "        shap_cols = [col for col in df_filtered.columns if shap_metric_pattern in col]\n",
    "        df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "        print(f\"Excluded SHAP-related columns: {shap_cols}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# ETFs\n",
    "# etf_list = ['XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLY', 'XLV', 'XLU']\n",
    "\n",
    "etf_list = ['BA',\n",
    "'AMGN',\n",
    "'DIS',\n",
    "'NKE',\n",
    "'HON',\n",
    "'MMM',\n",
    "'CAT',\n",
    "'KO',\n",
    "'PG',\n",
    "'AXP',\n",
    "'JPM',\n",
    "'MCD',\n",
    "'HD',\n",
    "'AAPL',\n",
    "'CSCO',\n",
    "'IBM',\n",
    "'MSFT',\n",
    "'TRV',\n",
    "'UNH',\n",
    "'CVX',\n",
    "'JNJ',\n",
    "'MRK',\n",
    "'AMZN',\n",
    "'WMT',\n",
    "'INTC',\n",
    "'VZ']\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 5e-5],\n",
    "    'n_steps': [20, 40],\n",
    "    'batch_size': [5, 10],\n",
    "    'gamma': [0.98, 0.99]\n",
    "}\n",
    "consolidated_file = 'stage2_rl_observations_optimized_DIA_ETF.csv'\n",
    "reward_type = 'mean_cvar'\n",
    "# data = pd.read_csv(consolidated_file, parse_dates=['Date'])\n",
    "# data = data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "data = pd.read_csv('stage2_rl_observations_optimized_DIA_ETF.csv', parse_dates=['Date'])\n",
    "price_data = pd.read_csv('stock_prices_DIA_ETF.csv')\n",
    "# price_data = pd.read_csv('stock_prices_10ETFs.csv')\n",
    "# Convert the Date column in price data, handling the timezone correctly\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'], utc=True)\n",
    "price_data['Date'] = price_data['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Rename price columns explicitly to 'price_{ticker}'\n",
    "price_cols = {col: f'Price_{col}' for col in price_data.columns if col != 'Date'}\n",
    "price_data.rename(columns=price_cols, inplace=True)\n",
    "\n",
    "# Merge datasets on Date\n",
    "merged_data = pd.merge(data, price_data, on='Date', how='inner')\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "# Check if merge was successful\n",
    "if len(merged_data) != len(data):\n",
    "    print(f\"Warning: Data length mismatch after merging (Original: {len(data)}, Merged: {len(merged_data)}).\")\n",
    "else:\n",
    "    print(\"Merged successfully with aligned dates.\")\n",
    "\n",
    "data_with_features_raw = add_stable_features(merged_data, etf_list)\n",
    "data_with_features_raw.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Usage Example clearly for benchmark (only price metrics, no predicted return or SHAP):\n",
    "data_with_features = filter_features(data_with_features_raw, \n",
    "                                 include_predicted_returns=True, \n",
    "                                 include_shap_metrics=True)\n",
    "################### override data to use SHAP only\n",
    "# data_with_features = data\n",
    "################### END override \n",
    "\n",
    "# Define your rolling window lengths clearly:\n",
    "train_window_days = 252 * 7\n",
    "validation_window_days = 252\n",
    "prediction_window_days = 252\n",
    "lookback_period = 21\n",
    "rebalance_period = 21\n",
    "\n",
    "start_indices = range(0, len(data) - (train_window_days + validation_window_days + prediction_window_days), prediction_window_days)\n",
    "all_weights = []\n",
    "model_path = 'ppo_single_train_best_model_DIA_ETF.zip'\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "def validate_and_tune(train_data, val_data, reward_type, rebalance_period=10, lookback_period=10, n_iter=8, timesteps=5000):\n",
    "    best_reward, best_params = -np.inf, None\n",
    "\n",
    "    # Narrow and meaningful parameter distribution\n",
    "    param_dist = {\n",
    "        'learning_rate': [3e-4, 1e-4],\n",
    "        'n_steps': [20, 40],\n",
    "        'batch_size': [10, 20],\n",
    "        'gamma': [0.95, 0.98],\n",
    "        'risk_coefficient': [0.1, 0.5, 1.0] if reward_type in ['mean_var', 'mean_cvar'] else [0.5],\n",
    "    }\n",
    "\n",
    "    sampled_params = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=42))\n",
    "\n",
    "    for params in sampled_params:\n",
    "        risk_coeff = params.pop('risk_coefficient', 0.5)\n",
    "\n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "        model = PPO('MlpPolicy', env,\n",
    "                    ent_coef=0.01,    # explicitly encourages exploration\n",
    "                    clip_range=0.2,\n",
    "                    **params, verbose=0)\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        val_env = PortfolioEnv(val_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        obs, _ = val_env.reset()\n",
    "        done, total_reward = False, 0\n",
    "\n",
    "        while not done:\n",
    "            num_samples = 50  # Recommended starting point\n",
    "            action_samples = []\n",
    "        \n",
    "            for _ in range(num_samples):\n",
    "                sampled_action, _ = model.predict(obs, deterministic=False)  # obs directly\n",
    "                action_samples.append(sampled_action)\n",
    "        \n",
    "            action = np.mean(action_samples, axis=0)\n",
    "        \n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_params = {**params, 'risk_coefficient': risk_coeff}\n",
    "\n",
    "    return best_params\n",
    "\n",
    "def scale_data(df, feature_cols, scaler):\n",
    "    scaled_features = scaler.transform(df[feature_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=feature_cols, index=df.index)\n",
    "\n",
    "    # Re-add columns that were not scaled (e.g., Date, Actual_Return_*)\n",
    "    for col in df.columns:\n",
    "        if col not in feature_cols:\n",
    "            scaled_df[col] = df[col].values\n",
    "\n",
    "    # Keep original column order\n",
    "    scaled_df = scaled_df[df.columns]\n",
    "    return scaled_df\n",
    "\n",
    "# Main execution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for idx, start_idx in enumerate(start_indices):\n",
    "    # for start_idx in range(0, 252*2, 252):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Explicit indices for training, validation, and prediction datasets\n",
    "    train_start_idx = start_idx\n",
    "    train_end_idx = train_start_idx + train_window_days\n",
    "\n",
    "    val_start_idx = train_end_idx\n",
    "    val_end_idx = val_start_idx + validation_window_days\n",
    "\n",
    "    pred_start_idx = val_end_idx\n",
    "    pred_end_idx = pred_start_idx + prediction_window_days\n",
    "\n",
    "    # Corresponding dates explicitly\n",
    "    train_start_date = data_with_features.loc[train_start_idx, 'Date']\n",
    "    train_end_date = data_with_features.loc[train_end_idx - 1, 'Date']\n",
    "\n",
    "    val_start_date = data_with_features.loc[val_start_idx, 'Date']\n",
    "    val_end_date = data_with_features.loc[val_end_idx - 1, 'Date']\n",
    "\n",
    "    pred_start_date = data_with_features.loc[pred_start_idx, 'Date']\n",
    "    pred_end_date = data_with_features.loc[pred_end_idx - 1, 'Date']\n",
    "\n",
    "    # Clearly print ranges for clarity\n",
    "    print(f\"Training period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "    print(f\"Validation period: {val_start_date.date()} to {val_end_date.date()}\")\n",
    "    print(f\"Prediction period: {pred_start_date.date()} to {pred_end_date.date()}\")\n",
    "\n",
    "    # Explicitly subset data accordingly\n",
    "    train_data = data_with_features.iloc[train_start_idx:train_end_idx].reset_index(drop=True)\n",
    "    val_data = data_with_features.iloc[val_start_idx:val_end_idx].reset_index(drop=True)\n",
    "    pred_data = data_with_features.iloc[pred_start_idx:pred_end_idx].reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [col for col in train_data.columns if col != 'Date' and not col.startswith('Actual_Return')]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data[feature_cols])\n",
    "\n",
    "    train_data_scaled = scale_data(train_data, feature_cols, scaler)\n",
    "    val_data_scaled = scale_data(val_data, feature_cols, scaler)\n",
    "    pred_data_scaled = scale_data(pred_data, feature_cols, scaler)\n",
    "\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    best_params = validate_and_tune(train_data_scaled, val_data_scaled, reward_type)\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "    incremental_timesteps = 5000\n",
    "    max_timesteps = 30000\n",
    "    patience = 3\n",
    "    \n",
    "    best_val_reward = -np.inf\n",
    "    no_improve_steps = 0\n",
    "\n",
    "    risk_coeff = best_params.pop('risk_coefficient',0.5)\n",
    "    policy_kwargs = dict(net_arch=[256, 256])\n",
    "\n",
    "    env = make_vec_env(lambda: PortfolioEnv(train_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "    \n",
    "    # Load previous model if exists\n",
    "    if idx > 0 and os.path.exists(model_path):\n",
    "        print(f\"Loading previous model from {model_path}...\")\n",
    "        model = PPO.load(model_path, env=env)\n",
    "        model.set_env(env)\n",
    "    else:\n",
    "        print(\"Initializing new PPO model...\")\n",
    "        model = PPO('MlpPolicy', env,\n",
    "                    policy_kwargs=policy_kwargs,\n",
    "                    ent_coef=0.01,\n",
    "                    clip_range=0.2,\n",
    "                    **best_params, verbose=0)\n",
    "     # always retrain\n",
    "    # model = PPO('MlpPolicy', env,\n",
    "    #             policy_kwargs=policy_kwargs,\n",
    "    #             ent_coef=0.01,    # explicitly encourages exploration\n",
    "    #             clip_range=0.2,\n",
    "    #             **best_params, verbose=0)\n",
    "    # print(\"Starting model training...\")\n",
    "    # model.learn(total_timesteps=20000)\n",
    "    print(\"Starting model training with early stopping...\")\n",
    "\n",
    "    for step in range(0, max_timesteps, incremental_timesteps):\n",
    "        model.learn(total_timesteps=incremental_timesteps)\n",
    "    \n",
    "        # Evaluate on validation environment\n",
    "        val_env = PortfolioEnv(val_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        val_obs, _ = val_env.reset()\n",
    "        val_done = False\n",
    "        val_total_reward = 0.0\n",
    "    \n",
    "        while not val_done:\n",
    "            # val_action, _ = model.predict(val_obs, deterministic=True)\n",
    "            num_samples = 50  # Recommended\n",
    "            value_action_samples = []\n",
    "    \n",
    "            for _ in range(num_samples):\n",
    "                value_sampled_action, _ = model.predict(val_obs, deterministic=False)\n",
    "                value_action_samples.append(value_sampled_action)\n",
    "        \n",
    "            val_action = np.mean(value_action_samples, axis=0)    \n",
    "            \n",
    "            val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "            val_total_reward += val_reward\n",
    "    \n",
    "        print(f\"Step: {step + incremental_timesteps}, Validation Total Reward: {val_total_reward:.4f}\")\n",
    "    \n",
    "        # Early stopping check\n",
    "        if val_total_reward > best_val_reward:\n",
    "            best_val_reward = val_total_reward\n",
    "            no_improve_steps = 0\n",
    "            # model.save(\"best_ppo_model.zip\")\n",
    "            model.save(model_path)\n",
    "            print(f\"Improved validation reward; model saved at step {step + incremental_timesteps}\")\n",
    "        else:\n",
    "            no_improve_steps += 1\n",
    "            print(f\"No improvement ({no_improve_steps}/{patience})\")\n",
    "    \n",
    "            if no_improve_steps >= patience:\n",
    "                print(\"Early stopping explicitly triggered.\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model explicitly\n",
    "    model = PPO.load(model_path)\n",
    "    print(\"Loaded the best PPO model explicitly for prediction.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Ensure historical context explicitly available in prediction\n",
    "    full_data = pd.concat([train_data_scaled, val_data_scaled, pred_data_scaled])\n",
    "    pred_data_with_history = full_data[full_data['Date'] >= (pred_start_date - pd.Timedelta(days=lookback_period))].reset_index(drop=True)\n",
    "\n",
    "    pred_env = PortfolioEnv(pred_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "    # pred_env = PortfolioEnv(pred_data_with_history, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "\n",
    "    obs, info = pred_env.reset()\n",
    "    done = False\n",
    "\n",
    "    action = np.zeros(len(etf_list), dtype=np.float32)\n",
    "\n",
    "    while not done:\n",
    "        if pred_env.current_step >= lookback_period and pred_env.current_step % pred_env.rebalance_period == 0:\n",
    "            # obs_for_agent = pred_data_with_history.drop(columns=['Date']).iloc[pred_env.current_step - lookback_period:pred_env.current_step].values.flatten().astype(np.float32)\n",
    "            # action, _ = model.predict(obs_for_agent, deterministic=True)\n",
    "\n",
    "            # v1 normalize weight\n",
    "            # action, _ = model.predict(obs, deterministic=True)\n",
    "            # use determinstic = FALSE       \n",
    "            # num_samples = 50  # Recommended\n",
    "            # action_samples = []\n",
    "            # for _ in range(num_samples):\n",
    "            #     sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "            #     action_samples.append(sampled_action)\n",
    "            # action = np.mean(action_samples, axis=0)    \n",
    "            # \n",
    "            # temperature = 0.5\n",
    "            # scaled_action = action / temperature\n",
    "            # weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "            # rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "            # all_weights.append([rebalance_date] + weights.tolist())\n",
    "\n",
    "\n",
    "            # v2 long short normalization\n",
    "            # action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # uncomment this for predictopm\n",
    "            num_samples = 50  # Recommended\n",
    "            action_samples = []\n",
    "\n",
    "            for _ in range(num_samples):\n",
    "                sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "                action_samples.append(sampled_action)\n",
    "\n",
    "            action = np.mean(action_samples, axis=0)    \n",
    "\n",
    "            # Explicitly apply your new 120/20 normalization logic (to match environment step)\n",
    "            desired_long = 1.20  # Explicitly 120% long exposure\n",
    "            desired_short = 0.20  # Explicitly 20% short exposure\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate after clipping explicitly\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "\n",
    "            rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "            all_weights.append([rebalance_date] + final_weights.tolist())\n",
    "\n",
    "        obs, _, done, _, _ = pred_env.step(action)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "columns = ['Date'] + etf_list\n",
    "weights_df = pd.DataFrame(all_weights, columns=columns)\n",
    "weights_df.to_csv('ppo_multi_year_weights_DIA_ETF.csv', index=False)\n",
    "print(\"Saved predictions to ppo_multi_year_weights_DIA_ETF.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c770edf2acf6b5a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns = ['Date'] + etf_list\n",
    "weights_df = pd.DataFrame(all_weights, columns=columns)\n",
    "weights_df.to_csv('ppo_multi_year_weights.csv', index=False)\n",
    "print(\"Saved predictions to ppo_multi_year_weights.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6c99175692295b5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "############################## This is start to run 25 iterations ##############################\n",
    "########################################################################################################################"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8acf6abff959b252",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged successfully with aligned dates.\n",
      "\n",
      "==== Starting Iteration 1/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
      "C:\\Users\\Jiayang\\AppData\\Local\\Temp\\ipykernel_19632\\2682022344.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.6942\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.6989\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.8117\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7394\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.7464\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.7793\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 0 completed in 566.4777 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_0.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.8299\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.8078\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.8179\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 3.7976\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 252 completed in 438.0087 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_0.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7458\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7365\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7190\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.7251\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 504 completed in 437.6472 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_0.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4166\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.4469\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.4544\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.4732\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.4626\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 4.5116\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 756 completed in 540.6169 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_0.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.8423\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.7843\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.7240\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.7743\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 1008 completed in 441.5851 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_0.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.9610\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8796\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.9502\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.9575\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 1260 completed in 435.9724 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_0.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.0353\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0602\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.0560\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.0732\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 6.1389\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 6.1624\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 1, start index 1512 completed in 542.5350 seconds\n",
      "\n",
      "==== Starting Iteration 2/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.6576\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7254\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7240\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 3.6847\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 3.7006\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 0 completed in 489.8701 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_1.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7878\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7792\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.7602\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 3.7901\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 3.7718\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 3.7978\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 252 completed in 541.4856 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_1.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7658\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7651\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7796\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.7823\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.7623\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 4.7584\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 504 completed in 541.6224 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_1.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.5439\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.5266\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.5666\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.5437\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 4.5460\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 4.5397\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 756 completed in 541.9715 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_1.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.7527\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.7927\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.8025\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.7852\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 6.7500\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 6.8338\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 1008 completed in 542.3848 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_1.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.8909\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8739\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.8813\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.9083\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.9158\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.9218\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 1260 completed in 542.5190 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_1.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.2219\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.1362\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.1619\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.1931\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 2, start index 1512 completed in 437.8489 seconds\n",
      "\n",
      "==== Starting Iteration 3/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7489\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7484\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.7367\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 3.7309\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 0 completed in 437.6201 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_2.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7413\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7895\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7963\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.8197\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 3.7665\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 3.7704\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 252 completed in 543.2738 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_2.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7983\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7914\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7739\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.7663\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 504 completed in 436.3414 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_2.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4806\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.5094\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.4905\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.4741\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.4804\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 756 completed in 488.6114 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_2.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.7099\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6624\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.6914\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.6560\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 1008 completed in 436.3390 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_2.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.8736\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8935\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.8846\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.8694\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.8569\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 1260 completed in 488.8012 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_2.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.0276\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0917\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.0638\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.0372\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.0206\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 3, start index 1512 completed in 489.1009 seconds\n",
      "\n",
      "==== Starting Iteration 4/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7790\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7645\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.8049\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7920\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.7883\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.7710\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 0 completed in 539.9713 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_3.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.8216\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.8587\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.8177\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 3.8325\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 3.8086\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 252 completed in 487.0319 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_3.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7631\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7140\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7239\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.6932\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 504 completed in 434.0146 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_3.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4818\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.5174\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.4880\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.4992\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.5755\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.5705\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 756 completed in 540.4133 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_3.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.5777\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6551\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.6465\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.6483\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.7033\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 6.7585\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 1008 completed in 540.1081 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_3.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.9305\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8904\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.9451\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.9695\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.9231\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 4.9562\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 1260 completed in 540.5720 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_3.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.1800\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.1850\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.0997\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.2262\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 6.1599\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 6.2116\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 4, start index 1512 completed in 540.4556 seconds\n",
      "\n",
      "==== Starting Iteration 5/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7150\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.6979\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.7485\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7722\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 3.7821\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 3.7625\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 0 completed in 539.7731 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_4.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.9126\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.9085\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.8956\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 3.8110\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 252 completed in 435.2005 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_4.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7044\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.6864\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.6785\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.6938\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 504 completed in 434.5272 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_4.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.3774\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.3489\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.3834\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.3947\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.4579\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.4269\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 756 completed in 540.7731 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_4.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.6939\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.7080\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.6906\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.6757\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.6782\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 1008 completed in 486.1575 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_4.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7445\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7492\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.7544\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.7643\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.7661\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.8076\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 1260 completed in 540.5888 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_4.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.0792\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0731\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.0594\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.0695\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 5, start index 1512 completed in 435.3613 seconds\n",
      "\n",
      "==== Starting Iteration 6/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7341\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7409\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.6954\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 3.7300\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 3.7430\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 3.7525\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 0 completed in 543.2212 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_5.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7279\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7440\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7603\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7492\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.7053\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.7645\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 252 completed in 542.3569 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_5.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7958\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8167\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.8326\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.8416\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.8394\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 4.8408\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 504 completed in 558.7369 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_5.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4132\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.4335\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.4321\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.4469\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.4296\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 4.4806\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 756 completed in 541.7150 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_5.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.4932\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.5963\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.6037\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.5521\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 6.5276\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 6.5135\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 1008 completed in 541.5847 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_5.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.8738\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.9143\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.8644\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.9043\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.9131\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 1260 completed in 488.5243 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_5.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.1592\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.1183\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.1330\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.1451\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 6, start index 1512 completed in 436.1418 seconds\n",
      "\n",
      "==== Starting Iteration 7/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7973\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.8177\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.8579\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.8287\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.8405\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.8146\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 0 completed in 544.4448 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_6.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.9280\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.9132\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.8037\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 3.7653\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 252 completed in 434.8960 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_6.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7918\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8026\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.8801\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.8246\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 4.8467\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 4.8471\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 504 completed in 538.5441 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_6.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4638\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.4663\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.4222\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.4373\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.4392\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 756 completed in 486.5174 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_6.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.5658\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.5951\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.5635\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.5297\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.6021\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 6.5817\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 1008 completed in 539.8546 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_6.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.9835\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.9825\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 5.0133\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 5.0157\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 5.0172\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 5.0366\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 1260 completed in 539.4479 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_6.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.0880\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0712\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.1281\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.1179\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 6.1124\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 6.0958\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 7, start index 1512 completed in 541.7845 seconds\n",
      "\n",
      "==== Starting Iteration 8/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.6531\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7164\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7608\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7924\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 3.8067\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 3.7883\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 0 completed in 541.1958 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_7.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7864\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.8049\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.8297\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.8089\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.8119\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.8084\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 252 completed in 539.8801 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_7.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7480\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7533\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.7732\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.7818\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.7579\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 4.7622\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 504 completed in 539.6369 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_7.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4892\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.4897\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.4815\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.4786\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.4975\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.5173\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 756 completed in 540.0217 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_7.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.6754\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6316\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.6421\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.7542\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 6.6251\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 6.7314\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 1008 completed in 538.7150 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_7.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.9449\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.9790\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.9494\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.9500\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.9475\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 1260 completed in 487.2259 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_7.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.1405\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0864\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.1143\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.0648\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 8, start index 1512 completed in 435.2576 seconds\n",
      "\n",
      "==== Starting Iteration 9/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7239\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7357\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7122\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 3.7136\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 3.7062\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 0 completed in 487.5707 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_8.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7871\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7604\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.8017\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7870\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.7717\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.7428\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 252 completed in 542.9236 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_8.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7894\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7979\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.7843\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.7744\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.7899\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 504 completed in 489.3149 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_8.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4777\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.4913\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.4945\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.5400\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.5836\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.5823\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 756 completed in 542.6250 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_8.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.7370\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.7494\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.7225\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.7573\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 6.7492\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 6.7542\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 1008 completed in 541.4931 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_8.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.8676\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8519\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.8583\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.9033\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.8848\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 4.8983\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 1260 completed in 542.0660 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_8.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.0325\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0306\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.0329\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.0203\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 6.0088\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 6.0205\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 9, start index 1512 completed in 543.6482 seconds\n",
      "\n",
      "==== Starting Iteration 10/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.8271\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7797\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.7807\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 3.7556\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 0 completed in 435.6099 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_9.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7578\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.8286\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.8360\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7603\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.7942\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.8210\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 252 completed in 541.7174 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_9.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7276\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7672\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.7684\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.7820\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.7449\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 4.7287\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 504 completed in 540.8298 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_9.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4943\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.5246\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.5365\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.5188\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 4.5557\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.5610\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 756 completed in 540.9715 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_9.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.6120\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6623\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.6674\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.7172\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 6.6564\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 6.6136\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 1008 completed in 540.8388 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_9.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.9014\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.9201\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.9039\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.8852\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.8941\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 1260 completed in 488.2063 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_9.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.1112\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.1125\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.1145\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.1039\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 6.1417\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 6.1196\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 10, start index 1512 completed in 541.3635 seconds\n",
      "\n",
      "==== Starting Iteration 11/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7023\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7175\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7361\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7243\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.7113\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.7464\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 0 completed in 542.9211 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_10.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.6210\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.6511\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7667\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7028\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.7190\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.7449\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 252 completed in 552.4820 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_10.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7611\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7573\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7338\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.7404\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 504 completed in 437.3026 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_10.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.3664\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.3721\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.4051\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.3897\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 4.3810\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 4.4167\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 756 completed in 543.3269 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_10.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.6370\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6238\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.6050\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.6372\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 6.6740\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 6.7159\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 1008 completed in 541.8062 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_10.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.8834\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8926\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.8934\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.8763\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 4.8929\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 4.8680\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 1260 completed in 542.0999 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_10.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.1432\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0904\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.0942\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.0795\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 11, start index 1512 completed in 436.1526 seconds\n",
      "\n",
      "==== Starting Iteration 12/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7698\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7529\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.7586\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 3.7181\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 0 completed in 435.9378 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_11.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7619\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7555\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.6969\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 3.7794\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 3.7331\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 3.7499\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 252 completed in 541.7303 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_11.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7064\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7240\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.6831\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.7006\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.7302\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.6916\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 504 completed in 540.8661 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_11.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4324\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.4267\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.4240\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.4510\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.4754\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.4775\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 756 completed in 540.2662 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_11.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.6704\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6748\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.6866\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.6763\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 6.6850\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 6.6449\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 1008 completed in 541.2396 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_11.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.8807\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8649\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7930\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.8635\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 1260 completed in 435.5019 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_11.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 5.9341\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 5.9301\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 5.9693\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 5.9784\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 5.9717\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 5.9648\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 12, start index 1512 completed in 542.6060 seconds\n",
      "\n",
      "==== Starting Iteration 13/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.8009\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7853\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.8163\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7895\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.7846\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.7930\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 0 completed in 539.9121 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_12.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.8462\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.8486\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.8722\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.8937\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 3.9097\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 3.9381\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 252 completed in 541.9495 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_12.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7313\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7214\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7288\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.7311\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 504 completed in 433.5623 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_12.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4719\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.4747\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.4850\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.4883\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.5299\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.5401\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 756 completed in 541.4252 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_12.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.8564\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.8569\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.8389\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.8366\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.8468\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 1008 completed in 486.8369 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_12.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.8350\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8565\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.8591\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.8133\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 4.8471\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 4.8331\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 1260 completed in 541.3115 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_12.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.0905\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0671\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.0946\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.0803\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 6.1089\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 6.1616\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 13, start index 1512 completed in 541.7726 seconds\n",
      "\n",
      "==== Starting Iteration 14/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7710\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7150\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.7829\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7616\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.7506\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.7513\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 0 completed in 540.5947 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_13.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.6710\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7167\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.6588\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 3.6721\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 3.6620\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 252 completed in 487.0826 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_13.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7599\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7487\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7512\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.7561\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 504 completed in 435.6925 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_13.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4109\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.4016\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.3964\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.3752\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 756 completed in 435.3320 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_13.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.4318\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.3784\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.3869\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.4539\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 6.3810\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 6.4308\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 1008 completed in 544.3340 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_13.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.8845\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8746\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.8532\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.8597\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 1260 completed in 435.2347 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_13.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.0869\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0766\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.1055\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.0584\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 6.0412\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 6.0642\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 14, start index 1512 completed in 541.5169 seconds\n",
      "\n",
      "==== Starting Iteration 15/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7241\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7157\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.6984\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 3.7407\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 3.7338\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 3.7462\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 0 completed in 541.8672 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_14.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7883\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.8079\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.8590\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.8609\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 3.8008\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 3.8203\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 252 completed in 541.2134 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_14.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7106\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.6809\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.6859\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.6918\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 504 completed in 435.1414 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_14.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4995\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.5277\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.5447\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.5497\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.5256\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 4.5289\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 756 completed in 541.8259 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_14.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.5995\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6004\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.5502\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.5615\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.5247\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 1008 completed in 488.3126 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_14.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.8894\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8964\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.8875\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.9055\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.9317\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.9280\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 1260 completed in 541.2448 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_14.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.1108\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.1117\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.1052\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.0738\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.1114\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 15, start index 1512 completed in 487.9774 seconds\n",
      "\n",
      "==== Starting Iteration 16/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.6912\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.6817\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.7117\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.6848\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.7206\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 3.6989\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 0 completed in 541.4584 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_15.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.6766\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.6488\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.6524\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 3.6537\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 252 completed in 436.2980 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_15.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7585\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7560\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.8011\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.7688\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 4.8079\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.8091\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 504 completed in 551.0252 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_15.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4975\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.5193\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.5051\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.4511\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.4595\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 756 completed in 489.4262 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_15.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.5741\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.5939\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.5863\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.4852\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.4303\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 1008 completed in 488.6483 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_15.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.9039\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.9101\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.9112\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.9095\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 4.8592\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 4.8743\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 1260 completed in 541.7755 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_15.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.0460\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0515\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.0418\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.0109\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.0374\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 16, start index 1512 completed in 488.4881 seconds\n",
      "\n",
      "==== Starting Iteration 17/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7320\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7602\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7919\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.8223\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 3.8271\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 3.7757\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 0 completed in 542.2433 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_16.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.8369\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.8689\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.8743\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.8423\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.8265\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.8446\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 252 completed in 541.4374 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_16.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7779\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7692\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7865\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.8119\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.8168\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.8175\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 504 completed in 541.9298 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_16.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4998\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.5505\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.5447\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.5068\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.5208\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 756 completed in 489.5756 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_16.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.5830\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6259\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.6971\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.6833\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 6.5727\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 6.6191\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 1008 completed in 541.5931 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_16.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.9152\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.9045\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.8785\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.8960\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 1260 completed in 435.8255 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_16.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.1209\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.1402\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.1330\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.1262\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.1253\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 17, start index 1512 completed in 487.9412 seconds\n",
      "\n",
      "==== Starting Iteration 18/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.6866\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7384\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.6964\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 3.6752\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 3.6834\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 0 completed in 489.8870 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_17.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.6589\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7372\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.8148\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7608\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.7280\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.6876\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 252 completed in 541.8136 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_17.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.6953\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7133\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.6955\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.7430\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.7563\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.7378\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 504 completed in 541.7715 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_17.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4474\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.4640\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.4562\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.4683\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.4762\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.4743\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 756 completed in 542.7168 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_17.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.6489\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.7013\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.7217\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.6995\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 6.7406\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 6.7276\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 1008 completed in 542.4729 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_17.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.8434\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8306\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7969\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.8039\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 1260 completed in 436.0357 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_17.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.1196\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.1301\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.1223\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.1507\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 6.1465\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 6.1826\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 18, start index 1512 completed in 542.3058 seconds\n",
      "\n",
      "==== Starting Iteration 19/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7122\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7523\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7390\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 3.7133\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 3.6840\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 0 completed in 489.5943 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_18.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7963\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.8006\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.6923\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 3.6746\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 3.6996\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 252 completed in 489.6918 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_18.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7546\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7326\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7306\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.7569\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.7468\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 4.7635\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 504 completed in 542.9003 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_18.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4256\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.4243\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.4215\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.4113\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 756 completed in 436.5765 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_18.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.5889\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6657\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.6202\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.5255\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.6508\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 1008 completed in 489.1895 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_18.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.8314\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8717\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.8652\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.8493\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.8306\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 1260 completed in 489.0193 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_18.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.0605\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0361\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.0244\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.0175\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 19, start index 1512 completed in 437.3529 seconds\n",
      "\n",
      "==== Starting Iteration 20/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7406\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7086\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.7781\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7967\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 3.7560\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 3.7383\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 0 completed in 541.9973 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_19.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7690\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7093\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.7516\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 3.7232\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 252 completed in 435.4893 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_19.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7430\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7518\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.7699\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.7912\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.7559\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 4.6954\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 504 completed in 541.4778 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_19.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.4564\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.4810\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.4943\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.3805\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 4.4594\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 4.4601\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 756 completed in 541.3817 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_19.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.6203\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6202\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.6068\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.5965\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 1008 completed in 437.6824 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_19.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.9398\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8962\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.8501\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.8576\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 1260 completed in 436.4778 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_19.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.1737\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.1672\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.1746\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.1559\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 6.1667\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 6.1694\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 20, start index 1512 completed in 542.5857 seconds\n",
      "\n",
      "==== Starting Iteration 21/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7163\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7691\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7416\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 3.7720\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 3.7656\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 3.7165\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 0 completed in 543.0461 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_20.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.6553\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.6898\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7838\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7297\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 3.6963\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 3.7218\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 252 completed in 542.7899 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_20.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7865\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7702\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7403\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.7587\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 504 completed in 434.2567 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_20.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.5351\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.5013\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.5702\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.5959\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.5727\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 4.5765\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 756 completed in 541.2757 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_20.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.6621\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6255\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.6331\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.6028\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 1008 completed in 435.3610 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_20.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.9462\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.9515\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.9381\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.9480\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.9533\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.9455\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 1260 completed in 539.8792 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_20.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.0430\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0685\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.0634\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.0305\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.0619\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 21, start index 1512 completed in 486.3062 seconds\n",
      "\n",
      "==== Starting Iteration 22/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7522\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7250\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.7048\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 3.7160\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 0 completed in 433.8869 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_21.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.8009\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.8379\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7271\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 3.7654\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 3.7993\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 252 completed in 487.4428 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_21.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7850\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7990\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.8089\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.7924\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 4.7855\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 4.7689\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 504 completed in 538.4964 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_21.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.3876\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.3653\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.4178\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.3759\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 4.4326\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.4834\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 756 completed in 539.6350 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_21.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.6645\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6875\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.7191\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.7239\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 6.7594\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 6.6921\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 1008 completed in 539.9823 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_21.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.9835\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.9957\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.9836\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.9786\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.9075\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 1260 completed in 487.5197 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_21.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.0421\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0260\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.0460\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 5.9939\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 5.9954\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 6.0331\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 22, start index 1512 completed in 540.0412 seconds\n",
      "\n",
      "==== Starting Iteration 23/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7547\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7308\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.7596\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7766\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 3.7672\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 3.7658\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 0 completed in 539.4721 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_22.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.8267\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.8300\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7759\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 3.7766\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 3.7667\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 252 completed in 488.6368 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_22.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7480\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7282\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7736\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.7788\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.7795\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.7668\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 504 completed in 540.7797 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_22.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.6267\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.5745\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.5461\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.5808\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 756 completed in 434.9761 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_22.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.7165\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.7246\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.7253\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.7419\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 6.7765\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 6.7279\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 1008 completed in 539.5658 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_22.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.9897\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.9667\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.9418\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.9307\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 1260 completed in 435.5906 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_22.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.0764\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.1264\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.1896\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 6.1913\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 6.1305\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 6.1095\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 23, start index 1512 completed in 541.1756 seconds\n",
      "\n",
      "==== Starting Iteration 24/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7666\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.8058\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7917\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 3.7140\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 3.7651\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 0 completed in 488.2624 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_23.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.6552\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.6183\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.7146\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 3.7205\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 3.7062\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 3.6693\n",
      "No improvement (2/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 252 completed in 541.8492 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_23.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.8325\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.8140\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.8086\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.8408\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 4.8587\n",
      "Improved validation reward; model saved at step 25000\n",
      "Step: 30000, Validation Total Reward: 4.8340\n",
      "No improvement (1/3)\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 504 completed in 541.4496 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_23.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.5682\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.5607\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.5589\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.4906\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 756 completed in 433.9787 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_23.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.6234\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6142\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.5570\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.6358\n",
      "Improved validation reward; model saved at step 20000\n",
      "Step: 25000, Validation Total Reward: 6.6300\n",
      "No improvement (1/3)\n",
      "Step: 30000, Validation Total Reward: 6.6432\n",
      "Improved validation reward; model saved at step 30000\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 1008 completed in 538.8764 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_23.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 5.0077\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 5.0203\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 4.9000\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 4.9170\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 4.9070\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 1260 completed in 487.5886 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_23.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.1880\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.0902\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 6.0022\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 6.0693\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 24, start index 1512 completed in 435.7057 seconds\n",
      "\n",
      "==== Starting Iteration 25/25 ====\n",
      "Training period: 2009-02-06 to 2016-02-09\n",
      "Validation period: 2016-02-10 to 2017-02-08\n",
      "Prediction period: 2017-02-09 to 2018-02-08\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Initializing new PPO model...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7282\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7640\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 3.7556\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 3.7443\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 3.7525\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 0 completed in 489.6697 seconds\n",
      "Training period: 2010-02-08 to 2017-02-08\n",
      "Validation period: 2017-02-09 to 2018-02-08\n",
      "Prediction period: 2018-02-09 to 2019-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_24.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 3.7984\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 3.7670\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 3.7486\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 3.7475\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 252 completed in 434.5726 seconds\n",
      "Training period: 2011-02-07 to 2018-02-08\n",
      "Validation period: 2018-02-09 to 2019-02-11\n",
      "Prediction period: 2019-02-12 to 2020-02-11\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_24.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7442\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7317\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7275\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.7439\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 504 completed in 436.0731 seconds\n",
      "Training period: 2012-02-07 to 2019-02-11\n",
      "Validation period: 2019-02-12 to 2020-02-11\n",
      "Prediction period: 2020-02-12 to 2021-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_24.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.5582\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.5505\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.5887\n",
      "Improved validation reward; model saved at step 15000\n",
      "Step: 20000, Validation Total Reward: 4.5803\n",
      "No improvement (1/3)\n",
      "Step: 25000, Validation Total Reward: 4.5461\n",
      "No improvement (2/3)\n",
      "Step: 30000, Validation Total Reward: 4.5343\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 756 completed in 538.7028 seconds\n",
      "Training period: 2013-02-08 to 2020-02-11\n",
      "Validation period: 2020-02-12 to 2021-02-10\n",
      "Prediction period: 2021-02-11 to 2022-02-09\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_24.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.5762\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.6251\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.5354\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.6046\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.5928\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 1008 completed in 487.1881 seconds\n",
      "Training period: 2014-02-10 to 2021-02-10\n",
      "Validation period: 2021-02-11 to 2022-02-09\n",
      "Prediction period: 2022-02-10 to 2023-02-10\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_24.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 4.7606\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 4.7528\n",
      "No improvement (1/3)\n",
      "Step: 15000, Validation Total Reward: 4.7541\n",
      "No improvement (2/3)\n",
      "Step: 20000, Validation Total Reward: 4.7540\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 1260 completed in 434.8892 seconds\n",
      "Training period: 2015-02-10 to 2022-02-09\n",
      "Validation period: 2022-02-10 to 2023-02-10\n",
      "Prediction period: 2023-02-13 to 2024-02-13\n",
      "Starting hyperparameter tuning...\n",
      "Best parameters: {'n_steps': 20, 'learning_rate': 0.0003, 'gamma': 0.95, 'batch_size': 20, 'risk_coefficient': 1.0}\n",
      "Loading previous model from ppo_train_best_model_iteration_24.zip...\n",
      "Starting model training with early stopping...\n",
      "Step: 5000, Validation Total Reward: 6.0880\n",
      "Improved validation reward; model saved at step 5000\n",
      "Step: 10000, Validation Total Reward: 6.1012\n",
      "Improved validation reward; model saved at step 10000\n",
      "Step: 15000, Validation Total Reward: 6.0710\n",
      "No improvement (1/3)\n",
      "Step: 20000, Validation Total Reward: 6.0383\n",
      "No improvement (2/3)\n",
      "Step: 25000, Validation Total Reward: 6.0664\n",
      "No improvement (3/3)\n",
      "Early stopping explicitly triggered.\n",
      "Loaded the best PPO model explicitly for prediction.\n",
      "Iteration 25, start index 1512 completed in 495.2041 seconds\n",
      "Saved all iterations' allocations to ppo_allocations_multiple_iterations_DIA_ETF.csv\n"
     ]
    }
   ],
   "source": [
    "# ITERATION - final variable: 128/20 - retrain - 50kx30k sample - mean cvar - determinstic false with 50 - 7 yr train by 21 day test\n",
    "# start of stage 2 training\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import time\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data, etf_list, reward_type='mean_cvar', risk_coefficient=0.5, rebalance_period=21, lookback_period=21):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.etf_list = etf_list\n",
    "        self.reward_type = reward_type\n",
    "        self.risk_coefficient = risk_coefficient\n",
    "        self.rebalance_period = rebalance_period\n",
    "        self.lookback_period = lookback_period\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(etf_list),), dtype=np.float32)\n",
    "\n",
    "        # Explicitly select feature columns (excluding Date and returns used only for calculating reward)\n",
    "        self.feature_cols = [col for col in data.columns if col not in ['Date'] and not col.startswith('Actual_Return')]\n",
    "        self.num_features_per_day = len(self.feature_cols)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(self.num_features_per_day * self.lookback_period,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(etf_list)] * len(etf_list))\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.current_step = self.lookback_period\n",
    "        self.done = False\n",
    "        self.cumulative_wealth = 1.0\n",
    "        self.current_weights = np.array([1.0 / len(self.etf_list)] * len(self.etf_list))\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        next_step = self.current_step + 1\n",
    "\n",
    "        if self.current_step % self.rebalance_period == 0:\n",
    "            # v2 long short\n",
    "            desired_long = 1.20  # 120% long exposure explicitly\n",
    "            desired_short = 0.20  # 20% short exposure explicitly\n",
    "            clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "            raw_weights = action.copy()\n",
    "\n",
    "            # Separate explicitly positive (long) and negative (short) actions\n",
    "            long_weights = np.maximum(raw_weights, 0)\n",
    "            short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "            has_longs = np.sum(long_weights) > 0\n",
    "            has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "            if has_longs and has_shorts:\n",
    "                # Normal 120/20 explicitly0\n",
    "                normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "            elif has_longs and not has_shorts:\n",
    "                # Only long explicitly: default realistically to 100% long\n",
    "                normalized_long = long_weights / np.sum(long_weights)\n",
    "                normalized_short = np.zeros_like(short_weights)\n",
    "            elif not has_longs and has_shorts:\n",
    "                # Only short explicitly (unrealistic), fallback clearly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "            else:\n",
    "                # All zeros explicitly: fallback explicitly to equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                normalized_long = np.ones(num_assets) / num_assets\n",
    "                normalized_short = np.zeros(num_assets)\n",
    "\n",
    "            # Apply explicit clipping\n",
    "            combined_weights = normalized_long - normalized_short\n",
    "            clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "            # Re-separate explicitly after clipping\n",
    "            long_clipped = np.maximum(clipped_weights, 0)\n",
    "            short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "            has_long_clipped = np.sum(long_clipped) > 0\n",
    "            has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "            # Final explicit normalization after clipping\n",
    "            if has_long_clipped and has_short_clipped:\n",
    "                final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "            elif has_long_clipped and not has_short_clipped:\n",
    "                final_long = long_clipped / np.sum(long_clipped)  # exactly 100% long\n",
    "                final_short = np.zeros_like(short_clipped)\n",
    "            else:\n",
    "                # Realistic fallback explicitly: equal-weight long-only\n",
    "                num_assets = len(raw_weights)\n",
    "                final_long = np.ones(num_assets) / num_assets\n",
    "                final_short = np.zeros(num_assets)\n",
    "\n",
    "            final_weights = final_long - final_short\n",
    "            self.current_weights = final_weights\n",
    "            \n",
    "            # v1 softmax normalization\n",
    "            # \n",
    "            # temperature = 0.5  # Explicitly lower for higher concentration (try 0.2 to 0.8)\n",
    "            # scaled_action = action / temperature\n",
    "            # self.current_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "\n",
    "        else:\n",
    "            returns_today = np.array([self.data.loc[self.current_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            self.current_weights *= (1 + returns_today)\n",
    "            self.current_weights /= np.sum(self.current_weights)\n",
    "\n",
    "        if next_step >= len(self.data):\n",
    "            terminated = True\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            returns = np.array([self.data.loc[next_step, f'Actual_Return_{etf}'] for etf in self.etf_list])\n",
    "            portfolio_return = np.dot(self.current_weights, returns)\n",
    "            self.cumulative_wealth *= (1 + portfolio_return)\n",
    "            reward = self.calculate_reward(portfolio_return, returns)\n",
    "            terminated = next_step >= len(self.data) - 1\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "        # def _get_obs(self):\n",
    "        #     obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        #     obs_window = obs_window.drop(columns=['Date']).values.flatten().astype(np.float32)\n",
    "        #     return obs_window\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs_window = self.data.iloc[self.current_step - self.lookback_period:self.current_step]\n",
    "        obs_window = obs_window[self.feature_cols].values.flatten().astype(np.float32)\n",
    "        return obs_window\n",
    "\n",
    "    def calculate_reward(self, portfolio_return, asset_returns):\n",
    "        if self.reward_type == 'cumulative_return':\n",
    "            return self.cumulative_wealth - 1.0\n",
    "        elif self.reward_type == 'log_wealth':\n",
    "            return np.log(self.cumulative_wealth)\n",
    "        elif self.reward_type == 'mean_var':\n",
    "            return portfolio_return - self.risk_coefficient * np.var(asset_returns)\n",
    "        elif self.reward_type == 'mean_cvar':\n",
    "            alpha = 0.05\n",
    "            var = np.percentile(asset_returns, 100 * alpha)\n",
    "            cvar = np.mean(asset_returns[asset_returns <= var])\n",
    "            return portfolio_return - self.risk_coefficient * cvar\n",
    "        else:\n",
    "            raise ValueError('Invalid reward type')\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_stable_features(df, etf_list):\n",
    "    data = df.copy()\n",
    "\n",
    "    for etf in etf_list:\n",
    "        price_col = f'Price_{etf}'\n",
    "\n",
    "        # Volatility (20-day)\n",
    "        data[f'Volatility_{etf}'] = data[price_col].pct_change().rolling(20).std()\n",
    "\n",
    "        # Momentum indicators (returns over 5, 10, 20 days)\n",
    "        data[f'Momentum_5d_{etf}'] = data[price_col].pct_change(periods=5)\n",
    "        data[f'Momentum_10d_{etf}'] = data[price_col].pct_change(periods=10)\n",
    "        data[f'Momentum_20d_{etf}'] = data[price_col].pct_change(periods=20)\n",
    "\n",
    "        # Moving averages (5-day and 20-day)\n",
    "        data[f'MA_5d_{etf}'] = data[price_col].rolling(5).mean()\n",
    "        data[f'MA_20d_{etf}'] = data[price_col].rolling(20).mean()\n",
    "\n",
    "        # Moving average crossover (5-day MA - 20-day MA)\n",
    "        data[f'MA_Crossover_{etf}'] = data[f'MA_5d_{etf}'] - data[f'MA_20d_{etf}']\n",
    "\n",
    "    # Drop NaN values due to rolling calculations\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def filter_features(df, include_predicted_returns=True, include_shap_metrics=True):\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # Explicit patterns to identify columns\n",
    "    predicted_return_pattern = 'Predicted_Return'\n",
    "    shap_metric_pattern = 'SHAP'\n",
    "\n",
    "    # Exclude Predicted Returns explicitly if requested\n",
    "    if not include_predicted_returns:\n",
    "        predicted_cols = [col for col in df_filtered.columns if predicted_return_pattern in col]\n",
    "        df_filtered.drop(columns=predicted_cols, inplace=True)\n",
    "        print(f\"Excluded predicted return columns: {predicted_cols}\")\n",
    "\n",
    "    # Exclude SHAP-related metrics explicitly if requested\n",
    "    if not include_shap_metrics:\n",
    "        shap_cols = [col for col in df_filtered.columns if shap_metric_pattern in col]\n",
    "        df_filtered.drop(columns=shap_cols, inplace=True)\n",
    "        print(f\"Excluded SHAP-related columns: {shap_cols}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# ETFs\n",
    "# etf_list = ['XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLY', 'XLV', 'XLU']\n",
    "etf_list = ['BA',\n",
    "'AMGN',\n",
    "'DIS',\n",
    "'NKE',\n",
    "'HON',\n",
    "'MMM',\n",
    "'CAT',\n",
    "'KO',\n",
    "'PG',\n",
    "'AXP',\n",
    "'JPM',\n",
    "'MCD',\n",
    "'HD',\n",
    "'AAPL',\n",
    "'CSCO',\n",
    "'IBM',\n",
    "'MSFT',\n",
    "'TRV',\n",
    "'UNH',\n",
    "'CVX',\n",
    "'JNJ',\n",
    "'MRK',\n",
    "'AMZN',\n",
    "'WMT',\n",
    "'INTC',\n",
    "'VZ']\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 5e-5],\n",
    "    'n_steps': [20, 40],\n",
    "    'batch_size': [5, 10],\n",
    "    'gamma': [0.98, 0.99]\n",
    "}\n",
    "consolidated_file = 'stage2_rl_observations_optimized_DIA_ETF.csv'\n",
    "reward_type = 'mean_cvar'\n",
    "# data = pd.read_csv(consolidated_file, parse_dates=['Date'])\n",
    "# data = data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "data = pd.read_csv('stage2_rl_observations_optimized_DIA_ETF.csv', parse_dates=['Date'])\n",
    "price_data = pd.read_csv('stock_prices_DIA_ETF.csv')\n",
    "\n",
    "# Convert the Date column in price data, handling the timezone correctly\n",
    "price_data['Date'] = pd.to_datetime(price_data['Date'], utc=True)\n",
    "price_data['Date'] = price_data['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Rename price columns explicitly to 'price_{ticker}'\n",
    "price_cols = {col: f'Price_{col}' for col in price_data.columns if col != 'Date'}\n",
    "price_data.rename(columns=price_cols, inplace=True)\n",
    "\n",
    "# Merge datasets on Date\n",
    "merged_data = pd.merge(data, price_data, on='Date', how='inner')\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "# Check if merge was successful\n",
    "if len(merged_data) != len(data):\n",
    "    print(f\"Warning: Data length mismatch after merging (Original: {len(data)}, Merged: {len(merged_data)}).\")\n",
    "else:\n",
    "    print(\"Merged successfully with aligned dates.\")\n",
    "\n",
    "data_with_features_raw = add_stable_features(merged_data, etf_list)\n",
    "data_with_features_raw.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Usage Example clearly for benchmark (only price metrics, no predicted return or SHAP):\n",
    "data_with_features = filter_features(data_with_features_raw, \n",
    "                                 include_predicted_returns=True, \n",
    "                                 include_shap_metrics=True)\n",
    "################### override data to use SHAP only\n",
    "# data_with_features = data\n",
    "################### END override \n",
    "\n",
    "# Define your rolling window lengths clearly:\n",
    "train_window_days = 252 * 7\n",
    "validation_window_days = 252\n",
    "prediction_window_days = 252\n",
    "lookback_period = 21\n",
    "rebalance_period = 21\n",
    "\n",
    "start_indices = range(0, len(data) - (train_window_days + validation_window_days + prediction_window_days), prediction_window_days)\n",
    "all_weights = []\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "def validate_and_tune(train_data, val_data, reward_type, rebalance_period=10, lookback_period=10, n_iter=8, timesteps=5000):\n",
    "    best_reward, best_params = -np.inf, None\n",
    "\n",
    "    # Narrow and meaningful parameter distribution\n",
    "    param_dist = {\n",
    "        'learning_rate': [3e-4, 1e-4],\n",
    "        'n_steps': [20, 40],\n",
    "        'batch_size': [10, 20],\n",
    "        'gamma': [0.95, 0.98],\n",
    "        'risk_coefficient': [0.1, 0.5, 1.0] if reward_type in ['mean_var', 'mean_cvar'] else [0.5],\n",
    "    }\n",
    "\n",
    "    sampled_params = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=42))\n",
    "\n",
    "    for params in sampled_params:\n",
    "        risk_coeff = params.pop('risk_coefficient', 0.5)\n",
    "\n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "        model = PPO('MlpPolicy', env,\n",
    "                    ent_coef=0.01,    # explicitly encourages exploration\n",
    "                    clip_range=0.2,\n",
    "                    **params, verbose=0)\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        val_env = PortfolioEnv(val_data, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        obs, _ = val_env.reset()\n",
    "        done, total_reward = False, 0\n",
    "        \n",
    "        # while not done:\n",
    "        #     action, _ = model.predict(obs, deterministic=True)\n",
    "        #     obs, reward, done, _, _ = val_env.step(action)\n",
    "        #     total_reward += reward\n",
    "        \n",
    "        while not done:\n",
    "            num_samples = 50  # Recommended starting point\n",
    "            action_samples = []\n",
    "        \n",
    "            for _ in range(num_samples):\n",
    "                sampled_action, _ = model.predict(obs, deterministic=False)  # obs directly\n",
    "                action_samples.append(sampled_action)\n",
    "        \n",
    "            action = np.mean(action_samples, axis=0)\n",
    "        \n",
    "            obs, reward, done, _, _ = val_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_params = {**params, 'risk_coefficient': risk_coeff}\n",
    "\n",
    "    return best_params\n",
    "\n",
    "def scale_data(df, feature_cols, scaler):\n",
    "    scaled_features = scaler.transform(df[feature_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=feature_cols, index=df.index)\n",
    "\n",
    "    # Re-add columns that were not scaled (e.g., Date, Actual_Return_*)\n",
    "    for col in df.columns:\n",
    "        if col not in feature_cols:\n",
    "            scaled_df[col] = df[col].values\n",
    "\n",
    "    # Keep original column order\n",
    "    scaled_df = scaled_df[df.columns]\n",
    "    return scaled_df\n",
    "\n",
    "# Main execution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "iterations = 25\n",
    "all_weights_iterations = []\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    print(f\"\\n==== Starting Iteration {iteration + 1}/{iterations} ====\")\n",
    "    model_path = f\"ppo_train_best_model_iteration_{iteration}.zip\"\n",
    "    for idx, start_idx in enumerate(start_indices):\n",
    "        # for start_idx in range(0, 252*2, 252):\n",
    "        start_time = time.time()\n",
    "    \n",
    "        # Explicit indices for training, validation, and prediction datasets\n",
    "        train_start_idx = start_idx\n",
    "        train_end_idx = train_start_idx + train_window_days\n",
    "    \n",
    "        val_start_idx = train_end_idx\n",
    "        val_end_idx = val_start_idx + validation_window_days\n",
    "    \n",
    "        pred_start_idx = val_end_idx\n",
    "        pred_end_idx = pred_start_idx + prediction_window_days\n",
    "    \n",
    "        # Corresponding dates explicitly\n",
    "        train_start_date = data_with_features.loc[train_start_idx, 'Date']\n",
    "        train_end_date = data_with_features.loc[train_end_idx - 1, 'Date']\n",
    "    \n",
    "        val_start_date = data_with_features.loc[val_start_idx, 'Date']\n",
    "        val_end_date = data_with_features.loc[val_end_idx - 1, 'Date']\n",
    "    \n",
    "        pred_start_date = data_with_features.loc[pred_start_idx, 'Date']\n",
    "        pred_end_date = data_with_features.loc[pred_end_idx - 1, 'Date']\n",
    "    \n",
    "        # Clearly print ranges for clarity\n",
    "        print(f\"Training period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "        print(f\"Validation period: {val_start_date.date()} to {val_end_date.date()}\")\n",
    "        print(f\"Prediction period: {pred_start_date.date()} to {pred_end_date.date()}\")\n",
    "    \n",
    "        # Explicitly subset data accordingly\n",
    "        train_data = data_with_features.iloc[train_start_idx:train_end_idx].reset_index(drop=True)\n",
    "        val_data = data_with_features.iloc[val_start_idx:val_end_idx].reset_index(drop=True)\n",
    "        pred_data = data_with_features.iloc[pred_start_idx:pred_end_idx].reset_index(drop=True)\n",
    "    \n",
    "        feature_cols = [col for col in train_data.columns if col != 'Date' and not col.startswith('Actual_Return')]\n",
    "    \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_data[feature_cols])\n",
    "    \n",
    "        train_data_scaled = scale_data(train_data, feature_cols, scaler)\n",
    "        val_data_scaled = scale_data(val_data, feature_cols, scaler)\n",
    "        pred_data_scaled = scale_data(pred_data, feature_cols, scaler)\n",
    "    \n",
    "        print(\"Starting hyperparameter tuning...\")\n",
    "        best_params = validate_and_tune(train_data_scaled, val_data_scaled, reward_type)\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "        incremental_timesteps = 5000\n",
    "        max_timesteps = 30000\n",
    "        patience = 3\n",
    "        \n",
    "        best_val_reward = -np.inf\n",
    "        no_improve_steps = 0\n",
    "    \n",
    "        risk_coeff = best_params.pop('risk_coefficient',0.5)\n",
    "        policy_kwargs = dict(net_arch=[256, 256])\n",
    "    \n",
    "        env = make_vec_env(lambda: PortfolioEnv(train_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period), n_envs=1)\n",
    "        \n",
    "        # Load previous model if exists\n",
    "        if idx > 0 and os.path.exists(model_path):\n",
    "            print(f\"Loading previous model from {model_path}...\")\n",
    "            model = PPO.load(model_path, env=env)\n",
    "            model.set_env(env)\n",
    "        else:\n",
    "            print(\"Initializing new PPO model...\")\n",
    "            model = PPO('MlpPolicy', env,\n",
    "                        policy_kwargs=policy_kwargs,\n",
    "                        ent_coef=0.01,\n",
    "                        clip_range=0.2,\n",
    "                        **best_params, verbose=0)\n",
    "         # always retrain\n",
    "        # model = PPO('MlpPolicy', env,\n",
    "        #             policy_kwargs=policy_kwargs,\n",
    "        #             ent_coef=0.01,    # explicitly encourages exploration\n",
    "        #             clip_range=0.2,\n",
    "        #             **best_params, verbose=0)\n",
    "        # print(\"Starting model training...\")\n",
    "        # model.learn(total_timesteps=20000)\n",
    "        print(\"Starting model training with early stopping...\")\n",
    "        \n",
    "        # model = PPO('MlpPolicy', env,\n",
    "        #             policy_kwargs=policy_kwargs,\n",
    "        #             ent_coef=0.01,    # explicitly encourages exploration\n",
    "        #             clip_range=0.2,\n",
    "        #             **best_params, verbose=0)\n",
    "        # print(\"Starting model training...\")\n",
    "        # model.learn(total_timesteps=20000)\n",
    "    \n",
    "        for step in range(0, max_timesteps, incremental_timesteps):\n",
    "            model.learn(total_timesteps=incremental_timesteps)\n",
    "        \n",
    "            # Evaluate on validation environment\n",
    "            val_env = PortfolioEnv(val_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "            val_obs, _ = val_env.reset()\n",
    "            val_done = False\n",
    "            val_total_reward = 0.0\n",
    "        \n",
    "            while not val_done:\n",
    "                # val_action, _ = model.predict(val_obs, deterministic=True)\n",
    "                # val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "                # val_total_reward += val_reward\n",
    "                \n",
    "                num_samples = 50  # Recommended\n",
    "                value_action_samples = []\n",
    "        \n",
    "                for _ in range(num_samples):\n",
    "                    value_sampled_action, _ = model.predict(val_obs, deterministic=False)\n",
    "                    value_action_samples.append(value_sampled_action)\n",
    "            \n",
    "                val_action = np.mean(value_action_samples, axis=0)    \n",
    "                \n",
    "                val_obs, val_reward, val_done, _, _ = val_env.step(val_action)\n",
    "                val_total_reward += val_reward\n",
    "        \n",
    "            print(f\"Step: {step + incremental_timesteps}, Validation Total Reward: {val_total_reward:.4f}\")\n",
    "        \n",
    "            # Early stopping check\n",
    "            if val_total_reward > best_val_reward:\n",
    "                best_val_reward = val_total_reward\n",
    "                no_improve_steps = 0\n",
    "                # model.save(\"best_ppo_model.zip\")\n",
    "                model.save(model_path)\n",
    "                print(f\"Improved validation reward; model saved at step {step + incremental_timesteps}\")\n",
    "            else:\n",
    "                no_improve_steps += 1\n",
    "                print(f\"No improvement ({no_improve_steps}/{patience})\")\n",
    "        \n",
    "                if no_improve_steps >= patience:\n",
    "                    print(\"Early stopping explicitly triggered.\")\n",
    "                    break\n",
    "        \n",
    "        # Load the best model explicitly\n",
    "        # model = PPO.load(\"best_ppo_model.zip\")\n",
    "        model = PPO.load(model_path)\n",
    "        \n",
    "        print(\"Loaded the best PPO model explicitly for prediction.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "        # Ensure historical context explicitly available in prediction\n",
    "        full_data = pd.concat([train_data_scaled, val_data_scaled, pred_data_scaled])\n",
    "        pred_data_with_history = full_data[full_data['Date'] >= (pred_start_date - pd.Timedelta(days=lookback_period))].reset_index(drop=True)\n",
    "    \n",
    "        pred_env = PortfolioEnv(pred_data_scaled, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "        # pred_env = PortfolioEnv(pred_data_with_history, etf_list, reward_type, risk_coeff, rebalance_period, lookback_period)\n",
    "    \n",
    "        obs, info = pred_env.reset()\n",
    "        done = False\n",
    "    \n",
    "        action = np.zeros(len(etf_list), dtype=np.float32)\n",
    "    \n",
    "        while not done:\n",
    "            if pred_env.current_step >= lookback_period and pred_env.current_step % pred_env.rebalance_period == 0:\n",
    "                # obs_for_agent = pred_data_with_history.drop(columns=['Date']).iloc[pred_env.current_step - lookback_period:pred_env.current_step].values.flatten().astype(np.float32)\n",
    "                # action, _ = model.predict(obs_for_agent, deterministic=True)\n",
    "    \n",
    "                # v1 normalize weight\n",
    "                # action, _ = model.predict(obs, deterministic=True)\n",
    "                \n",
    "                # num_samples = 50  # Recommended\n",
    "                # action_samples = []\n",
    "                # \n",
    "                # for _ in range(num_samples):\n",
    "                #     sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "                #     action_samples.append(sampled_action)\n",
    "                # \n",
    "                # action = np.mean(action_samples, axis=0)    \n",
    "                # \n",
    "                # temperature = 0.5\n",
    "                # scaled_action = action / temperature\n",
    "                # final_weights = np.exp(scaled_action) / np.sum(np.exp(scaled_action))\n",
    "                # rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "                # # all_weights.append([rebalance_date] + weights.tolist())\n",
    "                # all_weights_iterations.append([iteration + 1, rebalance_date] + final_weights.tolist())\n",
    "    \n",
    "                # v2 long short normalization\n",
    "                # action, _ = model.predict(obs, deterministic=True)\n",
    "                \n",
    "                num_samples = 50  # Recommended\n",
    "                action_samples = []\n",
    "\n",
    "                for _ in range(num_samples):\n",
    "                    sampled_action, _ = model.predict(obs, deterministic=False)\n",
    "                    action_samples.append(sampled_action)\n",
    "\n",
    "                action = np.mean(action_samples, axis=0)    \n",
    "\n",
    "                # Explicitly apply your new 120/20 normalization logic (to match environment step)\n",
    "                desired_long = 1.20  # Explicitly 120% long exposure\n",
    "                desired_short = 0.20  # Explicitly 20% short exposure\n",
    "                clip_bounds = (-0.2, 0.8)\n",
    "\n",
    "                raw_weights = action.copy()\n",
    "\n",
    "                # Separate explicitly positive (long) and negative (short) actions\n",
    "                long_weights = np.maximum(raw_weights, 0)\n",
    "                short_weights = np.abs(np.minimum(raw_weights, 0))\n",
    "\n",
    "                has_longs = np.sum(long_weights) > 0\n",
    "                has_shorts = np.sum(short_weights) > 0\n",
    "\n",
    "                if has_longs and has_shorts:\n",
    "                    normalized_long = desired_long * long_weights / np.sum(long_weights)\n",
    "                    normalized_short = desired_short * short_weights / np.sum(short_weights)\n",
    "                elif has_longs and not has_shorts:\n",
    "                    normalized_long = long_weights / np.sum(long_weights)\n",
    "                    normalized_short = np.zeros_like(short_weights)\n",
    "                elif not has_longs and has_shorts:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    normalized_long = np.ones(num_assets) / num_assets\n",
    "                    normalized_short = np.zeros(num_assets)\n",
    "                else:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    normalized_long = np.ones(num_assets) / num_assets\n",
    "                    normalized_short = np.zeros(num_assets)\n",
    "\n",
    "                combined_weights = normalized_long - normalized_short\n",
    "                clipped_weights = np.clip(combined_weights, clip_bounds[0], clip_bounds[1])\n",
    "\n",
    "                # Re-separate after clipping explicitly\n",
    "                long_clipped = np.maximum(clipped_weights, 0)\n",
    "                short_clipped = np.abs(np.minimum(clipped_weights, 0))\n",
    "\n",
    "                has_long_clipped = np.sum(long_clipped) > 0\n",
    "                has_short_clipped = np.sum(short_clipped) > 0\n",
    "\n",
    "                if has_long_clipped and has_short_clipped:\n",
    "                    final_long = desired_long * long_clipped / np.sum(long_clipped)\n",
    "                    final_short = desired_short * short_clipped / np.sum(short_clipped)\n",
    "                elif has_long_clipped and not has_short_clipped:\n",
    "                    final_long = long_clipped / np.sum(long_clipped)\n",
    "                    final_short = np.zeros_like(short_clipped)\n",
    "                else:\n",
    "                    num_assets = len(raw_weights)\n",
    "                    final_long = np.ones(num_assets) / num_assets\n",
    "                    final_short = np.zeros(num_assets)\n",
    "\n",
    "                final_weights = final_long - final_short\n",
    "\n",
    "                rebalance_date = pred_data_with_history.loc[pred_env.current_step, 'Date']\n",
    "                # all_weights.append([rebalance_date] + final_weights.tolist())\n",
    "                all_weights_iterations.append([iteration + 1, rebalance_date] + final_weights.tolist())\n",
    "                # \n",
    "            obs, _, done, _, _ = pred_env.step(action)\n",
    "    \n",
    "        end_time = time.time()\n",
    "        print(f\"Iteration {iteration + 1}, start index {start_idx} completed in {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "columns = ['Iteration', 'Date'] + etf_list\n",
    "weights_df = pd.DataFrame(all_weights_iterations, columns=columns)\n",
    "weights_df.to_csv('ppo_allocations_multiple_iterations_DIA_ETF.csv', index=False)\n",
    "print(\"Saved all iterations' allocations to ppo_allocations_multiple_iterations_DIA_ETF.csv\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-19T02:30:32.331249Z",
     "start_time": "2025-07-18T01:56:58.844033Z"
    }
   },
   "id": "86d1a98b60121652",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.0017681 , 0.08160286, 0.00324033, 0.0017681 , 0.02787323,\n       0.09653515, 0.06529096, 0.01211179, 0.0017681 , 0.09653515,\n       0.00258168, 0.0017681 , 0.00181724, 0.09653515, 0.0017681 ,\n       0.03328965, 0.09653515, 0.01756159, 0.09653515, 0.06530398,\n       0.0017681 , 0.00191536, 0.09653515, 0.09252811, 0.00263784,\n       0.00242582], dtype=float32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-16T12:01:03.956993Z",
     "start_time": "2025-07-16T12:01:03.946991Z"
    }
   },
   "id": "6947d247eaecd896",
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
