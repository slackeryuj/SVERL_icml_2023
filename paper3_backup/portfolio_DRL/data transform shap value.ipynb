{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "file_path = \"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/test_data_shap_metrics.csv\"  # Replace with the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
    "df['End_Date'] = pd.to_datetime(df['End_Date'])\n",
    "\n",
    "# Pivot the table to transform it into a time series format with Start_Date and End_Date\n",
    "df_pivot = df.pivot_table(\n",
    "    index=['Start_Date', 'End_Date'], \n",
    "    columns='Stock', \n",
    "    values=[col for col in df.columns if col not in ['Stock', 'Start_Date', 'End_Date', 'Phase']]\n",
    ")\n",
    "\n",
    "# Flatten the multi-index columns\n",
    "df_pivot.columns = [f\"{col[0]}_{col[1]}\" for col in df_pivot.columns]\n",
    "\n",
    "# Sort the dataframe by Start_Date and End_Date\n",
    "df_pivot.sort_index(inplace=True)\n",
    "\n",
    "# Save the transformed data to a new CSV file\n",
    "df_pivot.to_csv(\"transformed_shap_timeseries.csv\")\n",
    "\n",
    "print(\"Transformation complete. Data saved as 'transformed_shap_timeseries.csv'.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b60092c0db6e0b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the transformed SHAP dataset\n",
    "shap_file_path = \"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/transformed_shap_timeseries.csv\"\n",
    "df_shap = pd.read_csv(shap_file_path, index_col=[0, 1])\n",
    "\n",
    "# Load the ETF daily returns dataset\n",
    "returns_file_path = \"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/daily_returns.csv\"\n",
    "df_returns = pd.read_csv(returns_file_path, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# Ensure ETF tickers are correctly identified\n",
    "etfs = list(set([col.split(\"_\")[-1] for col in df_shap.columns if \"mean_return\" not in col and \"volatility\" not in col]))\n",
    "\n",
    "# Initialize columns for mean return and volatility\n",
    "for etf in etfs:\n",
    "    df_shap[f\"mean_return_{etf}\"] = np.nan\n",
    "    df_shap[f\"volatility_{etf}\"] = np.nan\n",
    "\n",
    "# Compute mean return and volatility for each ETF based on Start_Date and End_Date\n",
    "for (start_date, end_date) in df_shap.index[[0]]:\n",
    "\n",
    "    # Convert to datetime format\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Get mean return over the period\n",
    "    period_returns = df_returns.loc[start_date:end_date]\n",
    "    mean_returns = period_returns.mean()\n",
    "    period_returns_df = pd.DataFrame(period_returns)\n",
    "    # Get volatility using past 60 days from end_date\n",
    "    past_60_returns = df_returns.loc[:end_date].iloc[-60:]\n",
    "    volatilities = past_60_returns.std()\n",
    "\n",
    "    # Assign computed values to df_shap\n",
    "    for etf in etfs:\n",
    "        if etf in mean_returns and etf in volatilities:\n",
    "            df_shap.at[(start_date, end_date), f\"mean_return_{etf}\"] = mean_returns[etf]\n",
    "            df_shap.at[(start_date, end_date), f\"volatility_{etf}\"] = volatilities[etf]\n",
    "\n",
    "# Save the updated dataset\n",
    "updated_shap_file_path = \"transformed_shap_timeseries_updated_test.csv\"\n",
    "df_shap.to_csv(updated_shap_file_path)\n",
    "\n",
    "print(f\"Updated dataset saved as {updated_shap_file_path}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38a43f32252612a9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"transformed_shap_timeseries_updated.csv\"\n",
    "df_shap = pd.read_csv(file_path, index_col=[0, 1])\n",
    "# Sort columns first by feature type, then by ETF\n",
    "feature_order = sorted(df_shap.columns, key=lambda x: (x.split('_')[0], x.split('_')[-1]))\n",
    "df_shap = df_shap[feature_order]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8fd988e2d3c0575",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_shap.to_csv(\"transformed_shap_timeseries_updated2.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2574b4646c90bdb3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the daily returns dataset\n",
    "daily_returns_path = \"daily_returns.csv\"\n",
    "df_returns = pd.read_csv(daily_returns_path, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# Sort daily return columns in the same order as feature_order\n",
    "sorted_etfs =  [col.replace(\"mean_return_\", \"\") for col in feature_order if \"mean_return\" in col]\n",
    "df_returns = df_returns[sorted_etfs]\n",
    "df_returns.to_csv(\"daily_returns2.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f70cbcf3912eaa63",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "etfs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6643d475d373816",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "from SVERL_icml_2023.portfolio_DRL.data_function import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SVERL_icml_2023.portfolio_DRL.create_model import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from SVERL_icml_2023.shapley import Shapley\n",
    "import shap\n",
    "sys.path.append(\"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023\")\n",
    "# from portfolio_DRL.create_model import StockPredictWrapper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "file_path = \"E:/XAI/RL_with_SHAP/pythonProject1/SVERL_icml_2023/portfolio_DRL/transformed_shap_timeseries_updated_sort.csv\"\n",
    "df_shap = pd.read_csv(file_path, index_col=[0, 1])\n",
    "\n",
    "# Extract ETFs\n",
    "etfs = list(set([col.split(\"_\")[-1] for col in df_shap.columns if \"mean_return\" in col]))\n",
    "print(f'etfs={etfs}')\n",
    "\n",
    "global num_etfs\n",
    "num_etfs = len(etfs)\n",
    "print(f'num_etfs={num_etfs}')\n",
    "daily_returns_path = \"daily_returns_sort.csv\"\n",
    "df_returns = pd.read_csv(daily_returns_path, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "# Train agent with rolling 78-sample window\n",
    "window_size = 78\n",
    "num_samples = len(df_shap)\n",
    "out_of_sample_weights = []\n",
    "\n",
    "for start in range(0, num_samples - window_size, window_size):\n",
    "    end = start + window_size\n",
    "    train_data = df_shap.iloc[start:end]\n",
    "    env = make_vec_env(lambda: PortfolioTradingEnv(train_data, df_returns,etfs), n_envs=1)\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    \n",
    "    # Save trained model\n",
    "    model_path = f\"ppo_portfolio_model_{start}_{end}.zip\"\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "    \n",
    "    # Generate out-of-sample weights\n",
    "    test_data = df_shap.iloc[end:end+1]\n",
    "    obs = test_data.values\n",
    "    weights_list = []\n",
    "    for _ in range(100):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        weights_list.append(action)\n",
    "    avg_weights = np.mean(weights_list, axis=0)\n",
    "    out_of_sample_weights.append(avg_weights)\n",
    "\n",
    "# Save the results\n",
    "out_of_sample_df = pd.DataFrame(out_of_sample_weights, columns=etfs)\n",
    "out_of_sample_df.to_csv(\"out_of_sample_weights.csv\")\n",
    "print(\"Training complete. Models and out-of-sample weights saved.\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4245e0b55284d8d5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The algorithm only supports (<class 'gymnasium.spaces.box.Box'>, <class 'gymnasium.spaces.discrete.Discrete'>, <class 'gymnasium.spaces.multi_discrete.MultiDiscrete'>, <class 'gymnasium.spaces.multi_binary.MultiBinary'>) as action spaces but Box(0.0, 1.0, (12,), float32) was provided",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 94\u001B[0m\n\u001B[0;32m     92\u001B[0m train_data \u001B[38;5;241m=\u001B[39m df_shap\u001B[38;5;241m.\u001B[39miloc[start:end]\n\u001B[0;32m     93\u001B[0m env \u001B[38;5;241m=\u001B[39m make_vec_env(\u001B[38;5;28;01mlambda\u001B[39;00m: PortfolioTradingEnv(train_data, df_returns), n_envs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 94\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mPPO\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMlpPolicy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     95\u001B[0m model\u001B[38;5;241m.\u001B[39mlearn(total_timesteps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m)\n\u001B[0;32m     97\u001B[0m \u001B[38;5;66;03m# Save trained model\u001B[39;00m\n",
      "File \u001B[1;32mE:\\XAI\\RL_with_SHAP\\pythonProject1\\.venv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:109\u001B[0m, in \u001B[0;36mPPO.__init__\u001B[1;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     82\u001B[0m     policy: Union[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mtype\u001B[39m[ActorCriticPolicy]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    107\u001B[0m     _init_setup_model: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    108\u001B[0m ):\n\u001B[1;32m--> 109\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[43m        \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    112\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgamma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgae_lambda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgae_lambda\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[43m        \u001B[49m\u001B[43ment_coef\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43ment_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvf_coef\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvf_coef\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_grad_norm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_grad_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_sde\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_sde\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[43msde_sample_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msde_sample_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrollout_buffer_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrollout_buffer_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrollout_buffer_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrollout_buffer_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstats_window_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstats_window_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtensorboard_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensorboard_log\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    129\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_init_setup_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    130\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupported_action_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    131\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspaces\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBox\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    132\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspaces\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDiscrete\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspaces\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMultiDiscrete\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    134\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspaces\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMultiBinary\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    135\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;66;03m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001B[39;00m\n\u001B[0;32m    139\u001B[0m     \u001B[38;5;66;03m# because of the advantage normalization\u001B[39;00m\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m normalize_advantage:\n",
      "File \u001B[1;32mE:\\XAI\\RL_with_SHAP\\pythonProject1\\.venv\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:86\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.__init__\u001B[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     63\u001B[0m     policy: Union[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mtype\u001B[39m[ActorCriticPolicy]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     84\u001B[0m     supported_action_spaces: Optional[\u001B[38;5;28mtuple\u001B[39m[\u001B[38;5;28mtype\u001B[39m[spaces\u001B[38;5;241m.\u001B[39mSpace], \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     85\u001B[0m ):\n\u001B[1;32m---> 86\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m     87\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     88\u001B[0m \u001B[43m        \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     90\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     92\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     93\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_sde\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_sde\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[43m        \u001B[49m\u001B[43msde_sample_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msde_sample_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupport_multi_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     96\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmonitor_wrapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmonitor_wrapper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     97\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     98\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstats_window_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstats_window_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     99\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtensorboard_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensorboard_log\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    100\u001B[0m \u001B[43m        \u001B[49m\u001B[43msupported_action_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msupported_action_spaces\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    101\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    103\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_steps \u001B[38;5;241m=\u001B[39m n_steps\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgamma \u001B[38;5;241m=\u001B[39m gamma\n",
      "File \u001B[1;32mE:\\XAI\\RL_with_SHAP\\pythonProject1\\.venv\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:181\u001B[0m, in \u001B[0;36mBaseAlgorithm.__init__\u001B[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001B[0m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_vec_normalize_env \u001B[38;5;241m=\u001B[39m unwrap_vec_normalize(env)\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m supported_action_spaces \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 181\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space, supported_action_spaces), (\n\u001B[0;32m    182\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe algorithm only supports \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msupported_action_spaces\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m as action spaces \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    183\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m was provided\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    184\u001B[0m     )\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m support_multi_env \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_envs \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    188\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError: the model does not support multiple envs; it requires \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma single vectorized environment.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    189\u001B[0m     )\n",
      "\u001B[1;31mAssertionError\u001B[0m: The algorithm only supports (<class 'gymnasium.spaces.box.Box'>, <class 'gymnasium.spaces.discrete.Discrete'>, <class 'gymnasium.spaces.multi_discrete.MultiDiscrete'>, <class 'gymnasium.spaces.multi_binary.MultiBinary'>) as action spaces but Box(0.0, 1.0, (12,), float32) was provided"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "\n",
    "# Load the transformed SHAP dataset\n",
    "file_path = \"transformed_shap_timeseries_updated_sort.csv\"\n",
    "df_shap = pd.read_csv(file_path, index_col=[0, 1])\n",
    "\n",
    "# Load the daily returns dataset\n",
    "daily_returns_path = \"daily_returns_sort.csv\"\n",
    "df_returns = pd.read_csv(daily_returns_path, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# Extract ETFs\n",
    "etfs = list(set([col.split(\"_\")[-1] for col in df_shap.columns if \"mean_return\" in col]))\n",
    "global num_etfs\n",
    "num_etfs = len(etfs)\n",
    "\n",
    "class PortfolioTradingEnv(gym.Env):\n",
    "    def __init__(self, data, returns, initial_balance=1000, etfs=None):\n",
    "        super(PortfolioTradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.returns = returns\n",
    "        self.initial_balance = initial_balance\n",
    "        self.current_step = 0\n",
    "        self.etfs = etfs if etfs else sorted_etfs\n",
    "        self.num_etfs = len(self.etfs)\n",
    "        self.etfs = etfs if etfs else []\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(data.shape[1],), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=0, high=1.0, shape=(12,), dtype=np.float32)\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.weights = np.ones(self.num_etfs) / self.num_etfs\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "    def __init__(self, data, returns, initial_balance=1000):\n",
    "        super(PortfolioTradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.returns = returns\n",
    "        self.initial_balance = initial_balance\n",
    "        self.current_step = 0\n",
    "        self.etfs = etfs if etfs else sorted_etfs\n",
    "        self.num_etfs = len(self.etfs)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(data.shape[1],), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=np.array([0.0] * 12, dtype=np.float32), high=np.array([1.0] * 12, dtype=np.float32), dtype=np.float32)\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.weights = np.ones(self.num_etfs) / self.num_etfs\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.weights = np.ones(self.num_etfs) / self.num_etfs\n",
    "        return self.data.iloc[self.current_step].values\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.array(action, dtype=np.float32)\n",
    "        action = np.clip(action, 0, 1)\n",
    "        action /= np.sum(action)\n",
    "        self.weights = action\n",
    "\n",
    "        start_date, end_date = self.data.index[self.current_step]\n",
    "        period_returns = self.returns.loc[start_date:end_date, self.etfs].values\n",
    "        portfolio_values = [self.portfolio_value]\n",
    "        for daily_return in period_returns:\n",
    "            self.portfolio_value *= (1 + np.dot(daily_return, self.weights))\n",
    "            self.weights *= (1 + daily_return)\n",
    "            self.weights /= np.sum(self.weights)  # Normalize weights after drift\n",
    "            portfolio_values.append(self.portfolio_value)\n",
    "        cumulative_return = (portfolio_values[-1] / portfolio_values[0]) - 1\n",
    "        \n",
    "\n",
    "        self.portfolio_value *= (1 + cumulative_return)\n",
    "        reward = np.log(self.portfolio_value / self.initial_balance)\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        next_state = self.data.iloc[self.current_step].values if not done else np.zeros_like(self.data.iloc[0].values)\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Step: {self.current_step}, Portfolio Value: {self.portfolio_value:.2f}\")\n",
    "\n",
    "# Train agent with rolling 78-sample window\n",
    "window_size = 78\n",
    "num_samples = len(df_shap)\n",
    "out_of_sample_weights = []\n",
    "\n",
    "for start in range(0, num_samples - window_size, window_size):\n",
    "    end = start + window_size\n",
    "    train_data = df_shap.iloc[start:end]\n",
    "    env = make_vec_env(lambda: PortfolioTradingEnv(train_data, df_returns), n_envs=1)\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "    model.learn(total_timesteps=10000)\n",
    "    \n",
    "    # Save trained model\n",
    "    model_path = f\"ppo_portfolio_model_{start}_{end}.zip\"\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "    \n",
    "    # Generate out-of-sample weights\n",
    "    test_data = df_shap.iloc[end:end+1]\n",
    "    obs = test_data.values\n",
    "    weights_list = []\n",
    "    for _ in range(100):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        weights_list.append(action)\n",
    "    avg_weights = np.mean(weights_list, axis=0)\n",
    "    out_of_sample_weights.append(avg_weights)\n",
    "\n",
    "# Save the results\n",
    "out_of_sample_df = pd.DataFrame(out_of_sample_weights, columns=etfs)\n",
    "out_of_sample_df.to_csv(\"out_of_sample_weights.csv\")\n",
    "print(\"Training complete. Models and out-of-sample weights saved.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-19T03:03:40.997214Z",
     "start_time": "2025-03-19T03:03:37.434354Z"
    }
   },
   "id": "e8440aae0340d2af",
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
